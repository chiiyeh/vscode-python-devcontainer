{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d959c237-529f-4bcb-a242-fef1f8afa603",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chiiyeh/.local/share/virtualenvs/dev-WhXEzAwp/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import ctranslate2\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82e3fc81-c176-4ebe-b548-11d3a641a9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = '''The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.                                       \n",
    "'''\n",
    "text = '''\n",
    "Mod: Ok everyone, thank you for introducing yourselves to one another. I hope you all know who’s on your left and right already. I want to get started with today’s discussion actually. So very quickly, we’re talking about living in Singapore right? What are some words that you would use to describe Singapore society? So us as a society right, what are some words that come to mind? Just shout them out at me.\n",
    "Tim: Affluent. \n",
    "Mod: Ok, we’re an affluent society. What else?\n",
    "Mary: Expensive. \n",
    "Mod: Expensive, ok.\n",
    "John: Money conscious.\n",
    "Peter: Safe. \n",
    "Mod: We’re safe, ok.\n",
    "Jane: Busybody \n",
    "Mod: Busybody, ok. Maybe I hear one more, Chin?\n",
    "Chin: Stressful. \n",
    "Mod: Stressful? Ok I’m just curious, what do you mean busybody, as a society?\n",
    "Jane: Yeah, society is getting online, the media and everything. Those kind of netizen. \n",
    "Mod: Right, netizens. You mentioned money conscious?\n",
    "Peter: No. \n",
    "Mod: Who? You tell me what you mean by that, what are you thinking about, as a society, money conscious?\n",
    "John: Because I just moved back to Singapore, so like I, I attended some courses. So every time, every time like, when I do the course right, they say oh you must go up and you know speak your mind, or you go up and do, participate. You think about the money you paid, and make your money's worth. \n",
    "Mod: Oh, make your money worth, ok.\n",
    "John: So like I'm, there's this course right, they have it in Australia where I was living, so nobody talked about making your money’s worth. Everyone talked about, oh you know, it's something you can breakthrough for your personal life and you can, your own improvement and everything. But in Singapore right, everybody oh you go, you know, make your money worth, think about the money you spent, you know, get your money worth. Everyone tells me that. \n",
    "Mod: That's the difference there?\n",
    "John: It’s a big difference. \n",
    "Mod: Ok, just now you mentioned something?\n",
    "Tim: Affluent. \n",
    "Mod: Affluent, ok tell me what were you thinking about, as a society we are affluent? What does that mean?\n",
    "Tim: Because I think, I think because we are easily influenced by one another. So for example I make it very basic, you see your friend carrying an LV bag, so you decide you also want. \n",
    "Mod: Oh, you also want one,\n",
    "Tim: That kind of thing. So for us, eh my neighbour drive a very nice car, oh wow, BMW. I only got Toyota, maybe I should work harder for that, for my wife and me, ah that kind. So I think, I think in that sense, and I mean as simply as our phone also, you know, we want, we want the best for ourselves. Because you see that person having it you also want the same thing. \n",
    "Mod: Would you say that's a good thing or a bad thing? I mean, the way you're saying it, I don't know. But is it just like an observation?\n",
    "Tim: Um, competing to the point where it puts you in a lose-lose situation, I think it's not good. \n",
    "Mod: I see it's not good, just competing.\n",
    "'''\n",
    "# I1: Yeah.\n",
    "# Mod: Ok, great. Anything else? No? Ok, actually I want to show you all a set of words on the screen, so the guys on my right you all have to turn around. I'm just going to show you here, there are actually 5 words or terms that can describe our society right. Can each of you individually and in your own head, help me choose the top 3 that you would say best represents Singapore. So there’s 5 here, you choose the top 3 that you feel best represents Singapore. Then I want to know which one you chose, and why? I give you all like maybe 10 more seconds.\n",
    "# I3: Can explain what does the words mean? \n",
    "# Mod: Which one?\n",
    "# I2: All. \n",
    "# Mod: All? \n",
    "# I2: United people don't have, every day people take picture post on Stomp.\n",
    "# Mod: Ok, so I clarify the point of this activity. I get what you are saying, because I think you all understand the individual words right? Like for example, united people, for some people it could mean different things. And that's fine. So in your own heads, you see these 5 right, which one would you say yeah, Singapore society is like that, just do the top 3. Yes?\n",
    "# I7: Current or? Because like this, (Current, current. Currently our society, which 3 would you choose? The top 3, that you say that,)\n",
    "# I3: What if don’t have? (Don't have, not even 1?) What is, what’s social mobility? (Ok, thanks for asking. The rest of you know what social mobility is? Just curious, this term,)\n",
    "# I4: Is it like on the phone all the time, you know updating,\n",
    "# I1: That's what I was thinking also, (Is that how you all understand this term?)\n",
    "# I8: I don't know, it's like, to me social mobility is like you can move uh, wherever that is, where it’s better. It's like, (Like move to where it's better?) Yeah, maybe like Australia, (Oh ok, like countries, move? Ok. So we have two different understandings then.)\n",
    "# I7: What I understand is that social mobility in, in this thing is that, if you go to a work place with a different nationality, actually Singaporeans are able to cope with them.\n",
    "# Mod: I see, so can include them, sure. Ok, you see you've heard 3 different definitions already, and that's fine. That's how you all understand social mobility. I'm not going to correct you all at this point, it's fine. Ok can already, everyone have 3 already? I'm just going to call that one by one, you tell me if it's in the top 3. [I4: I only have one actually.] Why? [I4: I don't know, because the rest all I don't see it as very fitting to our,]\n",
    "# I2: You must choose inclusive to, (Ok, maybe,) Must choose. [I4: Must choose ah?] Must choose,\n",
    "# Mod: Who’s having problems choosing 3? [I6: Actually I have.] Really? So many, all of you? Ok, choose 2 can or not? [I6: Can choose 1 or not?] Choose 1? You choose one more for me. Let's go for united people, who has this in your top 2? 3 of you, that's great. So everyone should put up their hands twice. Social equality, so you say this represents our society, ok I5, just 1 person. Democratic nation, anyone? [I5: Used to be.] Nobody. Social mobility, who chose that? 7. And inclusive society, who chose that? 5 of you. Ok let me tally the votes, 16, correct yes.\n",
    "# I1: I chose 3. (You chose 3? Then you chose one only?)\n",
    "# I3: I chose 1 only. (Which one you chose 3?)\n",
    "# I2: Inclusive society is the last one.\n",
    "# Mod: Ok, but never mind. Hey I7 and I8, am\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ee271d5-9bc5-4a1a-aaab-fa5864a39460",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)/main/tokenizer.json: 100%|███████████████████████████████████████████████████████████| 2.42M/2.42M [00:01<00:00, 1.95MB/s]\n"
     ]
    }
   ],
   "source": [
    "translator = ctranslate2.Translator(\"flan-t5-xl-ct2\")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"google/flan-t5-xl\", model_max_length=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10aad637-cd4b-421f-8ef2-248b7d80d30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* flower information * double rose * zones 5-11 * full sun or partial sun * blooms late spring through fall\n"
     ]
    }
   ],
   "source": [
    "input_text = f\"\"\"\n",
    "Answer the following questions based on the context. List the chain of thoughts that lead to the answer\n",
    "\n",
    "Context:\n",
    "\n",
    "DBS Group Research raised the target price of AEM’s shares to S$3.35. This is pegged to 10 times the company’s forecast FY2024 earnings and is close to its historical mean. \n",
    "\n",
    "Analyst Lee Keng Ling said on Tuesday (May 2) that the near term remains “fundamentally challenging” for the semiconductor solutions provider, as headwinds in the industry persist. Moreover, Intel, a key customer of AEM, posted lacklustre results for the first quarter of 2023 amid a data centre and personal computer (PC) slump. \n",
    "\n",
    "However, Lee noted that AEM’s inventories for Q1 have depleted significantly, which suggests that it is near the trough. \n",
    "\n",
    "Full-year guidance by chip industry bellwethers has also affirmed a rebound from the second half of this year.\n",
    "\n",
    "Furthermore, Intel’s Q2 revenue guidance indicates that the PC market is bottoming, although there could be some softness in server demand. \n",
    "\n",
    "“With the key customer prospects improving and increasing visibility on H2 2023, we now believe that AEM is likely to make upward revisions to its conservative guidance,” said Lee. \n",
    "\n",
    "The brokerage has revised its forecast earnings estimates for AEM upwards by 30 per cent for FY2023 as the company’s inventories and visibility improve.\n",
    "\n",
    "Beyond optimism of a recovering chip market, Lee noted that AEM is currently one generation ahead of its competitors in providing system-level test (SLT) solutions, and believes it is “well-positioned to ride on the growing SLT market”.\n",
    "\n",
    "He added that as new technology such as artificial intelligence drives growth in test spend, it would lead to greater demand for AEM’s offerings in the long term.\n",
    "\n",
    "Shares of AEM were trading up S$0.20 or 5.9 per cent at S$3.58 as at 2.11pm on Tuesday. \n",
    "\n",
    "Question: What is the key takeaway?\n",
    "\"\"\"\n",
    "\n",
    "input_text = f\"\"\"\n",
    "--Summarise the Conversation--\n",
    "\n",
    "{text}\n",
    "\n",
    "--Summary--\n",
    "\"\"\"\n",
    "\n",
    "# input_text = f\"\"\"\n",
    "# Expand the following points into complete sentences:\n",
    "\n",
    "# * flower information \n",
    "# * double rose \n",
    "# * zones 5-11 \n",
    "# * full sun or partial sun \n",
    "# * blooms late spring through fall\n",
    "\n",
    "# Output:\n",
    "# \"\"\"\n",
    "input_tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(input_text))\n",
    "\n",
    "results = translator.translate_batch([input_tokens], beam_size=1, no_repeat_ngram_size=3, min_decoding_length=10)\n",
    "\n",
    "output_tokens = results[0].hypotheses[0]\n",
    "output_text = tokenizer.decode(tokenizer.convert_tokens_to_ids(output_tokens))\n",
    "\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2707e33c-c4a0-42d0-9912-b448193f8d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = ctranslate2.Translator(\"meeting-summary-ct2\")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"knkarthick/MEETING_SUMMARY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "08c8e403-f7ec-4f43-925f-29766f38f673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mod, Tim, Mary, Peter, Jane, Chin and John are talking about living in Singapore. They are discussing the differences between the rich and poor in Singapore and the affluent and busybody society in which they live.    They are also discussing how people can be easily influenced by one another and how they can be stressed out by it.  They also discuss how people should spend their money more effectively. .  . They also talk about the need to focus on their financial well-being.\n",
      "CPU times: user 53.7 s, sys: 293 ms, total: 54 s\n",
      "Wall time: 14.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "source = tokenizer.convert_ids_to_tokens(tokenizer.encode(text))\n",
    "results = translator.translate_batch([source],\n",
    "                                     beam_size=5, no_repeat_ngram_size=3, min_decoding_length=100, max_decoding_length=200)\n",
    "target = results[0].hypotheses[0]\n",
    "\n",
    "print(tokenizer.decode(tokenizer.convert_tokens_to_ids(target), skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fd39398e-d4b3-4206-9672-3fa14854b57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"knkarthick/MEETING_SUMMARY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1aed1411-d36a-4d8e-a4fc-5f15bb6c29a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.3 s, sys: 81.3 ms, total: 31.4 s\n",
      "Wall time: 8.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'Mod, Tim, Mary, Peter and Chin are talking about living in Singapore. They are discussing the differences between the money conscious Singapore society and the busybody society.'}]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "summarizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12f62f28-4e30-4a3c-944c-0132a6e99003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m      \u001b[0msummarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mType:\u001b[0m           SummarizationPipeline\n",
       "\u001b[0;31mString form:\u001b[0m    <transformers.pipelines.text2text_generation.SummarizationPipeline object at 0x7fee04e58910>\n",
       "\u001b[0;31mFile:\u001b[0m           ~/.local/share/virtualenvs/dev-WhXEzAwp/lib/python3.9/site-packages/transformers/pipelines/text2text_generation.py\n",
       "\u001b[0;31mSource:\u001b[0m        \n",
       "\u001b[0;34m@\u001b[0m\u001b[0madd_end_docstrings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPIPELINE_INIT_ARGS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0mSummarizationPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mText2TextGenerationPipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n",
       "\u001b[0;34m    Summarize news articles and other documents.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    This summarizing pipeline can currently be loaded from [`pipeline`] using the following task identifier:\u001b[0m\n",
       "\u001b[0;34m    `\"summarization\"`.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    The models that this pipeline can use are models that have been fine-tuned on a summarization task, which is\u001b[0m\n",
       "\u001b[0;34m    currently, '*bart-large-cnn*', '*t5-small*', '*t5-base*', '*t5-large*', '*t5-3b*', '*t5-11b*'. See the up-to-date\u001b[0m\n",
       "\u001b[0;34m    list of available models on [huggingface.co/models](https://huggingface.co/models?filter=summarization). For a list\u001b[0m\n",
       "\u001b[0;34m    of available parameters, see the [following\u001b[0m\n",
       "\u001b[0;34m    documentation](https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.generation.GenerationMixin.generate)\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Usage:\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    ```python\u001b[0m\n",
       "\u001b[0;34m    # use bart in pytorch\u001b[0m\n",
       "\u001b[0;34m    summarizer = pipeline(\"summarization\")\u001b[0m\n",
       "\u001b[0;34m    summarizer(\"An apple a day, keeps the doctor away\", min_length=5, max_length=20)\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    # use t5 in tf\u001b[0m\n",
       "\u001b[0;34m    summarizer = pipeline(\"summarization\", model=\"t5-base\", tokenizer=\"t5-base\", framework=\"tf\")\u001b[0m\n",
       "\u001b[0;34m    summarizer(\"An apple a day, keeps the doctor away\", min_length=5, max_length=20)\u001b[0m\n",
       "\u001b[0;34m    ```\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m# Used in the return key of the pipeline.\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"summary\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34mr\"\"\"\u001b[0m\n",
       "\u001b[0;34m        Summarize the text(s) given as inputs.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        Args:\u001b[0m\n",
       "\u001b[0;34m            documents (*str* or `List[str]`):\u001b[0m\n",
       "\u001b[0;34m                One or several articles (or one list of articles) to summarize.\u001b[0m\n",
       "\u001b[0;34m            return_text (`bool`, *optional*, defaults to `True`):\u001b[0m\n",
       "\u001b[0;34m                Whether or not to include the decoded texts in the outputs\u001b[0m\n",
       "\u001b[0;34m            return_tensors (`bool`, *optional*, defaults to `False`):\u001b[0m\n",
       "\u001b[0;34m                Whether or not to include the tensors of predictions (as token indices) in the outputs.\u001b[0m\n",
       "\u001b[0;34m            clean_up_tokenization_spaces (`bool`, *optional*, defaults to `False`):\u001b[0m\n",
       "\u001b[0;34m                Whether or not to clean up the potential extra spaces in the text output.\u001b[0m\n",
       "\u001b[0;34m            generate_kwargs:\u001b[0m\n",
       "\u001b[0;34m                Additional keyword arguments to pass along to the generate method of the model (see the generate method\u001b[0m\n",
       "\u001b[0;34m                corresponding to your framework [here](./model#generative-models)).\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        Return:\u001b[0m\n",
       "\u001b[0;34m            A list or a list of list of `dict`: Each result comes as a dictionary with the following keys:\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m            - **summary_text** (`str`, present when `return_text=True`) -- The summary of the corresponding input.\u001b[0m\n",
       "\u001b[0;34m            - **summary_token_ids** (`torch.Tensor` or `tf.Tensor`, present when `return_tensors=True`) -- The token\u001b[0m\n",
       "\u001b[0;34m              ids of the summary.\u001b[0m\n",
       "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mcheck_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
       "\u001b[0;34m        Checks whether there might be something wrong with given input with regard to the model.\u001b[0m\n",
       "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmin_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Your min_length={min_length} must be inferior than your max_length={max_length}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0minput_length\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34mf\"Your max_length is set to {max_length}, but you input_length is only {input_length}. You might \"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34mf\"consider decreasing max_length manually, e.g. summarizer('...', max_length={input_length//2})\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mCall docstring:\u001b[0m\n",
       "Summarize the text(s) given as inputs.\n",
       "\n",
       "Args:\n",
       "    documents (*str* or `List[str]`):\n",
       "        One or several articles (or one list of articles) to summarize.\n",
       "    return_text (`bool`, *optional*, defaults to `True`):\n",
       "        Whether or not to include the decoded texts in the outputs\n",
       "    return_tensors (`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not to include the tensors of predictions (as token indices) in the outputs.\n",
       "    clean_up_tokenization_spaces (`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not to clean up the potential extra spaces in the text output.\n",
       "    generate_kwargs:\n",
       "        Additional keyword arguments to pass along to the generate method of the model (see the generate method\n",
       "        corresponding to your framework [here](./model#generative-models)).\n",
       "\n",
       "Return:\n",
       "    A list or a list of list of `dict`: Each result comes as a dictionary with the following keys:\n",
       "\n",
       "    - **summary_text** (`str`, present when `return_text=True`) -- The summary of the corresponding input.\n",
       "    - **summary_token_ids** (`torch.Tensor` or `tf.Tensor`, present when `return_tensors=True`) -- The token\n",
       "      ids of the summary."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summarizer??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f04987-43c1-4372-8b8a-8abd2d0df440",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
