{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15094082-ef6f-4923-a1bb-3a66116ebd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CT2_USE_MKL\"] = \"0\"\n",
    "os.environ[\"CT2_VERBOSE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0039febb-6edb-454e-a045-b5739949e9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-18 06:49:23.029] [ctranslate2] [thread 2910] [info] Loaded model /home/chiiyeh/.cache/huggingface/hub/models--guillaumekln--faster-whisper-small.en/snapshots/e0e3c0a16c844a994ca4d6d1318ce35f68236052 on device cpu:0\n",
      "[2023-04-18 06:49:23.029] [ctranslate2] [thread 2910] [info]  - Binary version: 6\n",
      "[2023-04-18 06:49:23.029] [ctranslate2] [thread 2910] [info]  - Model specification revision: 3\n",
      "[2023-04-18 06:49:23.029] [ctranslate2] [thread 2910] [info]  - Selected compute type: float32\n",
      "[2023-04-18 06:49:23.029] [ctranslate2] [thread 2910] [warning] The compute type inferred from the saved model is float16, but the target device or backend do not support efficient float16 computation. The model weights have been automatically converted to use the float32 compute type instead.\n"
     ]
    }
   ],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "\n",
    "model_size = \"small.en\"\n",
    "\n",
    "# Run on GPU with FP16\n",
    "model = WhisperModel(model_size, device=\"cpu\", cpu_threads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d015a2cb-9fa1-4b50-aec8-363cee1343fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language 'en' with probability 1.000000\n",
      "[0.00s -> 12.56s]  Welcome to My Friend Tell Me One. Today, we're going to speak to the Go Bros, two engineers\n",
      "[12.56s -> 13.96s]  who work in DevOps.\n",
      "[13.96s -> 19.84s]  Okay, hi, my name is Ryan. I work on my course feature. So basically, I'm doing DevOps.\n",
      "[20.08s -> 20.96s]  Same as the guy.\n",
      "[28.40s -> 34.64s]  The textbook answer is DevOps is a culture. So in other words, the whole team knows how to do\n",
      "[34.64s -> 39.84s]  operations, some parts of operations as well as development, giving the responsibility of\n",
      "[39.84s -> 42.40s]  what operators normally do to developers.\n",
      "[42.40s -> 46.24s]  It's kind of a wide term. So depending on where you go, different organizations,\n",
      "[46.24s -> 55.68s]  different teams have different definition of it. So yeah, so it's not a new thing. Actually,\n",
      "[55.68s -> 58.56s]  it's been around since 1993, since the Agile Manifesto came about.\n",
      "[62.32s -> 67.36s]  Like, the 10th learning is cool. School cannot teach us. But it costs two units, really.\n",
      "[67.36s -> 73.04s]  Basically, we have to run our own services. So by running it, you probably have to do a lot of\n",
      "[73.04s -> 76.00s]  things to get it running. So that's when we start learning.\n",
      "[103.04s -> 109.76s]  It means you can start growing your own vegetables and cooking them, so that you manage the whole\n",
      "[109.76s -> 112.40s]  thing end to end. Maybe? Do everything yourself.\n",
      "[112.40s -> 116.72s]  Okay, so previously, right, assuming you've got two people, so one person just cook, right,\n",
      "[116.72s -> 120.56s]  and pass everything to a fan. You don't care what. So let's say you cook the egg, right,\n",
      "[120.56s -> 123.36s]  you cook one giant piece, then you just give it to the person. Then the front person is like,\n",
      "[123.36s -> 127.52s]  wow, you have to cut up the piece, you see. So it's like double work. So now, right, it's like,\n",
      "[127.52s -> 130.16s]  I cook already, I cut up for you, you go to the fan, then I catch it more efficiently, right?\n",
      "[130.16s -> 131.68s]  Of course, you just take one slice, put it inside.\n",
      "[136.24s -> 140.80s]  Okay, so Micros Future is a platform created to reduce unemployment in local PMAs, especially\n",
      "[140.80s -> 144.88s]  those in the 30s and 40s who find themselves unemployed and they are unable to rejoin the\n",
      "[144.88s -> 150.48s]  workforce. So we do this through different ways. So instead of matching people to jobs,\n",
      "[150.48s -> 152.00s]  we match skills to jobs, for example.\n",
      "[156.24s -> 157.76s]  Yeah, definitely, best.\n",
      "[160.16s -> 164.72s]  Yeah, I'm going to ask you why you don't want to do this. How do you answer it?\n",
      "[164.72s -> 166.48s]  I don't know.\n",
      "[166.48s -> 167.20s]  I don't know.\n",
      "[167.20s -> 168.00s]  I don't know.\n",
      "[168.00s -> 169.44s]  I don't know. I don't know. One game, right?\n",
      "[169.44s -> 171.92s]  Oh, Coco, actually, no. No, sweet.\n",
      "[174.80s -> 179.12s]  Hi, my name is Ryan. I work in DevOps because I think it's cool, and I have a lot of control\n",
      "[179.12s -> 183.68s]  over the things that I work at. And I can play a Chinese instrument, the Erhu.\n",
      "[184.32s -> 188.72s]  So I'm Joseph, and I like DevOps because it's a lot more challenging than development work.\n",
      "[188.72s -> 191.44s]  So one interesting thing about me is that I practice martial arts,\n",
      "[191.44s -> 192.56s]  Wing Chun to be specific.\n",
      "Time: 56.83277916908264\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "segments, info = model.transcribe(\"GovTech_MFTMEP1_16000_mono_16.wav\", beam_size=5, language='en')\n",
    "\n",
    "print(\"Detected language '%s' with probability %f\" % (info.language, info.language_probability))\n",
    "\n",
    "for segment in segments:\n",
    "    print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n",
    "\n",
    "print(f'Time: {time.time() - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "078f60bd-b41b-4e54-872e-e482e39e9853",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'LD_LIBRARY_PATH'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLD_LIBRARY_PATH\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/os.py:679\u001b[0m, in \u001b[0;36m_Environ.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    676\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencodekey(key)]\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;66;03m# raise KeyError with the original key value\u001b[39;00m\n\u001b[0;32m--> 679\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecodevalue(value)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'LD_LIBRARY_PATH'"
     ]
    }
   ],
   "source": [
    "os.environ['LD_LIBRARY_PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b69d7d2-77a7-40a9-8a88-a5e160e2bf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "febc2641-521e-49a7-8d0f-4a0d4311b91f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chiiyeh/.local/share/virtualenvs/dev-WhXEzAwp/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from stream import WhisperModel\n",
    "\n",
    "model_size = \"small.en\"\n",
    "\n",
    "# Run on GPU with FP16\n",
    "model = WhisperModel(model_size, compute_type=\"int8\", device=\"cpu\", cpu_threads=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8ec425d-5340-4e0b-ab9c-d6aa5a9f3504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time offset: 0.0\n",
      "segment duration: 5.0\n",
      "time offset: 0.0\n",
      "segment duration: 8.0\n",
      "time offset: 0.0\n",
      "segment duration: 15.0\n",
      "time offset: 0.0\n",
      "segment duration: 5.0\n",
      "time offset: 0.0\n",
      "segment duration: 8.0\n",
      "time offset: 0.0\n",
      "segment duration: 15.0\n",
      "time offset: 0.0\n",
      "segment duration: 8.0\n",
      "time offset: 0.0\n",
      "segment duration: 8.0\n"
     ]
    }
   ],
   "source": [
    "tokenizer, options, stream_options = model.init_options(\n",
    "    beam_size=1,\n",
    "    compression_ratio_threshold=2.2,\n",
    "    condition_on_previous_text=False, \n",
    "    finalised_segment_gap=1, \n",
    "    word_timestamps=True, \n",
    "    without_timestamps=True,\n",
    "    drop_out_of_bound=False,\n",
    "    log_prob_threshold=None,\n",
    "    use_prefix=True,\n",
    "    prefix_drop_num_tokens = 5,\n",
    "    temperature=0, #[0,0.3,0.6,0.9],\n",
    "    max_tokens_per_second = 5,\n",
    "    max_prompt_tokens = 30,\n",
    "    propagate_initial_prompt = True,\n",
    "    initial_prompt = \"Two DevOps working on MyCareersFuture: \"\n",
    ")\n",
    "model.warm_start(tokenizer, options, stream_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c9eb77f-a4b8-44d6-9cc6-4fc00f9cd066",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time offset: 0.0\n",
      "segment duration: 7.0\n",
      "0.0 3.96 0.38461538461538464 0  Music\n",
      "Processing time: 3.0807308809962706\n",
      "time offset: 3.96\n",
      "segment duration: 10.040000000000001\n",
      "6.6 9.3 1.0891089108910892 0  Welcome to My Friend Tell Me One.\n",
      "Not Finalised: 9.66 13.62 1.0891089108910892 0  Today we're going to speak to the Go Bros, two engineers who work in DevOps.\n",
      "Processing time: 3.454692649000208\n",
      "time offset: 9.3\n",
      "segment duration: 11.700000000000001\n",
      "9.42 13.68 1.2887323943661972 0  Today we're going to speak to the Go Bros, two engineers who work in DevOps.\n",
      "14.28 15.56 1.2887323943661972 0  Okay, hi my name is Ryan.\n",
      "16.22 17.36 1.2887323943661972 0  I work on my course feature.\n",
      "18.14 19.42 1.2887323943661972 0  So basically I'm doing DevOps.\n",
      "Not Finalised: 20.32 20.64 1.2887323943661972 0  See you in a second.\n",
      "Processing time: 4.069438673999684\n",
      "time offset: 19.42\n",
      "segment duration: 8.58\n",
      "20.22 20.66 0.6923076923076923 0  See you next time.\n",
      "Processing time: 3.793759330001194\n",
      "time offset: 20.66\n",
      "segment duration: 14.34\n",
      "28.4 30.36 1.1145833333333333 0  Thanks for answering, DevOps is a culture.\n",
      "Not Finalised: 31.68 34.98 1.1145833333333333 0  So in other words, the whole team knows how to do a whole thing.\n",
      "Processing time: 3.9594660889997613\n",
      "time offset: 30.36\n",
      "segment duration: 11.64\n",
      "Not Finalised: 31.18 41.76 1.4365079365079365 0  So in other words, the whole team knows how to do operations, some parts of operations as well as development, giving the responsibility of what operators normally do to developers.\n",
      "Processing time: 3.7868298319954192\n",
      "time offset: 30.36\n",
      "segment duration: 18.64\n",
      "31.18 41.92 1.6282722513089005 0  So in other words, the whole team knows how to do operations, some parts of operations as well as development, giving the responsibility of what operators normally do to developers.\n",
      "Not Finalised: 42.44 48.84 1.6282722513089005 0  It's kind of a wide term, so depending on where you go, different organisations, different teams have different definition of it.\n",
      "Processing time: 4.263225410002633\n",
      "time offset: 41.92\n",
      "segment duration: 14.08\n",
      "42.36 49.64 1.4335664335664335 0  It's kind of a wide term, so depending on where you go, different organisations, different teams have different definition of it.\n",
      "49.82 54.62 1.4335664335664335 0  So non-taxable, it's like, come on.\n",
      "Not Finalised: 54.62 55.98 1.4335664335664335 0  It's not a new thing, actually it's me.\n",
      "Processing time: 4.459529284999007\n",
      "time offset: 54.620000000000005\n",
      "segment duration: 8.38\n",
      "54.92 58.52 1.0561797752808988 0  It's not a new thing, actually it's been around since 1993 since the Agile Manifesto came out.\n",
      "Processing time: 4.021188954000536\n",
      "time offset: 58.52\n",
      "segment duration: 11.48\n",
      "62.34 65.24 1.2040816326530612 0  I didn't learn it in school, school cannot teach us.\n",
      "65.7 66.88 1.2040816326530612 0  But it costs 2 units.\n",
      "Not Finalised: 67.46 69.76 1.2040816326530612 0  Basically, we have to run our own services.\n",
      "Processing time: 3.9132868079977925\n",
      "time offset: 66.88\n",
      "segment duration: 10.120000000000001\n",
      "67.36 73.86 1.3628318584070795 0  Basically, we have to run our own story so that by running it you probably have to do a lot of things to get it running.\n",
      "74.44 75.9 1.3628318584070795 0  So that's when we start learning.\n",
      "Processing time: 4.593076281998947\n",
      "time offset: 75.9\n",
      "segment duration: 8.1\n",
      "79.3 80.68 0.68 0  No, I don't know.\n",
      "Processing time: 3.6391399310014094\n",
      "time offset: 80.68\n",
      "segment duration: 10.32\n",
      "84.04 86.2 0.6923076923076923 0  I don't like that.\n",
      "Processing time: 3.88670477100095\n",
      "time offset: 86.2\n",
      "segment duration: 11.8\n",
      "Not Finalised: 86.74 88.1 5.705882352941177 0  I think it's a secret.\n",
      "Not Finalised: 89.04 90.8 5.705882352941177 0  I think it's a secret.\n",
      "Not Finalised: 90.8 91.44 5.705882352941177 0  I think it's a secret.\n",
      "Not Finalised: 91.66 92.02 5.705882352941177 0  I think it's a secret.\n",
      "Not Finalised: 92.48 95.14 5.705882352941177 0  I think it's a secret.\n",
      "Not Finalised: 95.14 95.16 5.705882352941177 0  I think it's a secret.\n",
      "Not Finalised: 95.16 96.12 5.705882352941177 0  I think it's a secret.\n",
      "Processing time: 5.1634198440006\n",
      "time offset: 86.2\n",
      "segment duration: 18.8\n",
      "Not Finalised: 86.66 88.1 2.3055555555555554 0  I think it's a secret.\n",
      "Not Finalised: 89.98 91.4 2.3055555555555554 0  I think it's a secret.\n",
      "Not Finalised: 91.72 92.0 2.3055555555555554 0  I think it's a secret.\n",
      "Not Finalised: 92.58 95.9 2.3055555555555554 0  It's a secret.\n",
      "Processing time: 4.749005227000453\n",
      "time offset: 86.2\n",
      "segment duration: 25.8\n",
      "104.28 110.54 1.3125 0  If me, you can start growing your own vegetables and cooking them so that you manage the whole thing end to end.\n",
      "110.76 110.84 1.3125 0  Maybe?\n",
      "Not Finalised: 111.44 111.98 1.3125 0  Do everything or something?\n",
      "Processing time: 4.378858700001729\n",
      "time offset: 110.84\n",
      "segment duration: 8.16\n",
      "Not Finalised: 110.84 118.98 0.7333333333333333 0  Do everything as well.\n",
      "Processing time: 4.2194716119993245\n",
      "time offset: 110.84\n",
      "segment duration: 15.16\n",
      "Not Finalised: 110.84 125.98 0.7333333333333333 0  Do everything as well.\n",
      "Processing time: 4.01782918599929\n",
      "time offset: 110.84\n",
      "segment duration: 22.16\n",
      "111.42 112.64 1.7068273092369477 0  Do everything as well.\n",
      "112.68 113.66 1.7068273092369477 0  Okay, so previously, right?\n",
      "114.3 116.54 1.7068273092369477 0  Assuming you've got two people, so one person just cook, right?\n",
      "116.92 117.9 1.7068273092369477 0  And pass everything to a fan.\n",
      "118.06 118.74 1.7068273092369477 0  You don't care.\n",
      "118.92 120.62 1.7068273092369477 0  So let's say you cook the egg, right?\n",
      "120.78 122.72 1.7068273092369477 0  You cook one giant piece, then you just give it to the person.\n",
      "122.8 125.14 1.7068273092369477 0  Then the front person is like, wow, he has to cut out the piece, you see?\n",
      "125.58 126.32 1.7068273092369477 0  So it's like double work.\n",
      "126.94 127.34 1.7068273092369477 0  So now, right?\n",
      "127.48 129.04 1.7068273092369477 0  I cook already, I cut out for you, you go to the\n",
      "Processing time: 6.0016316100009135\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-09 s\n",
       "\n",
       "Total time: 79.4953 s\n",
       "File: /home/chiiyeh/dev/stream.py\n",
       "Function: simulate_fixed_interval_streaming at line 355\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   355                                               def simulate_fixed_interval_streaming(\n",
       "   356                                                   self,\n",
       "   357                                                   audio_file: str,\n",
       "   358                                                   tokenizer: Tokenizer,\n",
       "   359                                                   options: TranscriptionOptions,\n",
       "   360                                                   stream_options: StreamOptions,\n",
       "   361                                                   interval: float = 3,\n",
       "   362                                               ):\n",
       "   363         1       1329.0   1329.0      0.0          sampling_rate = self.feature_extractor.sampling_rate\n",
       "   364         1   44172875.0 44172875.0      0.1          full_audio = decode_audio(audio_file, sampling_rate=sampling_rate)\n",
       "   365         1        835.0    835.0      0.0          audio_read = 0\n",
       "   366         1       3806.0   3806.0      0.0          import time\n",
       "   367                                           \n",
       "   368         1        394.0    394.0      0.0          idx = 0\n",
       "   369         1        406.0    406.0      0.0          seek = 0\n",
       "   370         1        664.0    664.0      0.0          previous_tokens = []\n",
       "   371         1        249.0    249.0      0.0          prefix_tokens = []\n",
       "   372         1        207.0    207.0      0.0          current_audio = None\n",
       "   373         1         94.0     94.0      0.0          temperature_idx = 0\n",
       "   374                                           \n",
       "   375         1       2179.0   2179.0      0.0          if options.initial_prompt is not None:\n",
       "   376                                                       initial_prompt = \" \" + options.initial_prompt.strip()\n",
       "   377                                                       initial_prompt_tokens = tokenizer.encode(initial_prompt)\n",
       "   378                                                       previous_tokens.extend(initial_prompt_tokens)\n",
       "   379                                           \n",
       "   380        19      11267.0    593.0      0.0          while audio_read < len(full_audio):\n",
       "   381        19       9102.0    479.1      0.0              current_time = time.perf_counter()\n",
       "   382        19      30437.0   1601.9      0.0              samples_read = round(interval * sampling_rate)\n",
       "   383        19      36372.0   1914.3      0.0              new_audio = full_audio[audio_read:min(audio_read+samples_read, len(full_audio))]\n",
       "   384        19       5934.0    312.3      0.0              audio_read += samples_read\n",
       "   385                                           \n",
       "   386                                           \n",
       "   387        18      17679.0    982.2      0.0              if current_audio is not None:\n",
       "   388        18    4445421.0 246967.8      0.0                  current_audio = np.concatenate([current_audio, new_audio])\n",
       "   389                                                       else:\n",
       "   390         1        136.0    136.0      0.0                  current_audio = new_audio\n",
       "   391                                           \n",
       "   392        19       7111.0    374.3      0.0              (\n",
       "   393        19      47848.0   2518.3      0.0                  output_segments,\n",
       "   394        19       3704.0    194.9      0.0                  current_audio,\n",
       "   395        19       3063.0    161.2      0.0                  idx,\n",
       "   396        19       3953.0    208.1      0.0                  seek,\n",
       "   397        19       3218.0    169.4      0.0                  temperature_idx,\n",
       "   398        19       4935.0    259.7      0.0                  previous_tokens,\n",
       "   399        19       5470.0    287.9      0.0                  prefix_tokens,\n",
       "   400        19 79444265302.0 4181277121.2     99.9              ) = self.generate_segments(\n",
       "   401        19       4874.0    256.5      0.0                  current_audio,\n",
       "   402        19       6226.0    327.7      0.0                  tokenizer,\n",
       "   403        19       4855.0    255.5      0.0                  options,\n",
       "   404        19       4145.0    218.2      0.0                  stream_options,\n",
       "   405        19       3900.0    205.3      0.0                  idx,\n",
       "   406        19       3511.0    184.8      0.0                  seek,\n",
       "   407        19       5181.0    272.7      0.0                  temperature_idx,\n",
       "   408        19       4788.0    252.0      0.0                  previous_tokens,\n",
       "   409        19       4097.0    215.6      0.0                  prefix_tokens\n",
       "   410                                                       )\n",
       "   411        52      87808.0   1688.6      0.0              for seg in output_segments:\n",
       "   412        31      38553.0   1243.6      0.0                  if seg.final:\n",
       "   413        31    1320443.0  42594.9      0.0                      print(f\"{seg.start} {seg.end} {seg.compression_ratio} {seg.temperature} {seg.text}\")\n",
       "   414                                                           else:\n",
       "   415        21     557695.0  26556.9      0.0                      print(f\"Not Finalised: {seg.start} {seg.end} {seg.compression_ratio} {seg.temperature} {seg.text}\")\n",
       "   416        19     155132.0   8164.8      0.0              print(f\"Processing time: {time.perf_counter() - current_time}\")\n",
       "   417        18      19826.0   1101.4      0.0              if time.perf_counter() - current_time > 6:\n",
       "   418         1        203.0    203.0      0.0                  break\n",
       "\n",
       "Total time: 79.4421 s\n",
       "File: /home/chiiyeh/dev/stream.py\n",
       "Function: generate_segments at line 435\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   435                                               def generate_segments(\n",
       "   436                                                   self,\n",
       "   437                                                   audio: np.ndarray,\n",
       "   438                                                   tokenizer: Tokenizer,\n",
       "   439                                                   options: TranscriptionOptions,\n",
       "   440                                                   stream_options: StreamOptions,\n",
       "   441                                                   idx: int,\n",
       "   442                                                   seek: int,\n",
       "   443                                                   temperature_idx: int,\n",
       "   444                                                   previous_tokens: List[int],\n",
       "   445                                                   prefix_tokens: List[int],\n",
       "   446                                               ):\n",
       "   447        19      12027.0    633.0      0.0          jump = 0\n",
       "   448        19       4650.0    244.7      0.0          output_segments = []\n",
       "   449        19      15773.0    830.2      0.0          max_initial_timestamp = options.max_initial_timestamp\n",
       "   450        19       2792.0    146.9      0.0          wipe_token = False\n",
       "   451        19 4295674928.0 226088154.1      5.4          features = self.feature_extractor(audio)\n",
       "   452                                           \n",
       "   453        19      10507.0    553.0      0.0          relative_content_frames = (\n",
       "   454        19     157876.0   8309.3      0.0              features.shape[-1] - self.feature_extractor.nb_max_frames\n",
       "   455                                                   )\n",
       "   456        19      97834.0   5149.2      0.0          time_offset = seek * self.feature_extractor.time_per_frame\n",
       "   457        19      86962.0   4576.9      0.0          segment = features[:, : self.feature_extractor.nb_max_frames]\n",
       "   458                                                   # to handle the case where the audio exceeds 30s\n",
       "   459        19      50518.0   2658.8      0.0          segment_size = min(\n",
       "   460        19       4560.0    240.0      0.0              self.feature_extractor.nb_max_frames, relative_content_frames\n",
       "   461                                                   )\n",
       "   462        19      13595.0    715.5      0.0          segment_duration = segment_size * self.feature_extractor.time_per_frame\n",
       "   463        19    2198622.0 115716.9      0.0          print(f\"time offset: {time_offset}\")\n",
       "   464        19     152043.0   8002.3      0.0          print(f\"segment duration: {segment_duration}\")\n",
       "   465                                           \n",
       "   466        19     118063.0   6213.8      0.0          if self.logger.isEnabledFor(logging.DEBUG):\n",
       "   467                                                       self.logger.debug(\"Processing segment at %s\", format_timestamp(time_offset))\n",
       "   468                                           \n",
       "   469                                                   # TODO: Can consider retaining the initial prompt\n",
       "   470        19     239479.0  12604.2      0.0          prompt = self.get_prompt(\n",
       "   471        19       8903.0    468.6      0.0              tokenizer,\n",
       "   472        19       6554.0    344.9      0.0              previous_tokens,\n",
       "   473        19       5498.0    289.4      0.0              stream_options,\n",
       "   474        19      39854.0   2097.6      0.0              without_timestamps=options.without_timestamps,\n",
       "   475        19       7455.0    392.4      0.0              prefix=options.prefix if seek == 0 else None,\n",
       "   476                                                   )\n",
       "   477        19      27150.0   1428.9      0.0          self.max_length = max(\n",
       "   478        19      55664.0   2929.7      0.0              round(stream_options.max_tokens_per_second*segment_duration)*2, \n",
       "   479        19      14899.0    784.2      0.0              len(prompt) + round(stream_options.max_tokens_per_second*segment_duration)\n",
       "   480                                                   )\n",
       "   481                                           \n",
       "   482        12      15777.0   1314.8      0.0          if stream_options.use_prefix and len(prefix_tokens) > stream_options.prefix_drop_num_tokens:\n",
       "   483         7      14330.0   2047.1      0.0              prompt.extend(prefix_tokens[:-stream_options.prefix_drop_num_tokens])\n",
       "   484         7       2104.0    300.6      0.0              max_initial_timestamp = 25\n",
       "   485                                           \n",
       "   486        19 50624298682.0 2664436772.7     63.7          encoder_output = self.encode(segment)\n",
       "   487                                           \n",
       "   488        19      16485.0    867.6      0.0          (\n",
       "   489        19       8416.0    442.9      0.0              result,\n",
       "   490        19       3396.0    178.7      0.0              avg_logprob,\n",
       "   491        19       2618.0    137.8      0.0              temperature,\n",
       "   492        19       5345.0    281.3      0.0              compression_ratio,\n",
       "   493        19       7670.0    403.7      0.0              needs_fallback,\n",
       "   494        19 17406883680.0 916151772.6     21.9          ) = self.generate_with_fallback(encoder_output, prompt, tokenizer, options, [self.temperatures[temperature_idx]], max_initial_timestamp)\n",
       "   495                                           \n",
       "   496        19      12849.0    676.3      0.0          if needs_fallback and temperature_idx+1 == len(self.temperatures) and segment_duration > stream_options.max_segment_duration:\n",
       "   497                                                       #No more temperature to fallback on\n",
       "   498                                                       needs_fallback = False\n",
       "   499                                                       wipe_token = True\n",
       "   500                                           \n",
       "   501        19      11655.0    613.4      0.0          if options.no_speech_threshold is not None:\n",
       "   502                                                       # no voice activity check\n",
       "   503        19      38949.0   2049.9      0.0              should_skip = result.no_speech_prob > options.no_speech_threshold\n",
       "   504                                           \n",
       "   505        19       4064.0    213.9      0.0              if (\n",
       "   506        19       4715.0    248.2      0.0                  options.log_prob_threshold is not None\n",
       "   507                                                           and avg_logprob > options.log_prob_threshold\n",
       "   508                                                       ):\n",
       "   509                                                           # don't skip if the logprob is high enough, despite the no_speech_prob\n",
       "   510                                                           should_skip = False\n",
       "   511                                           \n",
       "   512        19       3943.0    207.5      0.0              if should_skip:\n",
       "   513                                                           self.logger.debug(\n",
       "   514                                                               \"No speech threshold is met (%f > %f)\",\n",
       "   515                                                               result.no_speech_prob,\n",
       "   516                                                               options.no_speech_threshold,\n",
       "   517                                                           )\n",
       "   518                                           \n",
       "   519                                                           # fast-forward to the next segment boundary\n",
       "   520                                                           jump = segment_size - 1  # compensating by one to prevent drifting.\n",
       "   521                                                           seek = seek + jump\n",
       "   522                                                           audio = audio[\n",
       "   523                                                               min(jump * self.feature_extractor.hop_length, len(audio)) :\n",
       "   524                                                           ]\n",
       "   525                                           \n",
       "   526                                                           return (\n",
       "   527                                                               output_segments,\n",
       "   528                                                               audio,\n",
       "   529                                                               idx,\n",
       "   530                                                               seek,\n",
       "   531                                                               temperature_idx,\n",
       "   532                                                               previous_tokens,\n",
       "   533                                                               [],\n",
       "   534                                                           )\n",
       "   535                                           \n",
       "   536        19      30164.0   1587.6      0.0          tokens = result.sequences_ids[0]\n",
       "   537                                           \n",
       "   538        19       4623.0    243.3      0.0          current_segments = []\n",
       "   539                                           \n",
       "   540                                                   # TODO: Keeping the code flexible for different options at the cost of runtime, possible to remove chunks if restricting the options\n",
       "   541        19       4302.0    226.4      0.0          single_timestamp_ending = (\n",
       "   542        18      12879.0    715.5      0.0              len(tokens) >= 2\n",
       "   543        18      39665.0   2203.6      0.0              and tokens[-2] < tokenizer.timestamp_begin\n",
       "   544        18      11406.0    633.7      0.0              and tokens[-1] >= tokenizer.timestamp_begin\n",
       "   545                                                   )\n",
       "   546                                           \n",
       "   547        19     231339.0  12175.7      0.0          consecutive_timestamps = [\n",
       "   548                                                       i\n",
       "   549        19      33264.0   1750.7      0.0              for i in range(len(tokens))\n",
       "   550                                                       if i > 0\n",
       "   551                                                       and tokens[i] >= tokenizer.timestamp_begin\n",
       "   552                                                       and tokens[i - 1] >= tokenizer.timestamp_begin\n",
       "   553                                                   ]\n",
       "   554                                           \n",
       "   555        19      12791.0    673.2      0.0          if len(consecutive_timestamps) > 0:\n",
       "   556                                                       slices = list(consecutive_timestamps)\n",
       "   557                                                       if single_timestamp_ending:\n",
       "   558                                                           slices.append(len(tokens))\n",
       "   559                                           \n",
       "   560                                                       last_slice = 0\n",
       "   561                                                       for current_slice in slices:\n",
       "   562                                                           sliced_tokens = tokens[last_slice:current_slice]\n",
       "   563                                                           start_timestamp_position = sliced_tokens[0] - tokenizer.timestamp_begin\n",
       "   564                                                           end_timestamp_position = sliced_tokens[-1] - tokenizer.timestamp_begin\n",
       "   565                                                           start_time = (\n",
       "   566                                                               time_offset + start_timestamp_position * self.time_precision\n",
       "   567                                                           )\n",
       "   568                                                           end_time = time_offset + end_timestamp_position * self.time_precision\n",
       "   569                                                           #dropping segment with start time exceeding segment duration\n",
       "   570                                                           if stream_options.drop_out_of_bound and start_time > time_offset + segment_duration:\n",
       "   571                                                               print(\"THROWN\")\n",
       "   572                                                               break\n",
       "   573                                                           else:\n",
       "   574                                                               current_segments.append(\n",
       "   575                                                                   dict(\n",
       "   576                                                                       seek=seek,\n",
       "   577                                                                       start=start_time,\n",
       "   578                                                                       end=end_time,\n",
       "   579                                                                       tokens=sliced_tokens,\n",
       "   580                                                                   )\n",
       "   581                                                               )\n",
       "   582                                                               last_slice = current_slice\n",
       "   583                                                       if not single_timestamp_ending:\n",
       "   584                                                           sliced_tokens = tokens[last_slice:len(tokens)]\n",
       "   585                                                           start_timestamp_position = sliced_tokens[0] - tokenizer.timestamp_begin\n",
       "   586                                                           start_time = (\n",
       "   587                                                               time_offset + start_timestamp_position * self.time_precision\n",
       "   588                                                           )\n",
       "   589                                                           if start_time < time_offset + segment_duration or not stream_options.drop_out_of_bound:\n",
       "   590                                                               current_segments.append(\n",
       "   591                                                                   dict(\n",
       "   592                                                                       seek=seek,\n",
       "   593                                                                       start=start_time,\n",
       "   594                                                                       end=time_offset + segment_duration,\n",
       "   595                                                                       tokens=sliced_tokens,\n",
       "   596                                                                   )\n",
       "   597                                                               )\n",
       "   598                                                   else:\n",
       "   599        19       8560.0    450.5      0.0              duration = segment_duration\n",
       "   600        19     173410.0   9126.8      0.0              timestamps = [\n",
       "   601        19       6349.0    334.2      0.0                  token for token in tokens if token >= tokenizer.timestamp_begin\n",
       "   602                                                       ]\n",
       "   603        19       8069.0    424.7      0.0              if len(timestamps) > 0 and timestamps[-1] != tokenizer.timestamp_begin:\n",
       "   604                                                           last_timestamp_position = timestamps[-1] - tokenizer.timestamp_begin\n",
       "   605                                                           duration = last_timestamp_position * self.time_precision\n",
       "   606                                           \n",
       "   607        19      16349.0    860.5      0.0              current_segments.append(\n",
       "   608        19      37114.0   1953.4      0.0                  dict(\n",
       "   609        19      10668.0    561.5      0.0                      seek=seek,\n",
       "   610        19      11212.0    590.1      0.0                      start=time_offset,\n",
       "   611        19       9451.0    497.4      0.0                      end=time_offset + duration,\n",
       "   612        19       4711.0    247.9      0.0                      tokens=tokens,\n",
       "   613                                                           )\n",
       "   614                                                       )\n",
       "   615                                           \n",
       "   616        19      10188.0    536.2      0.0              jump = segment_size - 1  # compensating by one to prevent drifting.\n",
       "   617                                           \n",
       "   618        19      12851.0    676.4      0.0          if options.word_timestamps:\n",
       "   619        19 7107951199.0 374102694.7      8.9              self.add_word_timestamps(\n",
       "   620        19       7466.0    392.9      0.0                  current_segments,\n",
       "   621        19       6153.0    323.8      0.0                  tokenizer,\n",
       "   622        19       4902.0    258.0      0.0                  encoder_output,\n",
       "   623        19       5766.0    303.5      0.0                  segment_size,\n",
       "   624        19      11064.0    582.3      0.0                  options.prepend_punctuations,\n",
       "   625        19      10316.0    542.9      0.0                  options.append_punctuations,\n",
       "   626                                                       )\n",
       "   627                                           \n",
       "   628                                                   #For word timestamp only will split via sentences\n",
       "   629        19      39792.0   2094.3      0.0          if options.word_timestamps and options.without_timestamps:\n",
       "   630        19      16644.0    876.0      0.0              if len(current_segments) > 1:\n",
       "   631                                                           self.logger.error(\"Without timestamps but more than one segment\")\n",
       "   632        19      23209.0   1221.5      0.0              segment = current_segments.pop()\n",
       "   633                                                       # Form own segments based on punctuations\n",
       "   634        19     541468.0  28498.3      0.0              current_segments = self.get_segments_from_wordtimestamp(segment, seek)\n",
       "   635                                           \n",
       "   636        19       9891.0    520.6      0.0          last_final_segment_end = time_offset\n",
       "   637        54      27198.0    503.7      0.0          for segment in current_segments:\n",
       "   638        54      65819.0   1218.9      0.0              tokens = segment[\"tokens\"]\n",
       "   639        54     817204.0  15133.4      0.0              text = tokenizer.decode(tokens)\n",
       "   640                                           \n",
       "   641        52      51288.0    986.3      0.0              if segment[\"start\"] == segment[\"end\"] or not text.strip():\n",
       "   642                                                           continue\n",
       "   643                                           \n",
       "   644                                                       # need another variable to keep track of segments not finalised\n",
       "   645        52      16184.0    311.2      0.0              current_segment_count = idx\n",
       "   646        52      19295.0    371.1      0.0              prefix_tokens = []\n",
       "   647        31       9820.0    316.8      0.0              if (\n",
       "   648        52      31465.0    605.1      0.0                  final := segment[\"end\"] - time_offset\n",
       "   649        52      44186.0    849.7      0.0                  < segment_duration - stream_options.finalised_segment_gap\n",
       "   650        42      14754.0    351.3      0.0                  and not needs_fallback\n",
       "   651                                                       ):\n",
       "   652        31      11455.0    369.5      0.0                  last_final_segment_end = segment[\"end\"]\n",
       "   653        31      13084.0    422.1      0.0                  idx += 1\n",
       "   654        31      24322.0    784.6      0.0                  previous_tokens.extend(tokens)\n",
       "   655        21      18161.0    864.8      0.0              elif final := segment[\"end\"] - segment[\"start\"] > stream_options.max_segment_duration and not needs_fallback:\n",
       "   656                                                           last_final_segment_end = segment[\"end\"]\n",
       "   657                                                           idx += 1\n",
       "   658                                                           previous_tokens.extend(tokens)\n",
       "   659                                                       else:\n",
       "   660        21      17463.0    831.6      0.0                  if options.word_timestamps and stream_options.drop_out_of_bound:\n",
       "   661                                                               segment[\"words\"] = [word for word in segment[\"words\"] if word[\"start\"] < time_offset+segment_duration]\n",
       "   662                                                               text = \"\".join([word[\"word\"] for word in segment[\"words\"]]).strip()\n",
       "   663                                                               if tokens[0] >= tokenizer.timestamp_begin:\n",
       "   664                                                                   tokens = [tokens[0]]\n",
       "   665                                                               else:\n",
       "   666                                                                   tokens = []\n",
       "   667                                                               for word in segment[\"words\"]:\n",
       "   668                                                                   tokens += word[\"tokens\"]\n",
       "   669                                                               if not text:\n",
       "   670                                                                   break\n",
       "   671        11       4791.0    435.5      0.0                  if stream_options.use_prefix and not needs_fallback and not wipe_token:\n",
       "   672        10       5454.0    545.4      0.0                      if not options.without_timestamps :\n",
       "   673                                                                   tokens[0] = tokens[0] - round((last_final_segment_end-time_offset)/self.time_precision)\n",
       "   674                                                                   if tokens[-1] >= tokenizer.timestamp_begin:\n",
       "   675                                                                       tokens[-1] = tokens[-1] - round((last_final_segment_end-time_offset)/self.time_precision)\n",
       "   676        10       6548.0    654.8      0.0                      prefix_tokens.extend(tokens)\n",
       "   677        52      21371.0    411.0      0.0              current_segment_count += 1\n",
       "   678                                                       #! Can return to client here instead of outputting it\n",
       "   679        52      26493.0    509.5      0.0              output_segments.append(\n",
       "   680        52     119300.0   2294.2      0.0                  StreamSegment(\n",
       "   681        52      16825.0    323.6      0.0                      id=current_segment_count,\n",
       "   682        52      19587.0    376.7      0.0                      start=segment[\"start\"],\n",
       "   683        52      17846.0    343.2      0.0                      end=segment[\"end\"],\n",
       "   684        52      16762.0    322.3      0.0                      text=text,\n",
       "   685        52      17218.0    331.1      0.0                      temperature=temperature,\n",
       "   686        52      18949.0    364.4      0.0                      avg_logprob=avg_logprob,\n",
       "   687        52      20745.0    398.9      0.0                      compression_ratio=compression_ratio,\n",
       "   688        52     121544.0   2337.4      0.0                      no_speech_prob=result.no_speech_prob,\n",
       "   689                                                               words=(\n",
       "   690        52     486355.0   9353.0      0.0                          [Word(**word) for word in segment[\"words\"]]\n",
       "   691        52      22500.0    432.7      0.0                          if options.word_timestamps\n",
       "   692                                                                   else None\n",
       "   693                                                               ),\n",
       "   694        52      17570.0    337.9      0.0                      final=final,\n",
       "   695                                                           )\n",
       "   696                                                       )\n",
       "   697                                                       # Keeping the audio for those segment not finalised\n",
       "   698                                                       # TODO: Possible to add option keep only those words within the segment_end_thres\n",
       "   699                                                       # TODO: to reduce audio kept and lesser token generated will need a var to keep track\n",
       "   700                                                       # TODO: of the unfinalised tokens and add them to the prompt. Do this only when needs_fallback is False.\n",
       "   701                                           \n",
       "   702                                                       # TODO: If options.without_timestamp is False and single_timestamp_ending is True should ensure the last segment final=False\n",
       "   703        52     124788.0   2399.8      0.0              seek_shift = round(\n",
       "   704        52      40488.0    778.6      0.0                  (last_final_segment_end - time_offset) * self.frames_per_second\n",
       "   705                                                       )\n",
       "   706                                           \n",
       "   707        38      15612.0    410.8      0.0              if seek_shift > 0:\n",
       "   708        38      23153.0    609.3      0.0                  jump = seek_shift\n",
       "   709                                                       else:\n",
       "   710        14       4990.0    356.4      0.0                  jump = 0\n",
       "   711                                           \n",
       "   712        19      13583.0    714.9      0.0          if not options.condition_on_previous_text or temperature > 0.5 or wipe_token:\n",
       "   713        19       7399.0    389.4      0.0              previous_tokens = []\n",
       "   714                                           \n",
       "   715        17       7168.0    421.6      0.0          if needs_fallback:\n",
       "   716         2       3309.0   1654.5      0.0              temperature_idx = min(len(self.temperatures) - 1, temperature_idx + 1)\n",
       "   717                                                   else:\n",
       "   718        17       6212.0    365.4      0.0              temperature_idx = 0\n",
       "   719                                           \n",
       "   720                                                   # Need to discard audio that is finalized and add in need audio\n",
       "   721                                                   # Check if text occurred in the added silence\n",
       "   722        19       8387.0    441.4      0.0          if jump >= segment_size:\n",
       "   723                                                       jump = segment_size - 1\n",
       "   724        19       8799.0    463.1      0.0          seek = seek + jump\n",
       "   725        19      50868.0   2677.3      0.0          audio = audio[min(jump * self.feature_extractor.hop_length, len(audio)) :]\n",
       "   726                                           \n",
       "   727        19       6442.0    339.1      0.0          return output_segments, audio, idx, seek, temperature_idx, previous_tokens, prefix_tokens"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f model.simulate_fixed_interval_streaming -f model.generate_segments model.simulate_fixed_interval_streaming(\"GovTech_MFTMEP1_16000_mono_16.wav\", tokenizer, options, stream_options, interval=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb986b47-6482-4bd9-bb14-8cec698fe4d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50362"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.no_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3863e7e-7115-491f-9755-718f62cf43cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50363"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.timestamp_begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "669c2353-7723-4473-8dde-3042af3b80aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc368623-79f6-4fd6-b41a-8dcb8a81fa61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50360"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sot_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d1cdc5d-2a1a-4469-85b8-24fd7ac58102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time offset: 0.0\n",
      "segment duration: 5.0\n",
      "time offset: 0.0\n",
      "segment duration: 8.0\n",
      "time offset: 0.0\n",
      "segment duration: 15.0\n",
      "time offset: 0.0\n",
      "segment duration: 5.0\n",
      "time offset: 0.0\n",
      "segment duration: 8.0\n",
      "time offset: 0.0\n",
      "segment duration: 15.0\n",
      "time offset: 0.0\n",
      "segment duration: 5.0\n"
     ]
    }
   ],
   "source": [
    "tokenizer, options, stream_options = model.init_options(\n",
    "    beam_size=1, \n",
    "    condition_on_previous_text=False, \n",
    "    finalised_segment_gap=2, \n",
    "    word_timestamps=True, \n",
    "    without_timestamps=True,\n",
    "    drop_out_of_bound=False,\n",
    "    log_prob_threshold=None,\n",
    "    use_prefix=False,\n",
    "    prefix_drop_num_tokens = 5,\n",
    "    temperature=0,\n",
    ")\n",
    "model.max_length=448\n",
    "model.warm_start(tokenizer, options, stream_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbb3a6d9-a252-4271-a3d3-3c3626773c02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time offset: 0.0\n",
      "segment duration: 7.0\n",
      "0.0 3.96 0  Music\n",
      "Processing time: 3.432962167004007\n",
      "time offset: 3.96\n",
      "segment duration: 10.040000000000001\n",
      "6.6 9.3 0  Welcome to My Friend Tell Me One.\n",
      "Not Finalised: 9.66 13.62 0  Today we're going to speak to the Go Bros, two engineers who work in DevOps.\n",
      "Processing time: 4.15741924699978\n",
      "time offset: 9.3\n",
      "segment duration: 11.700000000000001\n",
      "9.42 13.68 0  Today we're going to speak to the Go Bros, two engineers who work in DevOps.\n",
      "14.28 15.56 0  Okay, hi my name is Ryan.\n",
      "16.22 17.36 0  I work on my course feature.\n",
      "Not Finalised: 18.14 19.42 0  So basically I'm doing DevOps.\n",
      "Not Finalised: 20.32 20.64 0  See you in a second.\n",
      "Processing time: 4.40155586299079\n",
      "time offset: 17.36\n",
      "segment duration: 10.64\n",
      "17.68 19.38 0  So basically I'm doing Death Ops.\n",
      "Processing time: 3.637236599010066\n",
      "time offset: 19.38\n",
      "segment duration: 15.620000000000001\n",
      "28.3 30.34 0  Thanks for answering, DevOps is a culture.\n",
      "Not Finalised: 31.7 34.68 0  So in other words, the whole team knows how to do\n",
      "Processing time: 4.136735190011677\n",
      "time offset: 30.34\n",
      "segment duration: 11.66\n",
      "Not Finalised: 31.18 41.76 0  So in other words, the whole team knows how to do operations, some parts of operations as well as development, giving the responsibility of what operators normally do to developers.\n",
      "Processing time: 4.200496573001146\n",
      "time offset: 30.34\n",
      "segment duration: 18.66\n",
      "31.22 41.94 0  So in other words, the whole team knows how to do operations, some parts of operations as well as development, giving the responsibility of what operators normally do to developers.\n",
      "Not Finalised: 42.6 48.84 0  It's kind of a wide term, so depending on where you go, different organisations, different teams have different definition of it.\n",
      "Processing time: 4.7496789950091625\n",
      "time offset: 41.94\n",
      "segment duration: 14.06\n",
      "42.36 49.66 0  It's kind of a wide term, so depending on where you go, different organisations, different teams have different definition of it.\n",
      "49.82 53.98 0  So non-taxable, it's like, come on.\n",
      "Not Finalised: 54.1 55.98 0  It's not a new thing, actually it's me.\n",
      "Processing time: 4.528177917003632\n",
      "time offset: 53.980000000000004\n",
      "segment duration: 9.02\n",
      "54.92 55.56 0  It's not a new thing.\n",
      "55.72 56.88 0  Actually it's been around since 1993.\n",
      "57.34 58.52 0  Since the Agile Manifesto came out.\n",
      "Processing time: 4.01339267998992\n",
      "time offset: 58.52\n",
      "segment duration: 11.48\n",
      "62.34 65.24 0  I didn't learn it in school, school cannot teach us.\n",
      "65.7 66.88 0  But it costs 2 units.\n",
      "Not Finalised: 67.46 69.76 0  Basically, we have to run our own services.\n",
      "Processing time: 4.306875886002672\n",
      "time offset: 66.88\n",
      "segment duration: 10.120000000000001\n",
      "67.36 73.86 0  Basically, we have to run our own story so that by running it you probably have to do a lot of things to get it running.\n",
      "Not Finalised: 74.44 75.9 0  So that's when we start learning.\n",
      "Processing time: 4.544810335995862\n",
      "time offset: 73.86\n",
      "segment duration: 10.14\n",
      "73.98 75.94 0  So that's when we start learning.\n",
      "Processing time: 3.7937914280046243\n",
      "time offset: 75.94\n",
      "segment duration: 15.06\n",
      "79.4 80.08 0  No one.\n",
      "80.22 80.44 0  No one.\n",
      "80.56 86.14 0  I don't like that.\n",
      "87.26 87.38 0  I think.\n",
      "87.86 87.92 0  Six.\n",
      "Not Finalised: 88.5 89.5 0  No, I'm saying yeah.\n",
      "Not Finalised: 89.8 90.56 0  I say yeah.\n",
      "Processing time: 4.264664253001683\n",
      "time offset: 87.92\n",
      "segment duration: 10.08\n",
      "88.12 89.38 0  I'm not sure.\n",
      "89.98 90.34 0  I'm not sure.\n",
      "91.72 92.28 0  I'm not sure.\n",
      "Not Finalised: 92.92 96.76 0  I'm not sure.\n",
      "Processing time: 3.9533442149986513\n",
      "time offset: 92.28\n",
      "segment duration: 12.72\n",
      "Not Finalised: 92.28 103.42 0  It\n",
      "Processing time: 3.5257061529991915\n",
      "time offset: 92.28\n",
      "segment duration: 19.72\n",
      "Not Finalised: 104.3 111.98 0  If you can start growing your own vegetables and cooking them so that you manage the whole thing end to end maybe do everything was\n",
      "Processing time: 4.190273567990516\n",
      "time offset: 92.28\n",
      "segment duration: 26.72\n",
      "101.68 110.6 0  You can start growing your own vegetables and cooking them so that you manage the whole thing end to end.\n",
      "110.76 110.84 0  Maybe?\n",
      "111.42 112.3 0  Do everything yourself.\n",
      "112.68 113.68 0  Okay, so previously, right?\n",
      "Not Finalised: 114.36 117.92 0  Assuming you've got two people, so one person just cook right and pass everything to a friend.\n",
      "Not Finalised: 118.06 118.98 0  You don't care what now.\n",
      "Processing time: 4.937275871998281\n",
      "time offset: 113.68\n",
      "segment duration: 12.32\n",
      "114.28 117.92 0  Assuming you got 2 people, so one person just coat right and pass everything to a fan.\n",
      "118.06 119.14 0  He don't care what not.\n",
      "119.64 122.74 0  So let's say you coat the egg right, you coat one giant piece, then you just give it to the person.\n",
      "Not Finalised: 122.82 125.14 0  Then the front person is like, wow, he has to cut out the piece, you see.\n",
      "Not Finalised: 125.6 125.92 0  So it's like,\n",
      "Processing time: 5.121345010993537\n",
      "time offset: 122.74000000000001\n",
      "segment duration: 10.26\n",
      "122.74 125.04 0  And then the front person is like, what, you have to cut up the PCC.\n",
      "125.62 126.34 0  So it's like double work.\n",
      "126.98 130.26 0  So now, right, I cut up for you, you go to the front, then I cash in more efficient, right?\n",
      "Not Finalised: 130.4 131.62 0  Of course, you just take one slice, put it inside.\n",
      "Processing time: 5.081566603999818\n",
      "time offset: 130.26\n",
      "segment duration: 9.74\n",
      "130.26 132.04 0  Because you just take one slice, put it inside.\n",
      "Not Finalised: 133.06 139.98 0  So Micros Future is a platform created to reduce unemployment in local countries.\n",
      "Processing time: 4.962434749002568\n",
      "time offset: 132.04\n",
      "segment duration: 14.96\n",
      "Not Finalised: 135.8 145.3 0  Okay, so my current future is a platform created to reduce unemployment in local PMAs, especially those in the 30s and 40s who find themselves unemployed and they are unable to rejoin the workforce.\n",
      "Processing time: 4.286627130000852\n",
      "time offset: 132.04\n",
      "segment duration: 21.96\n",
      "136.06 145.3 0  Okay so my career's future is a platform created to reduce unemployment in local PMAs, especially those in the 30s and 40s who find themselves unemployed and they are unable to rejoin the workforce.\n",
      "146.14 151.86 0  So we do this through different ways, so instead of matching people's jobs, we match skills to jobs for example.\n",
      "Processing time: 4.948508244007826\n",
      "time offset: 151.86\n",
      "segment duration: 9.14\n",
      "Not Finalised: 156.02 160.98 0  I'm going to do a little bit of this.\n",
      "Processing time: 3.706547073001275\n",
      "time offset: 151.86\n",
      "segment duration: 16.14\n",
      "159.62 163.0 0  I'm going to ask you to ask me about your hair.\n",
      "163.84 164.54 0  How are your arms?\n",
      "Not Finalised: 165.24 166.16 0  It's okay.\n",
      "Not Finalised: 166.64 167.02 0  It's okay.\n",
      "Processing time: 4.15925098900334\n",
      "time offset: 164.54\n",
      "segment duration: 10.46\n",
      "164.54 164.76 0  I'm not sure.\n",
      "165.62 166.2 0  I'm not sure.\n",
      "167.5 167.98 0  I'm not sure.\n",
      "167.98 168.5 0  I'm not sure.\n",
      "169.5 170.04 0  I'm not sure.\n",
      "Processing time: 4.111245559994131\n",
      "time offset: 170.04\n",
      "segment duration: 11.96\n",
      "170.04 170.34 0  I'm not a pro at all.\n",
      "Not Finalised: 170.9 181.96 0  I'm not a pro at all.\n",
      "Processing time: 3.8086914789892035\n",
      "time offset: 170.34\n",
      "segment duration: 18.66\n",
      "174.32 175.62 0  Hi, my name is Ryan.\n",
      "176.18 180.56 0  I work in DevOps because I think it's cool and I have a lot of control over the things I work at.\n",
      "181.18 183.76 0  I can play a Chinese instrument at the earthwool.\n",
      "Not Finalised: 184.44 188.58 0  So I'm Joseph and I like DevOps because it's a lot more challenging than development work.\n",
      "Processing time: 4.972597048996249\n",
      "time offset: 183.76\n",
      "segment duration: 12.24\n",
      "184.14 188.6 0  So I'm Joseph and I like DevOps because it's a lot more challenging than development work.\n",
      "189.04 192.44 0  So one interesting thing about me is that I practice martial arts, going to be specific.\n",
      "Processing time: 4.2405496849969495\n",
      "time offset: 192.44\n",
      "segment duration: 6.36\n",
      "Not Finalised: 192.44 198.78 0  Thank you.\n",
      "Processing time: 3.6165037870086962\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-09 s\n",
       "\n",
       "Total time: 123.826 s\n",
       "File: /home/chiiyeh/dev/stream.py\n",
       "Function: simulate_fixed_interval_streaming at line 342\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   342                                               def simulate_fixed_interval_streaming(\n",
       "   343                                                   self,\n",
       "   344                                                   audio_file: str,\n",
       "   345                                                   tokenizer: Tokenizer,\n",
       "   346                                                   options: TranscriptionOptions,\n",
       "   347                                                   stream_options: StreamOptions,\n",
       "   348                                                   interval: float = 3,\n",
       "   349                                               ):\n",
       "   350         1       2330.0   2330.0      0.0          sampling_rate = self.feature_extractor.sampling_rate\n",
       "   351         1   36390517.0 36390517.0      0.0          full_audio = decode_audio(audio_file, sampling_rate=sampling_rate)\n",
       "   352         1        268.0    268.0      0.0          audio_read = 0\n",
       "   353         1       3216.0   3216.0      0.0          import time\n",
       "   354                                           \n",
       "   355         1        324.0    324.0      0.0          idx = 0\n",
       "   356         1        431.0    431.0      0.0          seek = 0\n",
       "   357         1        525.0    525.0      0.0          previous_tokens = []\n",
       "   358         1        334.0    334.0      0.0          prefix_tokens = []\n",
       "   359         1        475.0    475.0      0.0          current_audio = None\n",
       "   360         1        256.0    256.0      0.0          temperature_idx = 0\n",
       "   361                                           \n",
       "   362         1       2582.0   2582.0      0.0          if options.initial_prompt is not None:\n",
       "   363                                                       initial_prompt = \" \" + options.initial_prompt.strip()\n",
       "   364                                                       initial_prompt_tokens = tokenizer.encode(initial_prompt)\n",
       "   365                                                       previous_tokens.extend(initial_prompt_tokens)\n",
       "   366                                           \n",
       "   367        29      26256.0    905.4      0.0          while audio_read < len(full_audio):\n",
       "   368        29      15090.0    520.3      0.0              current_time = time.perf_counter()\n",
       "   369        29      57958.0   1998.6      0.0              samples_read = round(interval * sampling_rate)\n",
       "   370        29      67006.0   2310.6      0.0              new_audio = full_audio[audio_read:min(audio_read+samples_read, len(full_audio))]\n",
       "   371        29      10468.0    361.0      0.0              audio_read += samples_read\n",
       "   372                                           \n",
       "   373                                           \n",
       "   374        28       7375.0    263.4      0.0              if current_audio is not None:\n",
       "   375        28   10441976.0 372927.7      0.0                  current_audio = np.concatenate([current_audio, new_audio])\n",
       "   376                                                       else:\n",
       "   377         1        452.0    452.0      0.0                  current_audio = new_audio\n",
       "   378                                           \n",
       "   379        29      10858.0    374.4      0.0              (\n",
       "   380        29     116781.0   4026.9      0.0                  output_segments,\n",
       "   381        29       7236.0    249.5      0.0                  current_audio,\n",
       "   382        29       6272.0    216.3      0.0                  idx,\n",
       "   383        29       8953.0    308.7      0.0                  seek,\n",
       "   384        29       6467.0    223.0      0.0                  temperature_idx,\n",
       "   385        29      10080.0    347.6      0.0                  previous_tokens,\n",
       "   386        29       8054.0    277.7      0.0                  prefix_tokens,\n",
       "   387        29 123775078162.0 4268106143.5    100.0              ) = self.generate_segments(\n",
       "   388        29      11203.0    386.3      0.0                  current_audio,\n",
       "   389        29      17406.0    600.2      0.0                  tokenizer,\n",
       "   390        29      14683.0    506.3      0.0                  options,\n",
       "   391        29      10568.0    364.4      0.0                  stream_options,\n",
       "   392        29      13844.0    477.4      0.0                  idx,\n",
       "   393        29      11393.0    392.9      0.0                  seek,\n",
       "   394        29      10894.0    375.7      0.0                  temperature_idx,\n",
       "   395        29       9316.0    321.2      0.0                  previous_tokens,\n",
       "   396        29       8042.0    277.3      0.0                  prefix_tokens\n",
       "   397                                                       )\n",
       "   398        78      77708.0    996.3      0.0              for seg in output_segments:\n",
       "   399        51      22406.0    439.3      0.0                  if seg.final:\n",
       "   400        51    2791127.0  54728.0      0.0                      print(f\"{seg.start} {seg.end} {seg.temperature} {seg.text}\")\n",
       "   401                                                           else:\n",
       "   402        27     793987.0  29406.9      0.0                      print(f\"Not Finalised: {seg.start} {seg.end} {seg.temperature} {seg.text}\")\n",
       "   403        29     279286.0   9630.6      0.0              print(f\"Processing time: {time.perf_counter() - current_time}\")\n",
       "\n",
       "Total time: 123.772 s\n",
       "File: /home/chiiyeh/dev/stream.py\n",
       "Function: generate_segments at line 419\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   419                                               def generate_segments(\n",
       "   420                                                   self,\n",
       "   421                                                   audio: np.ndarray,\n",
       "   422                                                   tokenizer: Tokenizer,\n",
       "   423                                                   options: TranscriptionOptions,\n",
       "   424                                                   stream_options: StreamOptions,\n",
       "   425                                                   idx: int,\n",
       "   426                                                   seek: int,\n",
       "   427                                                   temperature_idx: int,\n",
       "   428                                                   previous_tokens: List[int],\n",
       "   429                                                   prefix_tokens: List[int],\n",
       "   430                                               ):\n",
       "   431        29      21929.0    756.2      0.0          jump = 0\n",
       "   432        29      13042.0    449.7      0.0          output_segments = []\n",
       "   433        29      31591.0   1089.3      0.0          max_initial_timestamp = options.max_initial_timestamp\n",
       "   434        29       5851.0    201.8      0.0          wipe_token = False\n",
       "   435        29 7073683596.0 243920124.0      5.7          features = self.feature_extractor(audio)\n",
       "   436                                           \n",
       "   437        29      12648.0    436.1      0.0          relative_content_frames = (\n",
       "   438        29     177272.0   6112.8      0.0              features.shape[-1] - self.feature_extractor.nb_max_frames\n",
       "   439                                                   )\n",
       "   440        29     102689.0   3541.0      0.0          time_offset = seek * self.feature_extractor.time_per_frame\n",
       "   441        29     150400.0   5186.2      0.0          segment = features[:, : self.feature_extractor.nb_max_frames]\n",
       "   442                                                   # to handle the case where the audio exceeds 30s\n",
       "   443        29      79739.0   2749.6      0.0          segment_size = min(\n",
       "   444        29       6942.0    239.4      0.0              self.feature_extractor.nb_max_frames, relative_content_frames\n",
       "   445                                                   )\n",
       "   446        29      17702.0    610.4      0.0          segment_duration = segment_size * self.feature_extractor.time_per_frame\n",
       "   447        29    2889574.0  99640.5      0.0          print(f\"time offset: {time_offset}\")\n",
       "   448        29     195919.0   6755.8      0.0          print(f\"segment duration: {segment_duration}\")\n",
       "   449                                           \n",
       "   450        29     168916.0   5824.7      0.0          if self.logger.isEnabledFor(logging.DEBUG):\n",
       "   451                                                       self.logger.debug(\"Processing segment at %s\", format_timestamp(time_offset))\n",
       "   452                                           \n",
       "   453                                                   # TODO: Can consider retaining the initial prompt\n",
       "   454        29     252857.0   8719.2      0.0          prompt = self.get_prompt(\n",
       "   455        29      16223.0    559.4      0.0              tokenizer,\n",
       "   456        29      18156.0    626.1      0.0              previous_tokens,\n",
       "   457        29      45748.0   1577.5      0.0              without_timestamps=options.without_timestamps,\n",
       "   458        29       8810.0    303.8      0.0              prefix=options.prefix if seek == 0 else None,\n",
       "   459                                                   )\n",
       "   460                                           \n",
       "   461        29      28911.0    996.9      0.0          if stream_options.use_prefix and len(prefix_tokens) > stream_options.prefix_drop_num_tokens:\n",
       "   462                                                       prompt.extend(prefix_tokens[:-stream_options.prefix_drop_num_tokens])\n",
       "   463                                                       max_initial_timestamp = 25\n",
       "   464                                           \n",
       "   465        29 82438614427.0 2842710842.3     66.6          encoder_output = self.encode(segment)\n",
       "   466                                           \n",
       "   467        29      28197.0    972.3      0.0          (\n",
       "   468        29      24521.0    845.6      0.0              result,\n",
       "   469        29       4858.0    167.5      0.0              avg_logprob,\n",
       "   470        29      18096.0    624.0      0.0              temperature,\n",
       "   471        29      11511.0    396.9      0.0              compression_ratio,\n",
       "   472        29      15095.0    520.5      0.0              needs_fallback,\n",
       "   473        29 24917424017.0 859221517.8     20.1          ) = self.generate_with_fallback(encoder_output, prompt, tokenizer, options, [self.temperatures[temperature_idx]], max_initial_timestamp)\n",
       "   474                                           \n",
       "   475        28      12132.0    433.3      0.0          if needs_fallback and temperature_idx+1 == len(self.temperatures):\n",
       "   476                                                       #No more temperature to fallback on\n",
       "   477         1        436.0    436.0      0.0              needs_fallback = False\n",
       "   478         1        320.0    320.0      0.0              wipe_token = True\n",
       "   479                                           \n",
       "   480        29      25821.0    890.4      0.0          if options.no_speech_threshold is not None:\n",
       "   481                                                       # no voice activity check\n",
       "   482        29      59403.0   2048.4      0.0              should_skip = result.no_speech_prob > options.no_speech_threshold\n",
       "   483                                           \n",
       "   484        29      17291.0    596.2      0.0              if (\n",
       "   485        29      17602.0    607.0      0.0                  options.log_prob_threshold is not None\n",
       "   486                                                           and avg_logprob > options.log_prob_threshold\n",
       "   487                                                       ):\n",
       "   488                                                           # don't skip if the logprob is high enough, despite the no_speech_prob\n",
       "   489                                                           should_skip = False\n",
       "   490                                           \n",
       "   491        29      11346.0    391.2      0.0              if should_skip:\n",
       "   492                                                           self.logger.debug(\n",
       "   493                                                               \"No speech threshold is met (%f > %f)\",\n",
       "   494                                                               result.no_speech_prob,\n",
       "   495                                                               options.no_speech_threshold,\n",
       "   496                                                           )\n",
       "   497                                           \n",
       "   498                                                           # fast-forward to the next segment boundary\n",
       "   499                                                           jump = segment_size - 1  # compensating by one to prevent drifting.\n",
       "   500                                                           seek = seek + jump\n",
       "   501                                                           audio = audio[\n",
       "   502                                                               min(jump * self.feature_extractor.hop_length, len(audio)) :\n",
       "   503                                                           ]\n",
       "   504                                           \n",
       "   505                                                           return (\n",
       "   506                                                               output_segments,\n",
       "   507                                                               audio,\n",
       "   508                                                               idx,\n",
       "   509                                                               seek,\n",
       "   510                                                               temperature_idx,\n",
       "   511                                                               previous_tokens,\n",
       "   512                                                               [],\n",
       "   513                                                           )\n",
       "   514                                           \n",
       "   515        29      42824.0   1476.7      0.0          tokens = result.sequences_ids[0]\n",
       "   516                                           \n",
       "   517        29       8933.0    308.0      0.0          current_segments = []\n",
       "   518                                           \n",
       "   519                                                   # TODO: Keeping the code flexible for different options at the cost of runtime, possible to remove chunks if restricting the options\n",
       "   520        29      13493.0    465.3      0.0          single_timestamp_ending = (\n",
       "   521        27      17641.0    653.4      0.0              len(tokens) >= 2\n",
       "   522        27      84908.0   3144.7      0.0              and tokens[-2] < tokenizer.timestamp_begin\n",
       "   523        27      17491.0    647.8      0.0              and tokens[-1] >= tokenizer.timestamp_begin\n",
       "   524                                                   )\n",
       "   525                                           \n",
       "   526        29     438798.0  15131.0      0.0          consecutive_timestamps = [\n",
       "   527                                                       i\n",
       "   528        29      54207.0   1869.2      0.0              for i in range(len(tokens))\n",
       "   529                                                       if i > 0\n",
       "   530                                                       and tokens[i] >= tokenizer.timestamp_begin\n",
       "   531                                                       and tokens[i - 1] >= tokenizer.timestamp_begin\n",
       "   532                                                   ]\n",
       "   533                                           \n",
       "   534        29      22243.0    767.0      0.0          if len(consecutive_timestamps) > 0:\n",
       "   535                                                       slices = list(consecutive_timestamps)\n",
       "   536                                                       if single_timestamp_ending:\n",
       "   537                                                           slices.append(len(tokens))\n",
       "   538                                           \n",
       "   539                                                       last_slice = 0\n",
       "   540                                                       for current_slice in slices:\n",
       "   541                                                           sliced_tokens = tokens[last_slice:current_slice]\n",
       "   542                                                           start_timestamp_position = sliced_tokens[0] - tokenizer.timestamp_begin\n",
       "   543                                                           end_timestamp_position = sliced_tokens[-1] - tokenizer.timestamp_begin\n",
       "   544                                                           start_time = (\n",
       "   545                                                               time_offset + start_timestamp_position * self.time_precision\n",
       "   546                                                           )\n",
       "   547                                                           end_time = time_offset + end_timestamp_position * self.time_precision\n",
       "   548                                                           #dropping segment with start time exceeding segment duration\n",
       "   549                                                           if stream_options.drop_out_of_bound and start_time > time_offset + segment_duration:\n",
       "   550                                                               print(\"THROWN\")\n",
       "   551                                                               break\n",
       "   552                                                           else:\n",
       "   553                                                               current_segments.append(\n",
       "   554                                                                   dict(\n",
       "   555                                                                       seek=seek,\n",
       "   556                                                                       start=start_time,\n",
       "   557                                                                       end=end_time,\n",
       "   558                                                                       tokens=sliced_tokens,\n",
       "   559                                                                   )\n",
       "   560                                                               )\n",
       "   561                                                               last_slice = current_slice\n",
       "   562                                                       if not single_timestamp_ending:\n",
       "   563                                                           sliced_tokens = tokens[last_slice:len(tokens)]\n",
       "   564                                                           start_timestamp_position = sliced_tokens[0] - tokenizer.timestamp_begin\n",
       "   565                                                           start_time = (\n",
       "   566                                                               time_offset + start_timestamp_position * self.time_precision\n",
       "   567                                                           )\n",
       "   568                                                           if start_time < time_offset + segment_duration or not stream_options.drop_out_of_bound:\n",
       "   569                                                               current_segments.append(\n",
       "   570                                                                   dict(\n",
       "   571                                                                       seek=seek,\n",
       "   572                                                                       start=start_time,\n",
       "   573                                                                       end=time_offset + segment_duration,\n",
       "   574                                                                       tokens=sliced_tokens,\n",
       "   575                                                                   )\n",
       "   576                                                               )\n",
       "   577                                                   else:\n",
       "   578        29      20266.0    698.8      0.0              duration = segment_duration\n",
       "   579        29     279332.0   9632.1      0.0              timestamps = [\n",
       "   580        29       7555.0    260.5      0.0                  token for token in tokens if token >= tokenizer.timestamp_begin\n",
       "   581                                                       ]\n",
       "   582        29      11300.0    389.7      0.0              if len(timestamps) > 0 and timestamps[-1] != tokenizer.timestamp_begin:\n",
       "   583                                                           last_timestamp_position = timestamps[-1] - tokenizer.timestamp_begin\n",
       "   584                                                           duration = last_timestamp_position * self.time_precision\n",
       "   585                                           \n",
       "   586        29      26215.0    904.0      0.0              current_segments.append(\n",
       "   587        29      58828.0   2028.6      0.0                  dict(\n",
       "   588        29      13065.0    450.5      0.0                      seek=seek,\n",
       "   589        29      12588.0    434.1      0.0                      start=time_offset,\n",
       "   590        29      16708.0    576.1      0.0                      end=time_offset + duration,\n",
       "   591        29       6925.0    238.8      0.0                      tokens=tokens,\n",
       "   592                                                           )\n",
       "   593                                                       )\n",
       "   594                                           \n",
       "   595        29      15476.0    533.7      0.0              jump = segment_size - 1  # compensating by one to prevent drifting.\n",
       "   596                                           \n",
       "   597        29      25092.0    865.2      0.0          if options.word_timestamps:\n",
       "   598        29 9330830364.0 321752771.2      7.5              self.add_word_timestamps(\n",
       "   599        29       9548.0    329.2      0.0                  current_segments,\n",
       "   600        29       8021.0    276.6      0.0                  tokenizer,\n",
       "   601        29       7937.0    273.7      0.0                  encoder_output,\n",
       "   602        29       7819.0    269.6      0.0                  segment_size,\n",
       "   603        29      24490.0    844.5      0.0                  options.prepend_punctuations,\n",
       "   604        29      28840.0    994.5      0.0                  options.append_punctuations,\n",
       "   605                                                       )\n",
       "   606                                           \n",
       "   607                                                   #For word timestamp only will split via sentences\n",
       "   608        29      63425.0   2187.1      0.0          if options.word_timestamps and options.without_timestamps:\n",
       "   609        29      36907.0   1272.7      0.0              if len(current_segments) > 1:\n",
       "   610                                                           self.logger.error(\"Without timestamps but more than one segment\")\n",
       "   611        29      38886.0   1340.9      0.0              segment = current_segments.pop()\n",
       "   612                                                       # Form own segments based on punctuations\n",
       "   613        29     841646.0  29022.3      0.0              current_segments = self.get_segments_from_wordtimestamp(segment, seek)\n",
       "   614                                           \n",
       "   615        29      13855.0    477.8      0.0          last_final_segment_end = time_offset\n",
       "   616        79      43495.0    550.6      0.0          for segment in current_segments:\n",
       "   617        79      42590.0    539.1      0.0              tokens = segment[\"tokens\"]\n",
       "   618        79    1485233.0  18800.4      0.0              text = tokenizer.decode(tokens)\n",
       "   619                                           \n",
       "   620        78      89215.0   1143.8      0.0              if segment[\"start\"] == segment[\"end\"] or not text.strip():\n",
       "   621                                                           continue\n",
       "   622                                           \n",
       "   623                                                       # need another variable to keep track of segments not finalised\n",
       "   624        78      25876.0    331.7      0.0              current_segment_count = idx\n",
       "   625        78      29843.0    382.6      0.0              prefix_tokens = []\n",
       "   626        51      19047.0    373.5      0.0              if (\n",
       "   627        78      50403.0    646.2      0.0                  final := segment[\"end\"] - time_offset\n",
       "   628        78      72096.0    924.3      0.0                  < segment_duration - stream_options.finalised_segment_gap\n",
       "   629        51      20715.0    406.2      0.0                  and not needs_fallback\n",
       "   630                                                       ):\n",
       "   631        51      20338.0    398.8      0.0                  last_final_segment_end = segment[\"end\"]\n",
       "   632        51      21284.0    417.3      0.0                  idx += 1\n",
       "   633        51      41753.0    818.7      0.0                  previous_tokens.extend(tokens)\n",
       "   634        27      21114.0    782.0      0.0              elif segment[\"end\"] - segment[\"start\"] > 20:\n",
       "   635                                                           last_final_segment_end = segment[\"end\"]\n",
       "   636                                                           idx += 1\n",
       "   637                                                           previous_tokens.extend(tokens)\n",
       "   638                                                       else:\n",
       "   639        27      20836.0    771.7      0.0                  if options.word_timestamps and stream_options.drop_out_of_bound:\n",
       "   640                                                               segment[\"words\"] = [word for word in segment[\"words\"] if word[\"start\"] < time_offset+segment_duration]\n",
       "   641                                                               text = \"\".join([word[\"word\"] for word in segment[\"words\"]]).strip()\n",
       "   642                                                               if tokens[0] >= tokenizer.timestamp_begin:\n",
       "   643                                                                   tokens = [tokens[0]]\n",
       "   644                                                               else:\n",
       "   645                                                                   tokens = []\n",
       "   646                                                               for word in segment[\"words\"]:\n",
       "   647                                                                   tokens += word[\"tokens\"]\n",
       "   648                                                               if not text:\n",
       "   649                                                                   break\n",
       "   650        27      15286.0    566.1      0.0                  if stream_options.use_prefix and not needs_fallback and not wipe_token:\n",
       "   651                                                               if not options.without_timestamps :\n",
       "   652                                                                   tokens[0] = tokens[0] - round((last_final_segment_end-time_offset)/self.time_precision)\n",
       "   653                                                                   if tokens[-1] >= tokenizer.timestamp_begin:\n",
       "   654                                                                       tokens[-1] = tokens[-1] - round((last_final_segment_end-time_offset)/self.time_precision)\n",
       "   655                                                               prefix_tokens.extend(tokens)\n",
       "   656        78      36982.0    474.1      0.0              current_segment_count += 1\n",
       "   657                                                       #! Can return to client here instead of outputting it\n",
       "   658        78      39637.0    508.2      0.0              output_segments.append(\n",
       "   659        78     210561.0   2699.5      0.0                  StreamSegment(\n",
       "   660        78      25601.0    328.2      0.0                      id=current_segment_count,\n",
       "   661        78     102080.0   1308.7      0.0                      start=segment[\"start\"],\n",
       "   662        78      29466.0    377.8      0.0                      end=segment[\"end\"],\n",
       "   663        78      28220.0    361.8      0.0                      text=text,\n",
       "   664        78      30081.0    385.7      0.0                      temperature=temperature,\n",
       "   665        78      29858.0    382.8      0.0                      avg_logprob=avg_logprob,\n",
       "   666        78      31078.0    398.4      0.0                      compression_ratio=compression_ratio,\n",
       "   667        78     214097.0   2744.8      0.0                      no_speech_prob=result.no_speech_prob,\n",
       "   668                                                               words=(\n",
       "   669        78     958962.0  12294.4      0.0                          [Word(**word) for word in segment[\"words\"]]\n",
       "   670        78      33244.0    426.2      0.0                          if options.word_timestamps\n",
       "   671                                                                   else None\n",
       "   672                                                               ),\n",
       "   673        78      28621.0    366.9      0.0                      final=final,\n",
       "   674                                                           )\n",
       "   675                                                       )\n",
       "   676                                                       # Keeping the audio for those segment not finalised\n",
       "   677                                                       # TODO: Possible to add option keep only those words within the segment_end_thres\n",
       "   678                                                       # TODO: to reduce audio kept and lesser token generated will need a var to keep track\n",
       "   679                                                       # TODO: of the unfinalised tokens and add them to the prompt. Do this only when needs_fallback is False.\n",
       "   680                                           \n",
       "   681                                                       # TODO: If options.without_timestamp is False and single_timestamp_ending is True should ensure the last segment final=False\n",
       "   682        78     236996.0   3038.4      0.0              seek_shift = round(\n",
       "   683        78      73019.0    936.1      0.0                  (last_final_segment_end - time_offset) * self.frames_per_second\n",
       "   684                                                       )\n",
       "   685                                           \n",
       "   686        72      33696.0    468.0      0.0              if seek_shift > 0:\n",
       "   687        72      46410.0    644.6      0.0                  jump = seek_shift\n",
       "   688                                                       else:\n",
       "   689         6       2516.0    419.3      0.0                  jump = 0\n",
       "   690                                           \n",
       "   691        29      19011.0    655.6      0.0          if not options.condition_on_previous_text or temperature > 0.5 or wipe_token:\n",
       "   692        29      11455.0    395.0      0.0              previous_tokens = []\n",
       "   693                                           \n",
       "   694        29      12012.0    414.2      0.0          if needs_fallback:\n",
       "   695                                                       temperature_idx = min(len(self.temperatures) - 1, temperature_idx + 1)\n",
       "   696                                                   else:\n",
       "   697        29      10965.0    378.1      0.0              temperature_idx = 0\n",
       "   698                                           \n",
       "   699                                                   # Need to discard audio that is finalized and add in need audio\n",
       "   700                                                   # Check if text occurred in the added silence\n",
       "   701        29      12658.0    436.5      0.0          if jump >= segment_size:\n",
       "   702                                                       jump = segment_size - 1\n",
       "   703        29      14585.0    502.9      0.0          seek = seek + jump\n",
       "   704        29      81177.0   2799.2      0.0          audio = audio[min(jump * self.feature_extractor.hop_length, len(audio)) :]\n",
       "   705                                           \n",
       "   706        29      15170.0    523.1      0.0          return output_segments, audio, idx, seek, temperature_idx, previous_tokens, prefix_tokens"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f model.simulate_fixed_interval_streaming -f model.generate_segments model.simulate_fixed_interval_streaming(\"GovTech_MFTMEP1_16000_mono_16.wav\", tokenizer, options, stream_options, interval=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c11a2963-128f-4f59-bcfc-3ed3f8d9c8b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StreamOptions(finalised_segment_gap=2, out_of_bound_gap=5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8dd9213b-b089-4489-841b-91116dad68c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<faster_whisper.tokenizer.Tokenizer at 0x7f9058432640>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b70dc43e-f3e4-43d2-9351-1b862260d726",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import soundfile as sf\n",
    "import time\n",
    "import numpy as np\n",
    "import importlib\n",
    "import jeff_live\n",
    "importlib.reload(jeff_live)\n",
    "from jeff_live import WhisperModel_live\n",
    "\n",
    "model_size = 'small.en'\n",
    "model_dir = \"pretrained_models/faster_whisper/\"\n",
    "# audio_file = \"data/wav/IMDA_test_60s.wav\"\n",
    "audio_file = \"GovTech_MFTMEP1_16000_mono_16.wav\"\n",
    "\n",
    "# Load model before model files have been downloaded\n",
    "# faster_model_live = WhisperModel_live(model_size, compute_type=\"int8\", download_root=model_dir)\n",
    "# Load model after model files have been downloaded\n",
    "faster_model_live = WhisperModel_live(model_size, compute_type=\"int8\")\n",
    "\n",
    "def run_audio(audio_file, faster_model_live):\n",
    "  wav, fs = sf.read(audio_file)\n",
    "  wav_len = len(wav)\n",
    "  \n",
    "  RATE = 16000\n",
    "  CHUNK_S = 7\n",
    "  CHUNK = CHUNK_S * RATE\n",
    "  # max_time = 440.0\n",
    "  max_time = -1\n",
    "  compression_ratio_threshold = 2.4\n",
    "  \n",
    "  if max_time > 0:\n",
    "      wav_len = min(wav_len, int(max_time*RATE))\n",
    "  chunk_starts = list(range(0, wav_len, CHUNK))\n",
    "  final_text = ''\n",
    "  seek = 0.0\n",
    "  for chunk_start in chunk_starts[0:]:\n",
    "      t1 = time.time()\n",
    "      non_final_text = ''\n",
    "      repeat_text = ''\n",
    "      start_idx = min(chunk_start, int(seek*RATE))\n",
    "      end_idx = min(wav_len, chunk_start + CHUNK)\n",
    "  \n",
    "      # Make sure audio chunk is less than 30s\n",
    "      # Skip to most recent 30s if longer\n",
    "      start_idx = max(start_idx, end_idx - 30*RATE)\n",
    "      start_time = start_idx/RATE\n",
    "      end_time = end_idx/RATE\n",
    "  \n",
    "      print(f'Transcribing {start_time:.2f}->{end_time:.2f}s...')\n",
    "      wav_chunk = np.array(wav[start_idx:end_idx], dtype=np.float32)\n",
    "      \n",
    "      segments_live, info = faster_model_live.transcribe(wav_chunk, beam_size=1, temperature=0, vad_filter=False, word_timestamps=True)\n",
    "      seek = start_time\n",
    "      for seg in segments_live:\n",
    "          # print(seg)\n",
    "          print(f'SEG {start_time+seg.start:.2f}->{start_time+seg.end:.2f}, seek:{start_time+seg.seek_live:.2f}'\n",
    "                  + f', final:{seg.final}, compression_ratio: {seg.compression_ratio:.2f}, words: {seg.text.lstrip()}')\n",
    "  \n",
    "          if seg.compression_ratio > compression_ratio_threshold:\n",
    "              repeat_text += seg.text\n",
    "  \n",
    "          elif seg.final:\n",
    "              final_text += seg.text\n",
    "              seek = start_time + seg.seek_live\n",
    "          else:\n",
    "              non_final_text = seg.text\n",
    "  \n",
    "      t2 = time.time()\n",
    "      time_elapsed = t2 - t1\n",
    "      print(f'Output: {final_text.lstrip()} | {non_final_text.lstrip()}')\n",
    "      if repeat_text:\n",
    "          print(f'REPEAT: {repeat_text.lstrip()}')\n",
    "      print(f'Time elapsed: {time_elapsed:0.2f}s\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97f7a366-9c00-4b45-8b62-dfb64791bab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing 0.00->7.00s...\n",
      "SEG 0.00->3.96, seek:6.94, final:False, compression_ratio: 0.38, words: Music\n",
      "Output:  | Music\n",
      "Time elapsed: 3.66s\n",
      "\n",
      "Transcribing 0.00->14.00s...\n",
      "SEG 6.62->9.30, seek:9.60, final:True, compression_ratio: 1.09, words: Welcome to My Friend Tell Me One.\n",
      "SEG 9.66->13.62, seek:9.60, final:False, compression_ratio: 1.09, words: Today we're going to speak to the Go Bros, two engineers who work in DevOps.\n",
      "Output: Welcome to My Friend Tell Me One. | Today we're going to speak to the Go Bros, two engineers who work in DevOps.\n",
      "Time elapsed: 3.67s\n",
      "\n",
      "Transcribing 9.60->21.00s...\n",
      "SEG 9.60->13.68, seek:19.60, final:True, compression_ratio: 1.26, words: Today we're going to speak to the Go Bros, two engineers who work in DevOps.\n",
      "SEG 14.30->17.34, seek:19.60, final:True, compression_ratio: 1.26, words: Okay, hi, my name is Ryan. I work on my current future.\n",
      "SEG 18.14->19.40, seek:19.60, final:True, compression_ratio: 1.26, words: So basically I'm doing DevOps.\n",
      "SEG 20.38->20.74, seek:19.60, final:False, compression_ratio: 1.26, words: Same as I go.\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. | Same as I go.\n",
      "Time elapsed: 4.22s\n",
      "\n",
      "Transcribing 19.60->28.00s...\n",
      "SEG 20.22->20.66, seek:20.60, final:False, compression_ratio: 0.69, words: See you next time.\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. | See you next time.\n",
      "Time elapsed: 3.62s\n",
      "\n",
      "Transcribing 19.60->35.00s...\n",
      "SEG 19.90->20.88, seek:21.60, final:False, compression_ratio: 0.67, words: Same as the guy.\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. | Same as the guy.\n",
      "Time elapsed: 3.73s\n",
      "\n",
      "Transcribing 19.60->42.00s...\n",
      "SEG 39.02->41.98, seek:21.60, final:False, compression_ratio: 0.67, words: Same as the guy.\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. | Same as the guy.\n",
      "Time elapsed: 4.04s\n",
      "\n",
      "Transcribing 19.60->49.00s...\n",
      "SEG 19.90->20.86, seek:41.60, final:True, compression_ratio: 1.61, words: Same as the guy.\n",
      "SEG 24.44->30.36, seek:41.60, final:True, compression_ratio: 1.61, words: Textbook answer is DevOps is a culture.\n",
      "SEG 31.70->37.42, seek:41.60, final:True, compression_ratio: 1.61, words: So in other words, the whole team knows how to do operations, some parts of operations as well as development.\n",
      "SEG 37.82->41.92, seek:41.60, final:True, compression_ratio: 1.61, words: Giving the responsibility of what operators normally do to developers.\n",
      "SEG 42.44->48.84, seek:41.60, final:False, compression_ratio: 1.61, words: It's kind of a wide term. So depending on where you go, different organisations, different teams have different definition of it.\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. Same as the guy. Textbook answer is DevOps is a culture. So in other words, the whole team knows how to do operations, some parts of operations as well as development. Giving the responsibility of what operators normally do to developers. | It's kind of a wide term. So depending on where you go, different organisations, different teams have different definition of it.\n",
      "Time elapsed: 5.94s\n",
      "\n",
      "Transcribing 41.60->56.00s...\n",
      "SEG 42.38->46.60, seek:51.68, final:True, compression_ratio: 1.44, words: It's kind of a wide term, so depending on where you go, different organisations, different\n",
      "SEG 46.60->49.66, seek:51.68, final:True, compression_ratio: 1.44, words: teams have different definition of it.\n",
      "SEG 49.80->51.40, seek:51.68, final:True, compression_ratio: 1.44, words: Yeah, so non-textbook is how it comes.\n",
      "SEG 54.80->55.98, seek:51.68, final:False, compression_ratio: 1.44, words: It's not a new thing, actually it's me.\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. Same as the guy. Textbook answer is DevOps is a culture. So in other words, the whole team knows how to do operations, some parts of operations as well as development. Giving the responsibility of what operators normally do to developers. It's kind of a wide term, so depending on where you go, different organisations, different teams have different definition of it. Yeah, so non-textbook is how it comes. | It's not a new thing, actually it's me.\n",
      "Time elapsed: 4.37s\n",
      "\n",
      "Transcribing 51.68->63.00s...\n",
      "SEG 54.90->58.52, seek:58.68, final:False, compression_ratio: 1.04, words: It's not a new thing. Actually it's been around since 1993. Since the Agile Manifesto came out.\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. Same as the guy. Textbook answer is DevOps is a culture. So in other words, the whole team knows how to do operations, some parts of operations as well as development. Giving the responsibility of what operators normally do to developers. It's kind of a wide term, so depending on where you go, different organisations, different teams have different definition of it. Yeah, so non-textbook is how it comes. | It's not a new thing. Actually it's been around since 1993. Since the Agile Manifesto came out.\n",
      "Time elapsed: 3.60s\n",
      "\n",
      "Transcribing 51.68->70.00s...\n",
      "SEG 54.84->58.54, seek:67.68, final:True, compression_ratio: 1.35, words: It's not a new thing, actually it's been around since 1993. Since the Agile Manifesto came out.\n",
      "SEG 61.66->67.08, seek:67.68, final:True, compression_ratio: 1.35, words: Like, the then learning in school, school cannot teach us. But, it costs 2 unique units.\n",
      "SEG 67.54->69.76, seek:67.68, final:False, compression_ratio: 1.35, words: Basically, we have to run our own services.\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. Same as the guy. Textbook answer is DevOps is a culture. So in other words, the whole team knows how to do operations, some parts of operations as well as development. Giving the responsibility of what operators normally do to developers. It's kind of a wide term, so depending on where you go, different organisations, different teams have different definition of it. Yeah, so non-textbook is how it comes. It's not a new thing, actually it's been around since 1993. Since the Agile Manifesto came out. Like, the then learning in school, school cannot teach us. But, it costs 2 unique units. | Basically, we have to run our own services.\n",
      "Time elapsed: 4.41s\n",
      "\n",
      "Transcribing 67.68->77.00s...\n",
      "SEG 67.68->70.18, seek:74.56, final:True, compression_ratio: 1.36, words: Basically, we have to run our own service like that.\n",
      "SEG 70.38->73.82, seek:74.56, final:True, compression_ratio: 1.36, words: So by running it, you probably have to do a lot of things to get it running.\n",
      "SEG 74.42->75.88, seek:74.56, final:False, compression_ratio: 1.36, words: So that's when we start learning.\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. Same as the guy. Textbook answer is DevOps is a culture. So in other words, the whole team knows how to do operations, some parts of operations as well as development. Giving the responsibility of what operators normally do to developers. It's kind of a wide term, so depending on where you go, different organisations, different teams have different definition of it. Yeah, so non-textbook is how it comes. It's not a new thing, actually it's been around since 1993. Since the Agile Manifesto came out. Like, the then learning in school, school cannot teach us. But, it costs 2 unique units. Basically, we have to run our own service like that. So by running it, you probably have to do a lot of things to get it running. | So that's when we start learning.\n",
      "Time elapsed: 4.12s\n",
      "\n",
      "Transcribing 74.56->84.00s...\n",
      "SEG 74.56->75.92, seek:76.12, final:False, compression_ratio: 0.80, words: So that's when we start learning.\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. Same as the guy. Textbook answer is DevOps is a culture. So in other words, the whole team knows how to do operations, some parts of operations as well as development. Giving the responsibility of what operators normally do to developers. It's kind of a wide term, so depending on where you go, different organisations, different teams have different definition of it. Yeah, so non-textbook is how it comes. It's not a new thing, actually it's been around since 1993. Since the Agile Manifesto came out. Like, the then learning in school, school cannot teach us. But, it costs 2 unique units. Basically, we have to run our own service like that. So by running it, you probably have to do a lot of things to get it running. | So that's when we start learning.\n",
      "Time elapsed: 3.35s\n",
      "\n",
      "Transcribing 74.56->91.00s...\n",
      "SEG 74.56->76.00, seek:76.56, final:False, compression_ratio: 0.80, words: So that's when we start learning.\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. Same as the guy. Textbook answer is DevOps is a culture. So in other words, the whole team knows how to do operations, some parts of operations as well as development. Giving the responsibility of what operators normally do to developers. It's kind of a wide term, so depending on where you go, different organisations, different teams have different definition of it. Yeah, so non-textbook is how it comes. It's not a new thing, actually it's been around since 1993. Since the Agile Manifesto came out. Like, the then learning in school, school cannot teach us. But, it costs 2 unique units. Basically, we have to run our own service like that. So by running it, you probably have to do a lot of things to get it running. | So that's when we start learning.\n",
      "Time elapsed: 3.23s\n",
      "\n",
      "Transcribing 74.56->98.00s...\n",
      "SEG 74.56->75.96, seek:76.16, final:False, compression_ratio: 0.80, words: So that's when we start learning.\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. Same as the guy. Textbook answer is DevOps is a culture. So in other words, the whole team knows how to do operations, some parts of operations as well as development. Giving the responsibility of what operators normally do to developers. It's kind of a wide term, so depending on where you go, different organisations, different teams have different definition of it. Yeah, so non-textbook is how it comes. It's not a new thing, actually it's been around since 1993. Since the Agile Manifesto came out. Like, the then learning in school, school cannot teach us. But, it costs 2 unique units. Basically, we have to run our own service like that. So by running it, you probably have to do a lot of things to get it running. | So that's when we start learning.\n",
      "Time elapsed: 3.46s\n",
      "\n",
      "Transcribing 75.00->105.00s...\n",
      "SEG 75.32->75.96, seek:105.00, final:True, compression_ratio: 1.46, words: Maybe start learning.\n",
      "SEG 79.82->79.86, seek:105.00, final:True, compression_ratio: 1.46, words: No.\n",
      "SEG 80.12->80.74, seek:105.00, final:True, compression_ratio: 1.46, words: No, I don't know.\n",
      "SEG 83.32->85.42, seek:105.00, final:True, compression_ratio: 1.46, words: Two once.\n",
      "SEG 85.50->86.20, seek:105.00, final:True, compression_ratio: 1.46, words: Oh, I don't like that.\n",
      "SEG 87.26->87.38, seek:105.00, final:True, compression_ratio: 1.46, words: I think.\n",
      "SEG 87.84->87.92, seek:105.00, final:True, compression_ratio: 1.46, words: Six.\n",
      "SEG 88.40->90.52, seek:105.00, final:True, compression_ratio: 1.46, words: How do you say it?\n",
      "SEG 90.78->91.34, seek:105.00, final:True, compression_ratio: 1.46, words: Wait, no, I shouldn't.\n",
      "SEG 91.68->92.20, seek:105.00, final:True, compression_ratio: 1.46, words: Two for nine.\n",
      "SEG 95.00->96.36, seek:105.00, final:True, compression_ratio: 1.46, words: You've been based on my wall.\n",
      "SEG 97.80->100.76, seek:105.00, final:True, compression_ratio: 1.46, words: You've been based on my wall.\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. Same as the guy. Textbook answer is DevOps is a culture. So in other words, the whole team knows how to do operations, some parts of operations as well as development. Giving the responsibility of what operators normally do to developers. It's kind of a wide term, so depending on where you go, different organisations, different teams have different definition of it. Yeah, so non-textbook is how it comes. It's not a new thing, actually it's been around since 1993. Since the Agile Manifesto came out. Like, the then learning in school, school cannot teach us. But, it costs 2 unique units. Basically, we have to run our own service like that. So by running it, you probably have to do a lot of things to get it running. Maybe start learning. No. No, I don't know. Two once. Oh, I don't like that. I think. Six. How do you say it? Wait, no, I shouldn't. Two for nine. You've been based on my wall. You've been based on my wall. | \n",
      "Time elapsed: 8.16s\n",
      "\n",
      "Transcribing 105.00->112.00s...\n",
      "SEG 105.00->109.76, seek:109.72, final:True, compression_ratio: 1.29, words: you can start growing your own vegetables and cooking them so that you manage the\n",
      "SEG 109.76->111.98, seek:109.72, final:False, compression_ratio: 1.29, words: whole thing end to end. Maybe? Yeah. Do everything or something.\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. Same as the guy. Textbook answer is DevOps is a culture. So in other words, the whole team knows how to do operations, some parts of operations as well as development. Giving the responsibility of what operators normally do to developers. It's kind of a wide term, so depending on where you go, different organisations, different teams have different definition of it. Yeah, so non-textbook is how it comes. It's not a new thing, actually it's been around since 1993. Since the Agile Manifesto came out. Like, the then learning in school, school cannot teach us. But, it costs 2 unique units. Basically, we have to run our own service like that. So by running it, you probably have to do a lot of things to get it running. Maybe start learning. No. No, I don't know. Two once. Oh, I don't like that. I think. Six. How do you say it? Wait, no, I shouldn't. Two for nine. You've been based on my wall. You've been based on my wall. you can start growing your own vegetables and cooking them so that you manage the | whole thing end to end. Maybe? Yeah. Do everything or something.\n",
      "Time elapsed: 3.98s\n",
      "\n",
      "Transcribing 109.72->119.00s...\n",
      "SEG 109.72->112.16, seek:117.06, final:True, compression_ratio: 1.41, words: whole thing end to end maybe do everything yourself.\n",
      "SEG 112.76->117.04, seek:117.06, final:True, compression_ratio: 1.41, words: Okay so previously right, assuming you got two people so one person just quote right and\n",
      "SEG 117.04->118.98, seek:117.06, final:False, compression_ratio: 1.41, words: pass everything to a fan you don't care what I'm saying.\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. Same as the guy. Textbook answer is DevOps is a culture. So in other words, the whole team knows how to do operations, some parts of operations as well as development. Giving the responsibility of what operators normally do to developers. It's kind of a wide term, so depending on where you go, different organisations, different teams have different definition of it. Yeah, so non-textbook is how it comes. It's not a new thing, actually it's been around since 1993. Since the Agile Manifesto came out. Like, the then learning in school, school cannot teach us. But, it costs 2 unique units. Basically, we have to run our own service like that. So by running it, you probably have to do a lot of things to get it running. Maybe start learning. No. No, I don't know. Two once. Oh, I don't like that. I think. Six. How do you say it? Wait, no, I shouldn't. Two for nine. You've been based on my wall. You've been based on my wall. you can start growing your own vegetables and cooking them so that you manage the whole thing end to end maybe do everything yourself. Okay so previously right, assuming you got two people so one person just quote right and | pass everything to a fan you don't care what I'm saying.\n",
      "Time elapsed: 4.22s\n",
      "\n",
      "Transcribing 117.06->126.00s...\n",
      "SEG 117.06->118.76, seek:125.06, final:True, compression_ratio: 1.51, words: pass everything to a fan, he don't care.\n",
      "SEG 119.26->121.76, seek:125.06, final:True, compression_ratio: 1.51, words: So you cook the egg, you cook one giant piece,\n",
      "SEG 121.98->122.72, seek:125.06, final:True, compression_ratio: 1.51, words: then you just give it to the person.\n",
      "SEG 122.86->124.88, seek:125.06, final:True, compression_ratio: 1.51, words: Then the person is like, wow, you have to cut out the piece.\n",
      "SEG 125.36->125.98, seek:125.06, final:False, compression_ratio: 1.51, words: So it's like, dang.\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. Same as the guy. Textbook answer is DevOps is a culture. So in other words, the whole team knows how to do operations, some parts of operations as well as development. Giving the responsibility of what operators normally do to developers. It's kind of a wide term, so depending on where you go, different organisations, different teams have different definition of it. Yeah, so non-textbook is how it comes. It's not a new thing, actually it's been around since 1993. Since the Agile Manifesto came out. Like, the then learning in school, school cannot teach us. But, it costs 2 unique units. Basically, we have to run our own service like that. So by running it, you probably have to do a lot of things to get it running. Maybe start learning. No. No, I don't know. Two once. Oh, I don't like that. I think. Six. How do you say it? Wait, no, I shouldn't. Two for nine. You've been based on my wall. You've been based on my wall. you can start growing your own vegetables and cooking them so that you manage the whole thing end to end maybe do everything yourself. Okay so previously right, assuming you got two people so one person just quote right and pass everything to a fan, he don't care. So you cook the egg, you cook one giant piece, then you just give it to the person. Then the person is like, wow, you have to cut out the piece. | So it's like, dang.\n",
      "Time elapsed: 4.61s\n",
      "\n",
      "Transcribing 125.06->133.00s...\n",
      "SEG 125.48->126.50, seek:130.78, final:True, compression_ratio: 1.32, words: So it's like double work.\n",
      "SEG 127.02->129.80, seek:130.78, final:True, compression_ratio: 1.32, words: So now right, I cook with it, I cut up for you, you go to the front, then I cash in more\n",
      "SEG 129.80->130.24, seek:130.78, final:True, compression_ratio: 1.32, words: efficient, right?\n",
      "SEG 130.40->131.68, seek:130.78, final:False, compression_ratio: 1.32, words: Because you just take one slice, put it inside.\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. Same as the guy. Textbook answer is DevOps is a culture. So in other words, the whole team knows how to do operations, some parts of operations as well as development. Giving the responsibility of what operators normally do to developers. It's kind of a wide term, so depending on where you go, different organisations, different teams have different definition of it. Yeah, so non-textbook is how it comes. It's not a new thing, actually it's been around since 1993. Since the Agile Manifesto came out. Like, the then learning in school, school cannot teach us. But, it costs 2 unique units. Basically, we have to run our own service like that. So by running it, you probably have to do a lot of things to get it running. Maybe start learning. No. No, I don't know. Two once. Oh, I don't like that. I think. Six. How do you say it? Wait, no, I shouldn't. Two for nine. You've been based on my wall. You've been based on my wall. you can start growing your own vegetables and cooking them so that you manage the whole thing end to end maybe do everything yourself. Okay so previously right, assuming you got two people so one person just quote right and pass everything to a fan, he don't care. So you cook the egg, you cook one giant piece, then you just give it to the person. Then the person is like, wow, you have to cut out the piece. So it's like double work. So now right, I cook with it, I cut up for you, you go to the front, then I cash in more efficient, right? | Because you just take one slice, put it inside.\n",
      "Time elapsed: 4.23s\n",
      "\n",
      "Transcribing 130.78->140.00s...\n",
      "SEG 130.78->132.02, seek:138.02, final:False, compression_ratio: 0.72, words: one slice put inside.\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. Same as the guy. Textbook answer is DevOps is a culture. So in other words, the whole team knows how to do operations, some parts of operations as well as development. Giving the responsibility of what operators normally do to developers. It's kind of a wide term, so depending on where you go, different organisations, different teams have different definition of it. Yeah, so non-textbook is how it comes. It's not a new thing, actually it's been around since 1993. Since the Agile Manifesto came out. Like, the then learning in school, school cannot teach us. But, it costs 2 unique units. Basically, we have to run our own service like that. So by running it, you probably have to do a lot of things to get it running. Maybe start learning. No. No, I don't know. Two once. Oh, I don't like that. I think. Six. How do you say it? Wait, no, I shouldn't. Two for nine. You've been based on my wall. You've been based on my wall. you can start growing your own vegetables and cooking them so that you manage the whole thing end to end maybe do everything yourself. Okay so previously right, assuming you got two people so one person just quote right and pass everything to a fan, he don't care. So you cook the egg, you cook one giant piece, then you just give it to the person. Then the person is like, wow, you have to cut out the piece. So it's like double work. So now right, I cook with it, I cut up for you, you go to the front, then I cash in more efficient, right? | one slice put inside.\n",
      "Time elapsed: 3.35s\n",
      "\n",
      "Transcribing 130.78->147.00s...\n",
      "SEG 130.78->132.04, seek:147.02, final:False, compression_ratio: 0.72, words: one slice put inside.\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. Same as the guy. Textbook answer is DevOps is a culture. So in other words, the whole team knows how to do operations, some parts of operations as well as development. Giving the responsibility of what operators normally do to developers. It's kind of a wide term, so depending on where you go, different organisations, different teams have different definition of it. Yeah, so non-textbook is how it comes. It's not a new thing, actually it's been around since 1993. Since the Agile Manifesto came out. Like, the then learning in school, school cannot teach us. But, it costs 2 unique units. Basically, we have to run our own service like that. So by running it, you probably have to do a lot of things to get it running. Maybe start learning. No. No, I don't know. Two once. Oh, I don't like that. I think. Six. How do you say it? Wait, no, I shouldn't. Two for nine. You've been based on my wall. You've been based on my wall. you can start growing your own vegetables and cooking them so that you manage the whole thing end to end maybe do everything yourself. Okay so previously right, assuming you got two people so one person just quote right and pass everything to a fan, he don't care. So you cook the egg, you cook one giant piece, then you just give it to the person. Then the person is like, wow, you have to cut out the piece. So it's like double work. So now right, I cook with it, I cut up for you, you go to the front, then I cash in more efficient, right? | one slice put inside.\n",
      "Time elapsed: 3.45s\n",
      "\n",
      "Transcribing 130.78->154.00s...\n",
      "SEG 130.78->132.02, seek:132.78, final:False, compression_ratio: 0.72, words: one slice put inside.\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. Same as the guy. Textbook answer is DevOps is a culture. So in other words, the whole team knows how to do operations, some parts of operations as well as development. Giving the responsibility of what operators normally do to developers. It's kind of a wide term, so depending on where you go, different organisations, different teams have different definition of it. Yeah, so non-textbook is how it comes. It's not a new thing, actually it's been around since 1993. Since the Agile Manifesto came out. Like, the then learning in school, school cannot teach us. But, it costs 2 unique units. Basically, we have to run our own service like that. So by running it, you probably have to do a lot of things to get it running. Maybe start learning. No. No, I don't know. Two once. Oh, I don't like that. I think. Six. How do you say it? Wait, no, I shouldn't. Two for nine. You've been based on my wall. You've been based on my wall. you can start growing your own vegetables and cooking them so that you manage the whole thing end to end maybe do everything yourself. Okay so previously right, assuming you got two people so one person just quote right and pass everything to a fan, he don't care. So you cook the egg, you cook one giant piece, then you just give it to the person. Then the person is like, wow, you have to cut out the piece. So it's like double work. So now right, I cook with it, I cut up for you, you go to the front, then I cash in more efficient, right? | one slice put inside.\n",
      "Time elapsed: 3.43s\n",
      "\n",
      "Transcribing 131.00->161.00s...\n",
      "SEG 131.00->132.06, seek:161.00, final:True, compression_ratio: 1.54, words: and then you can apply the support inside.\n",
      "SEG 132.76->140.46, seek:161.00, final:True, compression_ratio: 1.54, words: So My Korea's Future is a platform created to reduce unemployment in local PMAs,\n",
      "SEG 140.60->143.86, seek:161.00, final:True, compression_ratio: 1.54, words: especially those in the 30s and 40s who find themselves unemployed\n",
      "SEG 143.86->145.30, seek:161.00, final:True, compression_ratio: 1.54, words: and they are unable to rejoin the workforce.\n",
      "SEG 146.14->149.08, seek:161.00, final:True, compression_ratio: 1.54, words: So we do this through different ways.\n",
      "SEG 149.36->151.98, seek:161.00, final:True, compression_ratio: 1.54, words: So instead of matching people's jobs, we match skills to jobs, for example.\n",
      "SEG 154.24->157.76, seek:161.00, final:True, compression_ratio: 1.54, words: Definitely mess.\n",
      "SEG 158.74->160.00, seek:134.00, final:False, compression_ratio: 0.72, words: So Chinese do it, or?\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. Same as the guy. Textbook answer is DevOps is a culture. So in other words, the whole team knows how to do operations, some parts of operations as well as development. Giving the responsibility of what operators normally do to developers. It's kind of a wide term, so depending on where you go, different organisations, different teams have different definition of it. Yeah, so non-textbook is how it comes. It's not a new thing, actually it's been around since 1993. Since the Agile Manifesto came out. Like, the then learning in school, school cannot teach us. But, it costs 2 unique units. Basically, we have to run our own service like that. So by running it, you probably have to do a lot of things to get it running. Maybe start learning. No. No, I don't know. Two once. Oh, I don't like that. I think. Six. How do you say it? Wait, no, I shouldn't. Two for nine. You've been based on my wall. You've been based on my wall. you can start growing your own vegetables and cooking them so that you manage the whole thing end to end maybe do everything yourself. Okay so previously right, assuming you got two people so one person just quote right and pass everything to a fan, he don't care. So you cook the egg, you cook one giant piece, then you just give it to the person. Then the person is like, wow, you have to cut out the piece. So it's like double work. So now right, I cook with it, I cut up for you, you go to the front, then I cash in more efficient, right? and then you can apply the support inside. So My Korea's Future is a platform created to reduce unemployment in local PMAs, especially those in the 30s and 40s who find themselves unemployed and they are unable to rejoin the workforce. So we do this through different ways. So instead of matching people's jobs, we match skills to jobs, for example. Definitely mess. | So Chinese do it, or?\n",
      "Time elapsed: 8.97s\n",
      "\n",
      "Transcribing 161.00->168.00s...\n",
      "SEG 161.00->163.20, seek:165.00, final:True, compression_ratio: 1.24, words: I'm not sure if I can do it right now\n",
      "SEG 163.20->163.82, seek:165.00, final:True, compression_ratio: 1.24, words: I'll try to do it\n",
      "SEG 163.82->167.98, seek:165.00, final:False, compression_ratio: 1.24, words: I'll try to do it\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. Same as the guy. Textbook answer is DevOps is a culture. So in other words, the whole team knows how to do operations, some parts of operations as well as development. Giving the responsibility of what operators normally do to developers. It's kind of a wide term, so depending on where you go, different organisations, different teams have different definition of it. Yeah, so non-textbook is how it comes. It's not a new thing, actually it's been around since 1993. Since the Agile Manifesto came out. Like, the then learning in school, school cannot teach us. But, it costs 2 unique units. Basically, we have to run our own service like that. So by running it, you probably have to do a lot of things to get it running. Maybe start learning. No. No, I don't know. Two once. Oh, I don't like that. I think. Six. How do you say it? Wait, no, I shouldn't. Two for nine. You've been based on my wall. You've been based on my wall. you can start growing your own vegetables and cooking them so that you manage the whole thing end to end maybe do everything yourself. Okay so previously right, assuming you got two people so one person just quote right and pass everything to a fan, he don't care. So you cook the egg, you cook one giant piece, then you just give it to the person. Then the person is like, wow, you have to cut out the piece. So it's like double work. So now right, I cook with it, I cut up for you, you go to the front, then I cash in more efficient, right? and then you can apply the support inside. So My Korea's Future is a platform created to reduce unemployment in local PMAs, especially those in the 30s and 40s who find themselves unemployed and they are unable to rejoin the workforce. So we do this through different ways. So instead of matching people's jobs, we match skills to jobs, for example. Definitely mess. I'm not sure if I can do it right now I'll try to do it | I'll try to do it\n",
      "Time elapsed: 4.06s\n",
      "\n",
      "Transcribing 165.00->175.00s...\n",
      "SEG 165.00->166.20, seek:171.00, final:True, compression_ratio: 2.20, words: I'm not sure.\n",
      "SEG 166.66->167.04, seek:171.00, final:True, compression_ratio: 2.20, words: I'm not sure.\n",
      "SEG 167.94->168.52, seek:171.00, final:True, compression_ratio: 2.20, words: I'm not sure.\n",
      "SEG 169.50->170.08, seek:171.00, final:False, compression_ratio: 2.20, words: I'm not sure.\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. Same as the guy. Textbook answer is DevOps is a culture. So in other words, the whole team knows how to do operations, some parts of operations as well as development. Giving the responsibility of what operators normally do to developers. It's kind of a wide term, so depending on where you go, different organisations, different teams have different definition of it. Yeah, so non-textbook is how it comes. It's not a new thing, actually it's been around since 1993. Since the Agile Manifesto came out. Like, the then learning in school, school cannot teach us. But, it costs 2 unique units. Basically, we have to run our own service like that. So by running it, you probably have to do a lot of things to get it running. Maybe start learning. No. No, I don't know. Two once. Oh, I don't like that. I think. Six. How do you say it? Wait, no, I shouldn't. Two for nine. You've been based on my wall. You've been based on my wall. you can start growing your own vegetables and cooking them so that you manage the whole thing end to end maybe do everything yourself. Okay so previously right, assuming you got two people so one person just quote right and pass everything to a fan, he don't care. So you cook the egg, you cook one giant piece, then you just give it to the person. Then the person is like, wow, you have to cut out the piece. So it's like double work. So now right, I cook with it, I cut up for you, you go to the front, then I cash in more efficient, right? and then you can apply the support inside. So My Korea's Future is a platform created to reduce unemployment in local PMAs, especially those in the 30s and 40s who find themselves unemployed and they are unable to rejoin the workforce. So we do this through different ways. So instead of matching people's jobs, we match skills to jobs, for example. Definitely mess. I'm not sure if I can do it right now I'll try to do it I'm not sure. I'm not sure. I'm not sure. | I'm not sure.\n",
      "Time elapsed: 5.52s\n",
      "\n",
      "Transcribing 171.00->182.00s...\n",
      "SEG 171.00->171.92, seek:173.00, final:False, compression_ratio: 0.56, words: No, sweet.\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. Same as the guy. Textbook answer is DevOps is a culture. So in other words, the whole team knows how to do operations, some parts of operations as well as development. Giving the responsibility of what operators normally do to developers. It's kind of a wide term, so depending on where you go, different organisations, different teams have different definition of it. Yeah, so non-textbook is how it comes. It's not a new thing, actually it's been around since 1993. Since the Agile Manifesto came out. Like, the then learning in school, school cannot teach us. But, it costs 2 unique units. Basically, we have to run our own service like that. So by running it, you probably have to do a lot of things to get it running. Maybe start learning. No. No, I don't know. Two once. Oh, I don't like that. I think. Six. How do you say it? Wait, no, I shouldn't. Two for nine. You've been based on my wall. You've been based on my wall. you can start growing your own vegetables and cooking them so that you manage the whole thing end to end maybe do everything yourself. Okay so previously right, assuming you got two people so one person just quote right and pass everything to a fan, he don't care. So you cook the egg, you cook one giant piece, then you just give it to the person. Then the person is like, wow, you have to cut out the piece. So it's like double work. So now right, I cook with it, I cut up for you, you go to the front, then I cash in more efficient, right? and then you can apply the support inside. So My Korea's Future is a platform created to reduce unemployment in local PMAs, especially those in the 30s and 40s who find themselves unemployed and they are unable to rejoin the workforce. So we do this through different ways. So instead of matching people's jobs, we match skills to jobs, for example. Definitely mess. I'm not sure if I can do it right now I'll try to do it I'm not sure. I'm not sure. I'm not sure. | No, sweet.\n",
      "Time elapsed: 3.88s\n",
      "\n",
      "Transcribing 171.00->189.00s...\n",
      "SEG 171.00->171.96, seek:173.00, final:False, compression_ratio: 0.64, words: No, not sweet.\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. Same as the guy. Textbook answer is DevOps is a culture. So in other words, the whole team knows how to do operations, some parts of operations as well as development. Giving the responsibility of what operators normally do to developers. It's kind of a wide term, so depending on where you go, different organisations, different teams have different definition of it. Yeah, so non-textbook is how it comes. It's not a new thing, actually it's been around since 1993. Since the Agile Manifesto came out. Like, the then learning in school, school cannot teach us. But, it costs 2 unique units. Basically, we have to run our own service like that. So by running it, you probably have to do a lot of things to get it running. Maybe start learning. No. No, I don't know. Two once. Oh, I don't like that. I think. Six. How do you say it? Wait, no, I shouldn't. Two for nine. You've been based on my wall. You've been based on my wall. you can start growing your own vegetables and cooking them so that you manage the whole thing end to end maybe do everything yourself. Okay so previously right, assuming you got two people so one person just quote right and pass everything to a fan, he don't care. So you cook the egg, you cook one giant piece, then you just give it to the person. Then the person is like, wow, you have to cut out the piece. So it's like double work. So now right, I cook with it, I cut up for you, you go to the front, then I cash in more efficient, right? and then you can apply the support inside. So My Korea's Future is a platform created to reduce unemployment in local PMAs, especially those in the 30s and 40s who find themselves unemployed and they are unable to rejoin the workforce. So we do this through different ways. So instead of matching people's jobs, we match skills to jobs, for example. Definitely mess. I'm not sure if I can do it right now I'll try to do it I'm not sure. I'm not sure. I'm not sure. | No, not sweet.\n",
      "Time elapsed: 3.77s\n",
      "\n",
      "Transcribing 171.00->196.00s...\n",
      "SEG 171.00->171.96, seek:191.00, final:True, compression_ratio: 1.61, words: No, I'm not sweet.\n",
      "SEG 174.86->178.04, seek:191.00, final:True, compression_ratio: 1.61, words: Hi, my name is Ryan. I work in DevOps because I think it's cool\n",
      "SEG 178.04->180.56, seek:191.00, final:True, compression_ratio: 1.61, words: and I have a lot of control over the things that I work at\n",
      "SEG 180.56->183.76, seek:191.00, final:True, compression_ratio: 1.61, words: and I can play a Chinese instrument at the Earthwool.\n",
      "SEG 184.46->188.58, seek:191.00, final:True, compression_ratio: 1.61, words: So I'm Joseph and I like DevOps because it's a lot more challenging than development work.\n",
      "SEG 189.02->191.28, seek:191.00, final:True, compression_ratio: 1.61, words: So one interesting thing about me is that I practice martial arts,\n",
      "SEG 191.60->192.54, seek:191.00, final:False, compression_ratio: 1.61, words: being tuned to be specific.\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. Same as the guy. Textbook answer is DevOps is a culture. So in other words, the whole team knows how to do operations, some parts of operations as well as development. Giving the responsibility of what operators normally do to developers. It's kind of a wide term, so depending on where you go, different organisations, different teams have different definition of it. Yeah, so non-textbook is how it comes. It's not a new thing, actually it's been around since 1993. Since the Agile Manifesto came out. Like, the then learning in school, school cannot teach us. But, it costs 2 unique units. Basically, we have to run our own service like that. So by running it, you probably have to do a lot of things to get it running. Maybe start learning. No. No, I don't know. Two once. Oh, I don't like that. I think. Six. How do you say it? Wait, no, I shouldn't. Two for nine. You've been based on my wall. You've been based on my wall. you can start growing your own vegetables and cooking them so that you manage the whole thing end to end maybe do everything yourself. Okay so previously right, assuming you got two people so one person just quote right and pass everything to a fan, he don't care. So you cook the egg, you cook one giant piece, then you just give it to the person. Then the person is like, wow, you have to cut out the piece. So it's like double work. So now right, I cook with it, I cut up for you, you go to the front, then I cash in more efficient, right? and then you can apply the support inside. So My Korea's Future is a platform created to reduce unemployment in local PMAs, especially those in the 30s and 40s who find themselves unemployed and they are unable to rejoin the workforce. So we do this through different ways. So instead of matching people's jobs, we match skills to jobs, for example. Definitely mess. I'm not sure if I can do it right now I'll try to do it I'm not sure. I'm not sure. I'm not sure. No, I'm not sweet. Hi, my name is Ryan. I work in DevOps because I think it's cool and I have a lot of control over the things that I work at and I can play a Chinese instrument at the Earthwool. So I'm Joseph and I like DevOps because it's a lot more challenging than development work. So one interesting thing about me is that I practice martial arts, | being tuned to be specific.\n",
      "Time elapsed: 6.35s\n",
      "\n",
      "Transcribing 191.00->198.81s...\n",
      "SEG 191.16->192.50, seek:193.00, final:False, compression_ratio: 0.80, words: That's Wing Chun to be specific.\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. Same as the guy. Textbook answer is DevOps is a culture. So in other words, the whole team knows how to do operations, some parts of operations as well as development. Giving the responsibility of what operators normally do to developers. It's kind of a wide term, so depending on where you go, different organisations, different teams have different definition of it. Yeah, so non-textbook is how it comes. It's not a new thing, actually it's been around since 1993. Since the Agile Manifesto came out. Like, the then learning in school, school cannot teach us. But, it costs 2 unique units. Basically, we have to run our own service like that. So by running it, you probably have to do a lot of things to get it running. Maybe start learning. No. No, I don't know. Two once. Oh, I don't like that. I think. Six. How do you say it? Wait, no, I shouldn't. Two for nine. You've been based on my wall. You've been based on my wall. you can start growing your own vegetables and cooking them so that you manage the whole thing end to end maybe do everything yourself. Okay so previously right, assuming you got two people so one person just quote right and pass everything to a fan, he don't care. So you cook the egg, you cook one giant piece, then you just give it to the person. Then the person is like, wow, you have to cut out the piece. So it's like double work. So now right, I cook with it, I cut up for you, you go to the front, then I cash in more efficient, right? and then you can apply the support inside. So My Korea's Future is a platform created to reduce unemployment in local PMAs, especially those in the 30s and 40s who find themselves unemployed and they are unable to rejoin the workforce. So we do this through different ways. So instead of matching people's jobs, we match skills to jobs, for example. Definitely mess. I'm not sure if I can do it right now I'll try to do it I'm not sure. I'm not sure. I'm not sure. No, I'm not sweet. Hi, my name is Ryan. I work in DevOps because I think it's cool and I have a lot of control over the things that I work at and I can play a Chinese instrument at the Earthwool. So I'm Joseph and I like DevOps because it's a lot more challenging than development work. So one interesting thing about me is that I practice martial arts, | That's Wing Chun to be specific.\n",
      "Time elapsed: 3.64s\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-09 s\n",
       "\n",
       "Total time: 123.883 s\n",
       "File: /tmp/ipykernel_105440/397555353.py\n",
       "Function: run_audio at line 20\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    20                                           def run_audio(audio_file, faster_model_live):\n",
       "    21         1   10578624.0 10578624.0      0.0    wav, fs = sf.read(audio_file)\n",
       "    22         1        680.0    680.0      0.0    wav_len = len(wav)\n",
       "    23                                             \n",
       "    24         1        442.0    442.0      0.0    RATE = 16000\n",
       "    25         1         95.0     95.0      0.0    CHUNK_S = 7\n",
       "    26         1        716.0    716.0      0.0    CHUNK = CHUNK_S * RATE\n",
       "    27                                             # max_time = 440.0\n",
       "    28         1        136.0    136.0      0.0    max_time = -1\n",
       "    29         1        290.0    290.0      0.0    compression_ratio_threshold = 2.4\n",
       "    30                                             \n",
       "    31         1        163.0    163.0      0.0    if max_time > 0:\n",
       "    32                                                 wav_len = min(wav_len, int(max_time*RATE))\n",
       "    33         1       4742.0   4742.0      0.0    chunk_starts = list(range(0, wav_len, CHUNK))\n",
       "    34         1        128.0    128.0      0.0    final_text = ''\n",
       "    35         1        105.0    105.0      0.0    seek = 0.0\n",
       "    36        29      19607.0    676.1      0.0    for chunk_start in chunk_starts[0:]:\n",
       "    37        29      12904.0    445.0      0.0        t1 = time.time()\n",
       "    38        29       5004.0    172.6      0.0        non_final_text = ''\n",
       "    39        29       4311.0    148.7      0.0        repeat_text = ''\n",
       "    40        29      68266.0   2354.0      0.0        start_idx = min(chunk_start, int(seek*RATE))\n",
       "    41        29      17247.0    594.7      0.0        end_idx = min(wav_len, chunk_start + CHUNK)\n",
       "    42                                             \n",
       "    43                                                 # Make sure audio chunk is less than 30s\n",
       "    44                                                 # Skip to most recent 30s if longer\n",
       "    45        29      30709.0   1058.9      0.0        start_idx = max(start_idx, end_idx - 30*RATE)\n",
       "    46        29      44015.0   1517.8      0.0        start_time = start_idx/RATE\n",
       "    47        29       8057.0    277.8      0.0        end_time = end_idx/RATE\n",
       "    48                                             \n",
       "    49        29     267435.0   9221.9      0.0        print(f'Transcribing {start_time:.2f}->{end_time:.2f}s...')\n",
       "    50        29    8486217.0 292628.2      0.0        wav_chunk = np.array(wav[start_idx:end_idx], dtype=np.float32)\n",
       "    51                                                 \n",
       "    52        29 6804787495.0 234647844.7      5.5        segments_live, info = faster_model_live.transcribe(wav_chunk, beam_size=1, temperature=0, vad_filter=False, word_timestamps=True)\n",
       "    53        29      18243.0    629.1      0.0        seek = start_time\n",
       "    54        83 117054574082.0 1410296073.3     94.5        for seg in segments_live:\n",
       "    55                                                     # print(seg)\n",
       "    56        83    3047178.0  36713.0      0.0            print(f'SEG {start_time+seg.start:.2f}->{start_time+seg.end:.2f}, seek:{start_time+seg.seek_live:.2f}'\n",
       "    57        83     216563.0   2609.2      0.0                    + f', final:{seg.final}, compression_ratio: {seg.compression_ratio:.2f}, words: {seg.text.lstrip()}')\n",
       "    58                                             \n",
       "    59        83      37474.0    451.5      0.0            if seg.compression_ratio > compression_ratio_threshold:\n",
       "    60                                                         repeat_text += seg.text\n",
       "    61                                             \n",
       "    62        55      11496.0    209.0      0.0            elif seg.final:\n",
       "    63        55      42226.0    767.7      0.0                final_text += seg.text\n",
       "    64        55      23703.0    431.0      0.0                seek = start_time + seg.seek_live\n",
       "    65                                                     else:\n",
       "    66        28       7207.0    257.4      0.0                non_final_text = seg.text\n",
       "    67                                             \n",
       "    68        29      59319.0   2045.5      0.0        t2 = time.time()\n",
       "    69        29      12485.0    430.5      0.0        time_elapsed = t2 - t1\n",
       "    70        29     355550.0  12260.3      0.0        print(f'Output: {final_text.lstrip()} | {non_final_text.lstrip()}')\n",
       "    71        29       9508.0    327.9      0.0        if repeat_text:\n",
       "    72                                                     print(f'REPEAT: {repeat_text.lstrip()}')\n",
       "    73        29     217923.0   7514.6      0.0        print(f'Time elapsed: {time_elapsed:0.2f}s\\n')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f run_audio run_audio(audio_file, faster_model_live)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc551fb4-0d5f-44d9-b7d7-e74f1bfd19e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
