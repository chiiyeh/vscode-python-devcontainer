{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15094082-ef6f-4923-a1bb-3a66116ebd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CT2_USE_MKL\"] = \"0\"\n",
    "os.environ[\"CT2_VERBOSE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0039febb-6edb-454e-a045-b5739949e9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-18 06:49:23.029] [ctranslate2] [thread 2910] [info] Loaded model /home/chiiyeh/.cache/huggingface/hub/models--guillaumekln--faster-whisper-small.en/snapshots/e0e3c0a16c844a994ca4d6d1318ce35f68236052 on device cpu:0\n",
      "[2023-04-18 06:49:23.029] [ctranslate2] [thread 2910] [info]  - Binary version: 6\n",
      "[2023-04-18 06:49:23.029] [ctranslate2] [thread 2910] [info]  - Model specification revision: 3\n",
      "[2023-04-18 06:49:23.029] [ctranslate2] [thread 2910] [info]  - Selected compute type: float32\n",
      "[2023-04-18 06:49:23.029] [ctranslate2] [thread 2910] [warning] The compute type inferred from the saved model is float16, but the target device or backend do not support efficient float16 computation. The model weights have been automatically converted to use the float32 compute type instead.\n"
     ]
    }
   ],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "\n",
    "model_size = \"small.en\"\n",
    "\n",
    "# Run on GPU with FP16\n",
    "model = WhisperModel(model_size, device=\"cpu\", cpu_threads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d015a2cb-9fa1-4b50-aec8-363cee1343fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language 'en' with probability 1.000000\n",
      "[0.00s -> 12.56s]  Welcome to My Friend Tell Me One. Today, we're going to speak to the Go Bros, two engineers\n",
      "[12.56s -> 13.96s]  who work in DevOps.\n",
      "[13.96s -> 19.84s]  Okay, hi, my name is Ryan. I work on my course feature. So basically, I'm doing DevOps.\n",
      "[20.08s -> 20.96s]  Same as the guy.\n",
      "[28.40s -> 34.64s]  The textbook answer is DevOps is a culture. So in other words, the whole team knows how to do\n",
      "[34.64s -> 39.84s]  operations, some parts of operations as well as development, giving the responsibility of\n",
      "[39.84s -> 42.40s]  what operators normally do to developers.\n",
      "[42.40s -> 46.24s]  It's kind of a wide term. So depending on where you go, different organizations,\n",
      "[46.24s -> 55.68s]  different teams have different definition of it. So yeah, so it's not a new thing. Actually,\n",
      "[55.68s -> 58.56s]  it's been around since 1993, since the Agile Manifesto came about.\n",
      "[62.32s -> 67.36s]  Like, the 10th learning is cool. School cannot teach us. But it costs two units, really.\n",
      "[67.36s -> 73.04s]  Basically, we have to run our own services. So by running it, you probably have to do a lot of\n",
      "[73.04s -> 76.00s]  things to get it running. So that's when we start learning.\n",
      "[103.04s -> 109.76s]  It means you can start growing your own vegetables and cooking them, so that you manage the whole\n",
      "[109.76s -> 112.40s]  thing end to end. Maybe? Do everything yourself.\n",
      "[112.40s -> 116.72s]  Okay, so previously, right, assuming you've got two people, so one person just cook, right,\n",
      "[116.72s -> 120.56s]  and pass everything to a fan. You don't care what. So let's say you cook the egg, right,\n",
      "[120.56s -> 123.36s]  you cook one giant piece, then you just give it to the person. Then the front person is like,\n",
      "[123.36s -> 127.52s]  wow, you have to cut up the piece, you see. So it's like double work. So now, right, it's like,\n",
      "[127.52s -> 130.16s]  I cook already, I cut up for you, you go to the fan, then I catch it more efficiently, right?\n",
      "[130.16s -> 131.68s]  Of course, you just take one slice, put it inside.\n",
      "[136.24s -> 140.80s]  Okay, so Micros Future is a platform created to reduce unemployment in local PMAs, especially\n",
      "[140.80s -> 144.88s]  those in the 30s and 40s who find themselves unemployed and they are unable to rejoin the\n",
      "[144.88s -> 150.48s]  workforce. So we do this through different ways. So instead of matching people to jobs,\n",
      "[150.48s -> 152.00s]  we match skills to jobs, for example.\n",
      "[156.24s -> 157.76s]  Yeah, definitely, best.\n",
      "[160.16s -> 164.72s]  Yeah, I'm going to ask you why you don't want to do this. How do you answer it?\n",
      "[164.72s -> 166.48s]  I don't know.\n",
      "[166.48s -> 167.20s]  I don't know.\n",
      "[167.20s -> 168.00s]  I don't know.\n",
      "[168.00s -> 169.44s]  I don't know. I don't know. One game, right?\n",
      "[169.44s -> 171.92s]  Oh, Coco, actually, no. No, sweet.\n",
      "[174.80s -> 179.12s]  Hi, my name is Ryan. I work in DevOps because I think it's cool, and I have a lot of control\n",
      "[179.12s -> 183.68s]  over the things that I work at. And I can play a Chinese instrument, the Erhu.\n",
      "[184.32s -> 188.72s]  So I'm Joseph, and I like DevOps because it's a lot more challenging than development work.\n",
      "[188.72s -> 191.44s]  So one interesting thing about me is that I practice martial arts,\n",
      "[191.44s -> 192.56s]  Wing Chun to be specific.\n",
      "Time: 56.83277916908264\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "segments, info = model.transcribe(\"GovTech_MFTMEP1_16000_mono_16.wav\", beam_size=5, language='en')\n",
    "\n",
    "print(\"Detected language '%s' with probability %f\" % (info.language, info.language_probability))\n",
    "\n",
    "for segment in segments:\n",
    "    print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n",
    "\n",
    "print(f'Time: {time.time() - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "078f60bd-b41b-4e54-872e-e482e39e9853",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'LD_LIBRARY_PATH'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLD_LIBRARY_PATH\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/os.py:679\u001b[0m, in \u001b[0;36m_Environ.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    676\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencodekey(key)]\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;66;03m# raise KeyError with the original key value\u001b[39;00m\n\u001b[0;32m--> 679\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecodevalue(value)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'LD_LIBRARY_PATH'"
     ]
    }
   ],
   "source": [
    "os.environ['LD_LIBRARY_PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b69d7d2-77a7-40a9-8a88-a5e160e2bf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "febc2641-521e-49a7-8d0f-4a0d4311b91f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model.bin:   0%|                                                                                                                                        | 0.00/1.53G [00:00<?, ?B/s]\n",
      "Downloading (…)1d2350ce/config.json:   0%|                                                                                                                          | 0.00/2.64k [00:00<?, ?B/s]\u001b[A\n",
      "Downloading (…)1d2350ce/config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2.64k/2.64k [00:00<00:00, 83.5kB/s]\u001b[A\n",
      "\n",
      "\n",
      "Downloading (…)350ce/vocabulary.txt:   0%|                                                                                                                           | 0.00/422k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading (…)350ce/vocabulary.txt: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 422k/422k [00:00<00:00, 2.64MB/s]\u001b[A\u001b[A\n",
      "Downloading model.bin:   1%|█▋                                                                                                                             | 21.0M/1.53G [00:01<01:12, 20.8MB/s]\n",
      "Downloading (…)350ce/tokenizer.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2.13M/2.13M [00:01<00:00, 1.73MB/s]\u001b[A\n",
      "Downloading model.bin: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.53G/1.53G [01:11<00:00, 21.3MB/s]\n",
      "[2023-10-06 07:34:20.236] [ctranslate2] [thread 38125] [warning] The compute type inferred from the saved model is float16, but the target device or backend do not support efficient float16 computation. The model weights have been automatically converted to use the float32 compute type instead.\n"
     ]
    }
   ],
   "source": [
    "from stream import WhisperModel\n",
    "import logging, sys\n",
    "\n",
    "model_size = \"medium.en\"\n",
    "\n",
    "# Run on GPU with FP16\n",
    "# model = WhisperModel(model_size, compute_type=\"int8\", device=\"cpu\", cpu_threads=0)\n",
    "model = WhisperModel(model_size, download_root='/home/chiiyeh/dev/ct_model')\n",
    "consoleHandler = logging.StreamHandler(stream=sys.stdout)\n",
    "consoleHandler.setLevel(logging.INFO)\n",
    "model.logger.addHandler(consoleHandler)\n",
    "model.logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "807ef079-2e2e-4976-9508-87a827647798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/chiiyeh/dev: Is a directory\n"
     ]
    }
   ],
   "source": [
    "!$PWD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8ec425d-5340-4e0b-ab9c-d6aa5a9f3504",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer, options, stream_options = model.init_options(\n",
    "    beam_size=1,\n",
    "    best_of=1,\n",
    "    condition_on_previous_text=True, \n",
    "    max_prompt_tokens=100,\n",
    "    drop_dup_prompt=True,\n",
    "    is_prompt_sentence=True,\n",
    "    finalised_segment_gap=3,\n",
    "    rm_seconds=0.3, \n",
    "    word_timestamps=True, \n",
    "    without_timestamps=True, \n",
    "    # log_prob_threshold=-2, \n",
    "    max_tokens_per_second=5,\n",
    "    use_prefix=False,\n",
    "    prefix_drop_num_tokens=4,\n",
    "    drop_prefix_duration=0,\n",
    "    )\n",
    "# model.warm_start(tokenizer, options, stream_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b5682d1-e897-431d-8ae9-a2e0ba23f029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time offset: 0.0\n",
      "segment duration: 5.0\n",
      "time offset: 0.0\n",
      "segment duration: 15.0\n",
      "time offset: 0.0\n",
      "segment duration: 8.0\n"
     ]
    }
   ],
   "source": [
    "model.warm_start(tokenizer, options, stream_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c9eb77f-a4b8-44d6-9cc6-4fc00f9cd066",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time offset: 0.0\n",
      "segment duration: 3.0\n",
      "Processing segment at 00:00.000\n",
      "Compression Ratio: 0.3333333333333333\n",
      "Temperature: 0\n",
      "token length: 5\n",
      "limit: 18\n",
      "Text: \n",
      "Not Finalised: 0.0 2.92  🎵\n",
      "Processing time: 4.265393501999824\n",
      "time offset: 0.0\n",
      "segment duration: 7.2700000000000005\n",
      "Processing segment at 00:00.000\n",
      "Compression Ratio: 0.5\n",
      "Temperature: 0\n",
      "token length: 4\n",
      "limit: 44\n",
      "Text: \n",
      "Not Finalised: 0.0 7.24  Welcome.\n",
      "Processing time: 5.012761594999574\n",
      "time offset: 0.0\n",
      "segment duration: 12.280000000000001\n",
      "Processing segment at 00:00.000\n",
      "Compression Ratio: 1.0459770114942528\n",
      "Temperature: 0\n",
      "token length: 28\n",
      "limit: 74\n",
      "Text: \n",
      "0.0 9.6  Welcome to My Friend Tell Me One.\n",
      "Not Finalised: 9.6 11.76  Today we're going to speak to the Go Bros.\n",
      "Not Finalised: 11.76 12.26  Two engineers.\n",
      "Processing time: 5.809826906000126\n",
      "time offset: 9.6\n",
      "segment duration: 8.49\n",
      "Processing segment at 00:09.600\n",
      "Compression Ratio: 1.1578947368421053\n",
      "Temperature: 0\n",
      "token length: 39\n",
      "limit: 51\n",
      "Text: \n",
      "9.6 13.76  Today we're going to speak to the Go Bros, two engineers who work in DevOps.\n",
      "Not Finalised: 14.32 17.439999999999998  Okay, hi, my name is Ryan. I work on my course feature.\n",
      "Processing time: 6.06151116700039\n",
      "time offset: 13.76\n",
      "segment duration: 10.39\n",
      "Processing segment at 00:13.760\n",
      "Compression Ratio: 1.08\n",
      "Temperature: 0\n",
      "token length: 36\n",
      "limit: 62\n",
      "Text: \n",
      "13.76 17.759999999999998  Okay, hi, my name is Ryan. I work on my current feature.\n",
      "17.759999999999998 19.759999999999998  So basically I'm doing DevOps.\n",
      "19.759999999999998 21.759999999999998  See you in a second.\n",
      "Processing time: 5.55835193799976\n",
      "time offset: 21.76\n",
      "segment duration: 7.95\n",
      "Processing segment at 00:21.760\n",
      "Compression Ratio: 0.7142857142857143\n",
      "Temperature: 0\n",
      "token length: 6\n",
      "limit: 48\n",
      "Text: \n",
      "Not Finalised: 21.76 29.720000000000002  Thanks for watching!\n",
      "Processing time: 4.69282291699983\n",
      "time offset: 21.76\n",
      "segment duration: 12.64\n",
      "Processing segment at 00:21.760\n",
      "Compression Ratio: 0.7142857142857143\n",
      "Temperature: 0\n",
      "token length: 6\n",
      "limit: 76\n",
      "Text: \n",
      "21.76 28.76  Thanks for watching!\n",
      "Processing time: 4.896004799999901\n",
      "time offset: 28.76\n",
      "segment duration: 10.540000000000001\n",
      "Processing segment at 00:28.760\n",
      "Compression Ratio: 1.3597122302158273\n",
      "Temperature: 0\n",
      "token length: 45\n",
      "limit: 63\n",
      "Text: \n",
      "28.76 30.76  The answer is DevOps is a culture.\n",
      "30.76 37.760000000000005  So in other words, the whole team knows how to do operations, some parts of operations as well as development.\n",
      "Not Finalised: 37.760000000000005 39.760000000000005  Giving the responsibility to the community.\n",
      "Processing time: 5.723195726000085\n",
      "time offset: 37.76\n",
      "segment duration: 7.26\n",
      "Processing segment at 00:37.760\n",
      "Compression Ratio: 1.1919191919191918\n",
      "Temperature: 0\n",
      "token length: 27\n",
      "limit: 44\n",
      "Text: \n",
      "37.76 42.16  giving the responsibility of what operators normally do to developers.\n",
      "Not Finalised: 42.16 45.04  It's kind of a wide term, so depending on where\n",
      "Processing time: 5.422503478999715\n",
      "time offset: 42.160000000000004\n",
      "segment duration: 8.290000000000001\n",
      "Processing segment at 00:42.160\n",
      "Compression Ratio: 1.3131313131313131\n",
      "Temperature: 0\n",
      "token length: 28\n",
      "limit: 50\n",
      "Text: \n",
      "Not Finalised: 42.160000000000004 49.760000000000005  It's kind of a white term, so depending on where you go, different organizations, different teams have different definition of it.\n",
      "Processing time: 5.062937017999502\n",
      "time offset: 42.160000000000004\n",
      "segment duration: 13.35\n",
      "Processing segment at 00:42.160\n",
      "Compression Ratio: 1.4015151515151516\n",
      "Temperature: 0\n",
      "token length: 50\n",
      "limit: 80\n",
      "Text: \n",
      "42.160000000000004 49.160000000000004  It's kind of a wide term, so depending on where you go, different organizations, different teams have different definition of it.\n",
      "49.160000000000004 52.160000000000004  Yeah, so non-taxable is like a...\n",
      "Not Finalised: 55.160000000000004 56.160000000000004  It's not a new thing.\n",
      "Processing time: 5.905666038999698\n",
      "time offset: 52.160000000000004\n",
      "segment duration: 9.25\n",
      "Processing segment at 00:52.160\n",
      "Compression Ratio: 1.043956043956044\n",
      "Temperature: 0\n",
      "token length: 26\n",
      "limit: 56\n",
      "Text: \n",
      "52.160000000000004 58.64  It's not a new thing. Actually it's been around since 1993. Since the Agile Manifesto came out.\n",
      "Processing time: 5.240478951000114\n",
      "time offset: 58.64\n",
      "segment duration: 8.01\n",
      "Processing segment at 00:58.640\n",
      "Compression Ratio: 0.8461538461538461\n",
      "Temperature: 0\n",
      "token length: 14\n",
      "limit: 48\n",
      "Text: \n",
      "58.64 64.64  I didn't learn in school, so I cannot teach.\n",
      "Processing time: 5.150501469000119\n",
      "time offset: 64.64\n",
      "segment duration: 7.17\n",
      "Processing segment at 01:04.640\n",
      "Compression Ratio: 1.1170212765957446\n",
      "Temperature: 0\n",
      "token length: 31\n",
      "limit: 43\n",
      "Text: \n",
      "64.64 67.48  But cost 2 unique earnings.\n",
      "67.48 70.28  Basically, we have to run our own service like that.\n",
      "Not Finalised: 70.28 71.76  So by running it, you...\n",
      "Processing time: 6.010438264000186\n",
      "time offset: 70.28\n",
      "segment duration: 7.54\n",
      "Processing segment at 01:10.280\n",
      "Compression Ratio: 1.2359550561797752\n",
      "Temperature: 0\n",
      "token length: 31\n",
      "limit: 45\n",
      "Text: \n",
      "70.28 74.6  So by running it, you probably have to do a lot of things to get it running.\n",
      "74.6 76.08  So that's when we start learning.\n",
      "Processing time: 5.315303442999721\n",
      "time offset: 76.08\n",
      "segment duration: 7.05\n",
      "Processing segment at 01:16.080\n",
      "Compression Ratio: 0.68\n",
      "Temperature: 0\n",
      "token length: 9\n",
      "limit: 42\n",
      "Text: \n",
      "76.08 81.08  No, I don't know.\n",
      "Processing time: 4.942185441999754\n",
      "time offset: 81.08\n",
      "segment duration: 6.99\n",
      "Processing segment at 01:21.080\n",
      "Compression Ratio: 0.5\n",
      "Temperature: 0\n",
      "token length: 5\n",
      "limit: 42\n",
      "Text: \n",
      "Not Finalised: 81.08 88.08  I think.\n",
      "Processing time: 4.793081171999802\n",
      "time offset: 81.08\n",
      "segment duration: 11.790000000000001\n",
      "Processing segment at 01:21.080\n",
      "Compression Ratio: 1.0294117647058822\n",
      "Temperature: 0\n",
      "token length: 41\n",
      "limit: 71\n",
      "Text: \n",
      "81.08 86.08  I don't like that.\n",
      "86.08 87.08  7?\n",
      "87.08 88.08  I think.\n",
      "88.08 89.08  6.\n",
      "89.08 90.08  How do you say it?\n",
      "90.08 91.08  2 for 9.\n",
      "Not Finalised: 91.08 92.08  2 for 9.\n",
      "Processing time: 6.274514253000234\n",
      "time offset: 91.08\n",
      "segment duration: 8.06\n",
      "Processing segment at 01:31.080\n",
      "Compression Ratio: 1.2571428571428571\n",
      "Temperature: 0\n",
      "token length: 21\n",
      "limit: 48\n",
      "Text: \n",
      "91.08 91.8  I'm not sure.\n",
      "91.8 92.44  You're not sure?\n",
      "95.44 96.08  I'm not sure.\n",
      "Processing time: 5.038179870000022\n",
      "time offset: 96.08\n",
      "segment duration: 8.1\n",
      "Processing segment at 01:36.080\n",
      "Compression Ratio: 0.8571428571428571\n",
      "Temperature: 0\n",
      "token length: 17\n",
      "limit: 49\n",
      "Text: \n",
      "96.08 97.08  Wow.\n",
      "99.08 100.08  You seemed to have been amazing.\n",
      "100.08 101.08  Wow.\n",
      "Processing time: 5.006999279999945\n",
      "time offset: 101.08\n",
      "segment duration: 8.11\n",
      "Processing segment at 01:41.080\n",
      "Compression Ratio: 1.0972222222222223\n",
      "Temperature: 0\n",
      "token length: 17\n",
      "limit: 49\n",
      "Text: \n",
      "Not Finalised: 101.08 109.16  It means you can start growing your own vegetables and cooking them so that you\n",
      "Processing time: 5.011608365999564\n",
      "time offset: 101.08\n",
      "segment duration: 13.120000000000001\n",
      "Processing segment at 01:41.080\n",
      "Compression Ratio: 1.3070866141732282\n",
      "Temperature: 0\n",
      "token length: 43\n",
      "limit: 79\n",
      "Text: \n",
      "101.08 109.84  It means you can start growing your own vegetables and cooking them so that you manage the whole\n",
      "109.84 110.84  thing end to end.\n",
      "110.84 111.84  Maybe?\n",
      "111.84 112.84  Do everything yourself.\n",
      "Not Finalised: 112.84 113.72  So previously right?\n",
      "Processing time: 5.948796933000267\n",
      "time offset: 112.84\n",
      "segment duration: 7.3100000000000005\n",
      "Processing segment at 01:52.840\n",
      "Compression Ratio: 1.2100840336134453\n",
      "Temperature: 0\n",
      "token length: 44\n",
      "limit: 44\n",
      "Text: \n",
      "112.84 114.36  So previously, right?\n",
      "114.36 115.52000000000001  Assuming you got two people,\n",
      "115.52000000000001 116.94  so one person just code right\n",
      "116.94 118.0  and pass everything to a fan.\n",
      "Not Finalised: 118.0 119.60000000000001  You don't care about that.\n",
      "Not Finalised: 119.60000000000001 120.15  So let\n",
      "Processing time: 5.564467545000298\n",
      "time offset: 118.0\n",
      "segment duration: 7.71\n",
      "Processing segment at 01:58.000\n",
      "Compression Ratio: 1.3017241379310345\n",
      "Temperature: 0\n",
      "token length: 46\n",
      "limit: 46\n",
      "Text: \n",
      "118.0 119.6  You don't care.\n",
      "119.6 120.76  So let's say you cook the egg, right?\n",
      "120.76 122.88  You cook one giant piece, then you just give it to the person.\n",
      "Not Finalised: 122.88 125.71  Then the front person is like, wow\n",
      "Processing time: 5.943147259000398\n",
      "time offset: 122.88\n",
      "segment duration: 8.77\n",
      "Processing segment at 02:02.880\n",
      "Compression Ratio: 1.3418803418803418\n",
      "Temperature: 0\n",
      "token length: 53\n",
      "limit: 53\n",
      "Text: \n",
      "122.88 125.67999999999999  And the front person is like, wow, you have to cut up the PCC.\n",
      "125.67999999999999 126.96  So it's like double work.\n",
      "126.96 129.28  So now, right, I cut up for you, you go to the front,\n",
      "Not Finalised: 129.28 131.65  then I cash in\n",
      "Processing time: 6.860820816999876\n",
      "time offset: 129.28\n",
      "segment duration: 9.24\n",
      "Processing segment at 02:09.280\n",
      "Compression Ratio: 1.208695652173913\n",
      "Temperature: 0\n",
      "token length: 40\n",
      "limit: 55\n",
      "Text: \n",
      "129.28 132.64000000000001  and a cache mode if you generate it, of course you just take one slice and put it inside.\n",
      "132.64000000000001 133.14000000000001  Ah.\n",
      "Not Finalised: 136.48 138.48  Okay, so MicroS future is a platform created.\n",
      "Processing time: 5.894048351999118\n",
      "time offset: 133.14000000000001\n",
      "segment duration: 11.27\n",
      "Processing segment at 02:13.140\n",
      "Compression Ratio: 1.286764705882353\n",
      "Temperature: 0\n",
      "token length: 42\n",
      "limit: 68\n",
      "Text: \n",
      "133.14000000000001 140.88000000000002  Okay, so MicroSuture is a platform created to reduce unemployment in local PMAs, especially\n",
      "Not Finalised: 140.88000000000002 144.44000000000003  those in the 30s and 40s who find themselves unemployed and they are unable to pay.\n",
      "Processing time: 5.5107044209999\n",
      "time offset: 140.88\n",
      "segment duration: 9.040000000000001\n",
      "Processing segment at 02:20.880\n",
      "Compression Ratio: 1.312\n",
      "Temperature: 0\n",
      "token length: 39\n",
      "limit: 54\n",
      "Text: \n",
      "140.88 145.88  those in the 30s and 40s who find themselves unemployed and they are unable to rejoin the workforce.\n",
      "Not Finalised: 145.88 149.88  So we do this through different ways. So instead of matching...\n",
      "Processing time: 5.73665832900042\n",
      "time offset: 145.88\n",
      "segment duration: 9.78\n",
      "Processing segment at 02:25.880\n",
      "Compression Ratio: 1.1875\n",
      "Temperature: 0\n",
      "token length: 29\n",
      "limit: 59\n",
      "Text: \n",
      "145.88 149.2  So we do this through different ways.\n",
      "149.2 152.2  So instead of matching people to jobs, we match skills to jobs, for example.\n",
      "Processing time: 5.4667141680001805\n",
      "time offset: 152.20000000000002\n",
      "segment duration: 8.92\n",
      "Processing segment at 02:32.200\n",
      "Compression Ratio: 0.7037037037037037\n",
      "Temperature: 0\n",
      "token length: 9\n",
      "limit: 54\n",
      "Text: \n",
      "Not Finalised: 152.20000000000002 161.08  I'm going to do it.\n",
      "Processing time: 4.950504424000428\n",
      "time offset: 152.20000000000002\n",
      "segment duration: 13.870000000000001\n",
      "Processing segment at 02:32.200\n",
      "Compression ratio threshold is not met with temperature 0.0 (3.337662 > 2.200000)\n",
      "Compression Ratio: 3.3376623376623376\n",
      "Temperature: 0\n",
      "token length: 83\n",
      "limit: 83\n",
      "Text: \n",
      "Not Finalised: 152.20000000000002 154.88000000000002  I'm going to ask you to ask me about your\n",
      "Not Finalised: 154.88000000000002 155.88000000000002  answer.\n",
      "Not Finalised: 155.88000000000002 156.88000000000002  How are your answers?\n",
      "Not Finalised: 156.88000000000002 157.88000000000002  Talk to me.\n",
      "Not Finalised: 157.88000000000002 158.88000000000002  I'm going to ask you to ask me about your answer.\n",
      "Not Finalised: 158.88000000000002 159.88000000000002  I'm going to ask you to ask me about your answer.\n",
      "Not Finalised: 159.88000000000002 160.88000000000002  I'm going to ask you to ask me about your answer.\n",
      "Not Finalised: 160.88000000000002 166.07000000000002  I'm going to ask you to\n",
      "Processing time: 6.758538883000256\n",
      "time offset: 152.20000000000002\n",
      "segment duration: 20.63\n",
      "Processing segment at 02:32.200\n",
      "Compression ratio threshold is not met with temperature 0.0 (7.380000 > 2.200000)\n",
      "Compression Ratio: 7.38\n",
      "Temperature: 0\n",
      "token length: 120\n",
      "limit: 124\n",
      "Text: \n",
      "Not Finalised: 152.20000000000002 154.20000000000002  I'm going to ask you about the game.\n",
      "Not Finalised: 154.20000000000002 156.20000000000002  I'm going to ask you about the game.\n",
      "Not Finalised: 156.20000000000002 158.20000000000002  I'm going to ask you about the game.\n",
      "Not Finalised: 158.20000000000002 160.20000000000002  I'm going to ask you about the game.\n",
      "Not Finalised: 160.20000000000002 162.20000000000002  I'm going to ask you about the game.\n",
      "Not Finalised: 162.20000000000002 164.20000000000002  I'm going to ask you about the game.\n",
      "Not Finalised: 164.20000000000002 166.20000000000002  I'm going to ask you about the game.\n",
      "Not Finalised: 166.20000000000002 168.20000000000002  I'm going to ask you about the game.\n",
      "Not Finalised: 168.20000000000002 170.20000000000002  I'm going to ask you about the game.\n",
      "Not Finalised: 170.20000000000002 172.20000000000002  I'm going to ask you about the game.\n",
      "Processing time: 7.480411190000268\n",
      "time offset: 152.20000000000002\n",
      "segment duration: 28.11\n",
      "Processing segment at 02:32.200\n",
      "Compression ratio threshold is not met with temperature 0.0 (3.443038 > 2.200000)\n",
      "Compression Ratio: 3.4430379746835444\n",
      "Temperature: 0\n",
      "token length: 169\n",
      "limit: 169\n",
      "Removing previous tokens!\n",
      "Text: \n",
      "152.20000000000002 154.20000000000002  I'm going to ask you about your own game.\n",
      "154.20000000000002 156.20000000000002  I'm going to ask you about your own game.\n",
      "156.20000000000002 158.20000000000002  How do you answer?\n",
      "158.20000000000002 160.20000000000002  I'm going to ask you about your own game.\n",
      "160.20000000000002 162.20000000000002  I'm going to ask you about your own game.\n",
      "162.20000000000002 164.20000000000002  I'm going to ask you about your own game.\n",
      "164.20000000000002 166.20000000000002  How do you answer?\n",
      "166.20000000000002 168.20000000000002  I'm going to ask you about your own game.\n",
      "168.20000000000002 170.20000000000002  I'm going to ask you about your own game.\n",
      "170.20000000000002 172.20000000000002  I'm going to ask you about your own game.\n",
      "172.20000000000002 174.20000000000002  I'm going to ask you about your own game.\n",
      "174.20000000000002 176.20000000000002  Hi, my name is Ryan.\n",
      "176.20000000000002 178.20000000000002  I work in Death Force because I think it's cool\n",
      "178.20000000000002 180.31  and I have a lot of control over the things that I work at.\n",
      "Processing time: 9.392756839999493\n",
      "time offset: 180.3\n",
      "segment duration: 9.41\n",
      "Processing segment at 03:00.300\n",
      "Compression Ratio: 1.3695652173913044\n",
      "Temperature: 0\n",
      "token length: 49\n",
      "limit: 56\n",
      "Text: \n",
      "180.3 184.3  and I can play a Chinese instrument at the Erhu.\n",
      "184.3 188.3  So I'm Joseph and I like DevOps because it's a lot more challenging than development work.\n",
      "Not Finalised: 188.3 190.3  So one interesting thing is that I'm a developer.\n",
      "Processing time: 5.64827749899996\n",
      "time offset: 188.3\n",
      "segment duration: 7.05\n",
      "Processing segment at 03:08.300\n",
      "Compression Ratio: 1.1\n",
      "Temperature: 0\n",
      "token length: 20\n",
      "limit: 42\n",
      "Text: \n",
      "188.3 192.70000000000002  So one interesting thing about me is that I practice martial arts, going to be specific.\n",
      "Processing time: 5.274116372000208\n",
      "time offset: 192.70000000000002\n",
      "segment duration: 6.1000000000000005\n",
      "Processing segment at 03:12.700\n",
      "Compression Ratio: 0.38461538461538464\n",
      "Temperature: 0\n",
      "No speech threshold is met (0.665885 > 0.600000)\n",
      "Processing time: 4.4805438630000936\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-09 s\n",
       "\n",
       "Total time: 205.161 s\n",
       "File: /home/chiiyeh/dev/stream.py\n",
       "Function: simulate_streaming at line 287\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   287                                               def simulate_streaming(\n",
       "   288                                                   self,\n",
       "   289                                                   audio_file: str,\n",
       "   290                                                   tokenizer: Tokenizer,\n",
       "   291                                                   options: TranscriptionOptions,\n",
       "   292                                                   stream_options: StreamOptions,\n",
       "   293                                                   min_interval: float = 3,\n",
       "   294                                               ):\n",
       "   295         1       3409.0   3409.0      0.0          sampling_rate = self.feature_extractor.sampling_rate\n",
       "   296         1   48143365.0 48143365.0      0.0          full_audio = decode_audio(audio_file, sampling_rate=sampling_rate)\n",
       "   297         1       2031.0   2031.0      0.0          audio_read = 0\n",
       "   298         1       5826.0   5826.0      0.0          import time\n",
       "   299                                           \n",
       "   300         1        724.0    724.0      0.0          idx = 0\n",
       "   301         1        842.0    842.0      0.0          seek = 0\n",
       "   302         1        894.0    894.0      0.0          previous_tokens_list = []\n",
       "   303         1        822.0    822.0      0.0          previous_text_list = []\n",
       "   304         1        361.0    361.0      0.0          prefix_tokens = []\n",
       "   305         1        526.0    526.0      0.0          initial_prompt_tokens = []\n",
       "   306         1        389.0    389.0      0.0          current_audio = None\n",
       "   307         1        455.0    455.0      0.0          temperature_idx = 0\n",
       "   308         1       3854.0   3854.0      0.0          previous_time = time.perf_counter()\n",
       "   309                                           \n",
       "   310         1       2323.0   2323.0      0.0          if options.initial_prompt is not None and options.initial_prompt.strip():\n",
       "   311                                                       initial_prompt = \" \" + options.initial_prompt.strip()\n",
       "   312                                                       initial_prompt_tokens = tokenizer.encode(initial_prompt)\n",
       "   313                                                       if not stream_options.propagate_initial_prompt:\n",
       "   314                                                           previous_tokens_list.append(initial_prompt_tokens)\n",
       "   315                                           \n",
       "   316        36      48414.0   1344.8      0.0          while audio_read < len(full_audio):\n",
       "   317        36      23905.0    664.0      0.0              current_time = time.perf_counter()\n",
       "   318        36      18378.0    510.5      0.0              time_diff = current_time - previous_time\n",
       "   319        35      27856.0    795.9      0.0              if time_diff < min_interval:\n",
       "   320         1 3008303905.0 3008303905.0      1.5                  time.sleep(min_interval-time_diff) # in seconds\n",
       "   321         1       6666.0   6666.0      0.0                  current_time = time.perf_counter()\n",
       "   322         1       4533.0   4533.0      0.0                  time_diff = current_time - previous_time\n",
       "   323                                           \n",
       "   324        36     212329.0   5898.0      0.0              samples_read = round(time_diff * sampling_rate)\n",
       "   325        36     779227.0  21645.2      0.0              new_audio = full_audio[audio_read:min(audio_read+samples_read, len(full_audio))]\n",
       "   326        36      26221.0    728.4      0.0              audio_read += samples_read\n",
       "   327                                           \n",
       "   328                                           \n",
       "   329        35      13740.0    392.6      0.0              if current_audio is not None:\n",
       "   330        35   12676794.0 362194.1      0.0                  current_audio = np.concatenate([current_audio, new_audio])\n",
       "   331                                                       else:\n",
       "   332         1        643.0    643.0      0.0                  current_audio = new_audio\n",
       "   333                                           \n",
       "   334        36      24540.0    681.7      0.0              (\n",
       "   335        36      58292.0   1619.2      0.0                  output_segments,\n",
       "   336        36      20598.0    572.2      0.0                  current_audio,\n",
       "   337        36      17378.0    482.7      0.0                  idx,\n",
       "   338        36      15579.0    432.8      0.0                  seek,\n",
       "   339        36      13768.0    382.4      0.0                  temperature_idx,\n",
       "   340        36      13537.0    376.0      0.0                  previous_tokens_list,\n",
       "   341        36      15844.0    440.1      0.0                  previous_text_list,\n",
       "   342        36      16749.0    465.2      0.0                  prefix_tokens,\n",
       "   343        36 202086337782.0 5613509382.8     98.5              ) = self.generate_segments(\n",
       "   344        36      21308.0    591.9      0.0                  current_audio,\n",
       "   345        36      19885.0    552.4      0.0                  tokenizer,\n",
       "   346        36      17419.0    483.9      0.0                  options,\n",
       "   347        36      13420.0    372.8      0.0                  stream_options,\n",
       "   348        36      16105.0    447.4      0.0                  idx,\n",
       "   349        36      15357.0    426.6      0.0                  seek,\n",
       "   350        36      12605.0    350.1      0.0                  temperature_idx,\n",
       "   351        36      16256.0    451.6      0.0                  previous_tokens_list,\n",
       "   352        36      17926.0    497.9      0.0                  previous_text_list,\n",
       "   353        36      17151.0    476.4      0.0                  prefix_tokens,\n",
       "   354        36       9964.0    276.8      0.0                  initial_prompt_tokens,\n",
       "   355                                                       )\n",
       "   356       109      76073.0    697.9      0.0              for seg in output_segments:\n",
       "   357        67      48056.0    717.3      0.0                  if seg.final:\n",
       "   358        67    2175801.0  32474.6      0.0                      print(f\"{seg.start} {seg.end} {seg.text}\")\n",
       "   359                                                           else:\n",
       "   360        42     873383.0  20794.8      0.0                      print(f\"Not Finalised: {seg.start} {seg.end} {seg.text}\")\n",
       "   361        36     512523.0  14236.8      0.0              print(f\"Processing time: {time.perf_counter() - current_time}\")\n",
       "   362                                           \n",
       "   363        36      30046.0    834.6      0.0              previous_time = current_time\n",
       "\n",
       "Total time: 202.078 s\n",
       "File: /home/chiiyeh/dev/stream.py\n",
       "Function: generate_segments at line 444\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   444                                               def generate_segments(\n",
       "   445                                                   self,\n",
       "   446                                                   audio: np.ndarray,\n",
       "   447                                                   tokenizer: Tokenizer,\n",
       "   448                                                   options: TranscriptionOptions,\n",
       "   449                                                   stream_options: StreamOptions,\n",
       "   450                                                   finalised_idx: int,\n",
       "   451                                                   seek: int,\n",
       "   452                                                   temperature_idx: int,\n",
       "   453                                                   previous_tokens_list: List[List[int]],\n",
       "   454                                                   previous_text_list: List[List[str]],\n",
       "   455                                                   prefix_tokens: List[int],\n",
       "   456                                                   initial_prompt_tokens: List[int],\n",
       "   457                                               ):\n",
       "   458        36      28477.0    791.0      0.0          seek_shift = 0\n",
       "   459        36      21481.0    596.7      0.0          output_segments = []\n",
       "   460        36      42261.0   1173.9      0.0          max_initial_timestamp = options.max_initial_timestamp\n",
       "   461        36      10617.0    294.9      0.0          finalise_all = False\n",
       "   462        36 9591186635.0 266421851.0      4.7          features = self.feature_extractor(audio)\n",
       "   463                                           \n",
       "   464                                                   #During the extraction of feat 30s of silence was appended at the\n",
       "   465                                                   #end, so to get content frames need to compensate this amount\n",
       "   466        36      24417.0    678.2      0.0          relative_content_frames = (\n",
       "   467        36     274214.0   7617.1      0.0              features.shape[-1] - self.feature_extractor.nb_max_frames\n",
       "   468                                                   )\n",
       "   469        36     172757.0   4798.8      0.0          time_offset = seek * self.feature_extractor.time_per_frame\n",
       "   470        36     355640.0   9878.9      0.0          segment = features[:, : self.feature_extractor.nb_max_frames]\n",
       "   471                                                   # to handle the case where the audio exceeds 30s\n",
       "   472        36     147283.0   4091.2      0.0          segment_size = min(\n",
       "   473        36      14034.0    389.8      0.0              self.feature_extractor.nb_max_frames, relative_content_frames\n",
       "   474                                                   )\n",
       "   475        36      34895.0    969.3      0.0          segment_duration = segment_size * self.feature_extractor.time_per_frame\n",
       "   476        36   55500954.0 1541693.2      0.0          self.logger.info(\"time offset: %s\\nsegment duration: %s\", time_offset, segment_duration)\n",
       "   477                                           \n",
       "   478        36     195402.0   5427.8      0.0          if self.logger.isEnabledFor(logging.DEBUG):\n",
       "   479        36   72659664.0 2018324.0      0.0              self.logger.debug(\"Processing segment at %s\", format_timestamp(time_offset))\n",
       "   480                                           \n",
       "   481        36     216902.0   6025.1      0.0          previous_tokens = list(itertools.chain.from_iterable(previous_tokens_list))\n",
       "   482        36     357250.0   9923.6      0.0          prompt = self.get_prompt(\n",
       "   483        36      22339.0    620.5      0.0              tokenizer,\n",
       "   484        36      16029.0    445.2      0.0              previous_tokens,\n",
       "   485        36      15761.0    437.8      0.0              stream_options,\n",
       "   486        36      18342.0    509.5      0.0              initial_prompt_tokens,\n",
       "   487        36     556864.0  15468.4      0.0              without_timestamps=options.without_timestamps,\n",
       "   488        36      19689.0    546.9      0.0              prefix=options.prefix if seek == 0 else None,\n",
       "   489                                                   )\n",
       "   490        36      72439.0   2012.2      0.0          self.max_length = max(\n",
       "   491        36     137809.0   3828.0      0.0              round(stream_options.max_tokens_per_second*segment_duration)*2,\n",
       "   492        36      46878.0   1302.2      0.0              len(prompt) + round(stream_options.max_tokens_per_second*segment_duration)\n",
       "   493                                                   )\n",
       "   494                                           \n",
       "   495        36      30600.0    850.0      0.0          if stream_options.use_prefix and len(prefix_tokens) > stream_options.prefix_drop_num_tokens:\n",
       "   496                                                       prompt.extend(prefix_tokens[:-stream_options.prefix_drop_num_tokens])\n",
       "   497                                                       self.logger.debug(\n",
       "   498                                                           \"Prefix Tokens Added: %s %s\", \n",
       "   499                                                           tokenizer.decode(prefix_tokens[:-stream_options.prefix_drop_num_tokens]), \n",
       "   500                                                           prefix_tokens[:-stream_options.prefix_drop_num_tokens]\n",
       "   501                                                           )\n",
       "   502                                                       # max_initial_timestamp = 25\n",
       "   503                                           \n",
       "   504        36 145965253752.0 4054590382.0     72.2          encoder_output = self.encode(segment)\n",
       "   505                                           \n",
       "   506        36      30146.0    837.4      0.0          (\n",
       "   507        36      15313.0    425.4      0.0              result,\n",
       "   508        36      12926.0    359.1      0.0              avg_logprob,\n",
       "   509        36      11929.0    331.4      0.0              temperature,\n",
       "   510        36       9756.0    271.0      0.0              compression_ratio,\n",
       "   511        36      13224.0    367.3      0.0              needs_fallback,\n",
       "   512        36 46244391446.0 1284566429.1     22.9          ) = self.generate_with_fallback(encoder_output, prompt, tokenizer, options, [self.temperatures[temperature_idx]], max_initial_timestamp)\n",
       "   513        36   35670888.0 990858.0      0.0          self.logger.debug(\"Compression Ratio: %s\\nTemperature: %s\", compression_ratio, temperature)\n",
       "   514                                           \n",
       "   515        36      62333.0   1731.5      0.0          if options.no_speech_threshold is not None:\n",
       "   516                                                       # no voice activity check\n",
       "   517        36     257132.0   7142.6      0.0              should_skip = result.no_speech_prob > options.no_speech_threshold\n",
       "   518                                           \n",
       "   519        36      22600.0    627.8      0.0              if (\n",
       "   520        36      27272.0    757.6      0.0                  options.log_prob_threshold is not None\n",
       "   521                                                           and avg_logprob > options.log_prob_threshold\n",
       "   522                                                       ):\n",
       "   523                                                           # don't skip if the logprob is high enough, despite the no_speech_prob\n",
       "   524                                                           should_skip = False\n",
       "   525                                           \n",
       "   526        35      17640.0    504.0      0.0              if should_skip:\n",
       "   527         1     642751.0 642751.0      0.0                  self.logger.debug(\n",
       "   528         1        308.0    308.0      0.0                      \"No speech threshold is met (%f > %f)\",\n",
       "   529         1       1685.0   1685.0      0.0                      result.no_speech_prob,\n",
       "   530         1        260.0    260.0      0.0                      options.no_speech_threshold,\n",
       "   531                                                           )\n",
       "   532                                           \n",
       "   533                                                           # fast-forward to the next segment boundary\n",
       "   534         1        420.0    420.0      0.0                  seek_shift = segment_size - 1  # compensating by one to prevent drifting.\n",
       "   535         1        468.0    468.0      0.0                  seek = seek + seek_shift\n",
       "   536         1       3945.0   3945.0      0.0                  audio = audio[\n",
       "   537         1      20639.0  20639.0      0.0                      min(seek_shift * self.feature_extractor.hop_length, len(audio)) :\n",
       "   538                                                           ]\n",
       "   539                                           \n",
       "   540         1        272.0    272.0      0.0                  return (\n",
       "   541         1        331.0    331.0      0.0                      output_segments,\n",
       "   542         1        269.0    269.0      0.0                      audio,\n",
       "   543         1        229.0    229.0      0.0                      finalised_idx,\n",
       "   544         1        228.0    228.0      0.0                      seek,\n",
       "   545         1        209.0    209.0      0.0                      temperature_idx,\n",
       "   546         1        285.0    285.0      0.0                      previous_tokens_list,\n",
       "   547         1        235.0    235.0      0.0                      previous_text_list,\n",
       "   548         1        318.0    318.0      0.0                      [],\n",
       "   549                                                           )\n",
       "   550                                           \n",
       "   551        35     149165.0   4261.9      0.0          tokens = result.sequences_ids[0]\n",
       "   552        35      45914.0   1311.8      0.0          tot_token_len = len(tokens)\n",
       "   553                                           \n",
       "   554        35      15678.0    447.9      0.0          current_segments = []\n",
       "   555                                           \n",
       "   556        35      33273.0    950.7      0.0          if not options.without_timestamps:\n",
       "   557        35      14941.0    426.9      0.0              single_timestamp_ending = (\n",
       "   558        35      22462.0    641.8      0.0                  tot_token_len >= 2\n",
       "   559        35     136681.0   3905.2      0.0                  and tokens[-2] < tokenizer.timestamp_begin\n",
       "   560        35      33912.0    968.9      0.0                  and tokens[-1] >= tokenizer.timestamp_begin\n",
       "   561                                                       )\n",
       "   562                                           \n",
       "   563        35     890804.0  25451.5      0.0              consecutive_timestamps = [\n",
       "   564                                                           i\n",
       "   565        35      48958.0   1398.8      0.0                  for i in range(tot_token_len)\n",
       "   566                                                           if i > 0\n",
       "   567                                                           and tokens[i] >= tokenizer.timestamp_begin\n",
       "   568                                                           and tokens[i - 1] >= tokenizer.timestamp_begin\n",
       "   569                                                       ]\n",
       "   570                                           \n",
       "   571        30      12904.0    430.1      0.0              if single_timestamp_ending:\n",
       "   572        30      27480.0    916.0      0.0                  consecutive_timestamps.append(tot_token_len)\n",
       "   573                                           \n",
       "   574        35      15922.0    454.9      0.0              last_slice = 0\n",
       "   575                                                       # Refactor\n",
       "   576                                                       # Using the timestamp rule where the first entry have to be timestamp\n",
       "   577                                                       # and timestamp have to come in pairs unless is EOT.\n",
       "   578       104      42776.0    411.3      0.0              for current_slice in consecutive_timestamps:\n",
       "   579       104      81342.0    782.1      0.0                  sliced_tokens = tokens[last_slice:current_slice]\n",
       "   580       104      92302.0    887.5      0.0                  start_timestamp_position = sliced_tokens[0] - tokenizer.timestamp_begin\n",
       "   581       104      87782.0    844.1      0.0                  end_timestamp_position = sliced_tokens[-1] - tokenizer.timestamp_begin\n",
       "   582       104      35684.0    343.1      0.0                  start_time = (\n",
       "   583       104     129661.0   1246.7      0.0                      time_offset + start_timestamp_position * self.time_precision\n",
       "   584                                                           )\n",
       "   585       104      56988.0    548.0      0.0                  end_time = time_offset + end_timestamp_position * self.time_precision\n",
       "   586                                                           #dropping segment with start time exceeding segment duration\n",
       "   587       104      54351.0    522.6      0.0                  if stream_options.drop_out_of_bound and start_time > time_offset + segment_duration:\n",
       "   588                                                               self.logger.debug(\"Segment start time %s exceeds total duration %s. Dropping segment and all after.\", start_time, time_offset + segment_duration)\n",
       "   589                                                               break\n",
       "   590                                                           else:\n",
       "   591       104      55203.0    530.8      0.0                      current_segments.append(\n",
       "   592       104     101272.0    973.8      0.0                          dict(\n",
       "   593       104      31912.0    306.8      0.0                              seek=seek,\n",
       "   594       104      31706.0    304.9      0.0                              start=start_time,\n",
       "   595       104      30747.0    295.6      0.0                              end=end_time,\n",
       "   596       104      32580.0    313.3      0.0                              tokens=sliced_tokens,\n",
       "   597                                                                   )\n",
       "   598                                                               )\n",
       "   599       104      38361.0    368.9      0.0                      last_slice = current_slice\n",
       "   600        30      13639.0    454.6      0.0              if not single_timestamp_ending:\n",
       "   601         5       3242.0    648.4      0.0                  sliced_tokens = tokens[last_slice:tot_token_len]\n",
       "   602         5       3793.0    758.6      0.0                  start_timestamp_position = sliced_tokens[0] - tokenizer.timestamp_begin\n",
       "   603         5       1529.0    305.8      0.0                  start_time = (\n",
       "   604         5       2528.0    505.6      0.0                      time_offset + start_timestamp_position * self.time_precision\n",
       "   605                                                           )\n",
       "   606         5       3563.0    712.6      0.0                  if start_time < time_offset + segment_duration or not stream_options.drop_out_of_bound:\n",
       "   607         5       2857.0    571.4      0.0                      current_segments.append(\n",
       "   608         5       3129.0    625.8      0.0                          dict(\n",
       "   609         5       1523.0    304.6      0.0                              seek=seek,\n",
       "   610         5       1466.0    293.2      0.0                              start=start_time,\n",
       "   611         5       1963.0    392.6      0.0                              end=time_offset + segment_duration,\n",
       "   612         5       1526.0    305.2      0.0                              tokens=sliced_tokens,\n",
       "   613                                                                   )\n",
       "   614                                                               )\n",
       "   615                                                   else:\n",
       "   616                                                       current_segments.append(\n",
       "   617                                                               dict(\n",
       "   618                                                                   seek=seek,\n",
       "   619                                                                   start=time_offset,\n",
       "   620                                                                   end=time_offset + segment_duration,\n",
       "   621                                                                   tokens=tokens,\n",
       "   622                                                               )\n",
       "   623                                                           )\n",
       "   624                                                       #seek_shift overwritten as finalisation method changes\n",
       "   625                                                       seek_shift = segment_size - 1  # compensating by one to prevent drifting.\n",
       "   626                                           \n",
       "   627        35      37139.0   1061.1      0.0          if options.word_timestamps:\n",
       "   628                                                       self.add_word_timestamps(\n",
       "   629                                                           current_segments,\n",
       "   630                                                           tokenizer,\n",
       "   631                                                           encoder_output,\n",
       "   632                                                           segment_size,\n",
       "   633                                                           options.prepend_punctuations,\n",
       "   634                                                           options.append_punctuations,\n",
       "   635                                                       )\n",
       "   636                                           \n",
       "   637                                                   #For word timestamp only will split via sentences\n",
       "   638        35      22755.0    650.1      0.0          if options.word_timestamps and options.without_timestamps:\n",
       "   639                                                       segment = current_segments.pop()\n",
       "   640                                                       # Form own segments based on punctuations\n",
       "   641                                                       current_segments = self.get_segments_from_wordtimestamp(segment, seek)\n",
       "   642                                           \n",
       "   643        35      16603.0    474.4      0.0          last_final_segment_end = time_offset\n",
       "   644        35      20195.0    577.0      0.0          prefix_tokens = []\n",
       "   645        35      15989.0    456.8      0.0          is_exceed_max_dur = False\n",
       "   646        35   49220546.0 1406301.3      0.0          self.logger.debug(\"token length: %s\\nlimit: %s\", tot_token_len, round(stream_options.max_tokens_per_second*segment_duration))\n",
       "   647                                                   # need another variable to keep track of segments not finalised\n",
       "   648        35      34254.0    978.7      0.0          current_segment_count = finalised_idx\n",
       "   649       109     189657.0   1740.0      0.0          for idx, segment in enumerate(current_segments):\n",
       "   650       109     110217.0   1011.2      0.0              tokens = segment[\"tokens\"]\n",
       "   651       109    3357417.0  30802.0      0.0              text = tokenizer.decode(tokens)\n",
       "   652                                           \n",
       "   653       109     547541.0   5023.3      0.0              if segment[\"start\"] == segment[\"end\"] or not text.strip():\n",
       "   654                                                           continue\n",
       "   655                                           \n",
       "   656       109      43245.0    396.7      0.0              is_end_within_bound = (\n",
       "   657       109     214931.0   1971.8      0.0                  segment[\"end\"] - time_offset < segment_duration - stream_options.finalised_segment_gap\n",
       "   658                                                           )\n",
       "   659       109      51530.0    472.8      0.0              is_exceed_max_dur = (\n",
       "   660       109     129826.0   1191.1      0.0                  segment_duration - last_final_segment_end + time_offset > stream_options.max_carry_forward_dur\n",
       "   661                                                           )\n",
       "   662       109     187179.0   1717.2      0.0              is_hit_limit = (idx == len(current_segments)-1) and (tot_token_len == round(stream_options.max_tokens_per_second*segment_duration))\n",
       "   663                                           \n",
       "   664       109      62693.0    575.2      0.0              final = is_end_within_bound and not needs_fallback and not is_hit_limit\n",
       "   665                                           \n",
       "   666                                                       # Currently if exceed max dur and need fallback will finalise all segments\n",
       "   667       105      46102.0    439.1      0.0              if needs_fallback and is_exceed_max_dur:\n",
       "   668         4       1718.0    429.5      0.0                  finalise_all = True\n",
       "   669                                           \n",
       "   670       109      52393.0    480.7      0.0              final = final or is_exceed_max_dur or finalise_all\n",
       "   671                                           \n",
       "   672                                           \n",
       "   673       102      72915.0    714.9      0.0              if segment[\"end\"] > time_offset + segment_duration:\n",
       "   674                                                           # dropping out of bound words\n",
       "   675         7       3283.0    469.0      0.0                  if options.word_timestamps and stream_options.drop_out_of_bound:\n",
       "   676                                                               segment[\"words\"] = [word for word in segment[\"words\"] if word[\"start\"] < time_offset+segment_duration]\n",
       "   677                                                               text = \"\".join([word[\"word\"] for word in segment[\"words\"]]).strip()\n",
       "   678                                                               if tokens[0] >= tokenizer.timestamp_begin:\n",
       "   679                                                                   tokens = [tokens[0]]\n",
       "   680                                                               else:\n",
       "   681                                                                   tokens = []\n",
       "   682                                                               for word in segment[\"words\"]:\n",
       "   683                                                                   tokens += word[\"tokens\"]\n",
       "   684                                                               if not text:\n",
       "   685                                                                   break\n",
       "   686                                                           #TODO: Possible to cap segment end and start to time_offset + segment_duration\n",
       "   687                                           \n",
       "   688        67      34158.0    509.8      0.0              if final:\n",
       "   689        67      34025.0    507.8      0.0                  last_final_segment_end = segment[\"end\"]\n",
       "   690        67      42311.0    631.5      0.0                  finalised_idx += 1\n",
       "   691        67     108275.0   1616.0      0.0                  if options.condition_on_previous_text and not needs_fallback:\n",
       "   692                                                               if stream_options.drop_dup_prompt and (text in previous_text_list):\n",
       "   693                                                                   self.logger.debug(\"Drop duplicate prompt!\")\n",
       "   694                                                               else:\n",
       "   695                                                                   previous_text_list.append(text)\n",
       "   696                                                                   previous_tokens_list.append(tokens)\n",
       "   697                                                                   tot_len = len(previous_tokens) + len(tokens)\n",
       "   698                                                                   self.logger.info(\"Len of previous tokens: %s\\n Len of current token: %s\", len(previous_tokens), len(tokens))\n",
       "   699                                                                   old_tokens = []\n",
       "   700                                                                   while (tot_len > stream_options.max_prompt_tokens) and previous_tokens_list:\n",
       "   701                                                                       old_text = previous_text_list.pop(0)\n",
       "   702                                                                       old_tokens = previous_tokens_list.pop(0)\n",
       "   703                                                                       tot_len -= len(old_tokens)\n",
       "   704                                                                   if not stream_options.is_prompt_sentence and old_tokens:\n",
       "   705                                                                       previous_text_list.insert(0, old_text)\n",
       "   706                                                                       previous_tokens_list.insert(0, old_tokens)\n",
       "   707                                                       else:\n",
       "   708                                                           # For using timestamp and prefix, need to adjust the timestamp, though not working as expected\n",
       "   709        42      28565.0    680.1      0.0                  if stream_options.use_prefix and not needs_fallback:\n",
       "   710                                                               if not options.without_timestamps :\n",
       "   711                                                                   tokens[0] = tokens[0] - round((last_final_segment_end-time_offset)/self.time_precision)\n",
       "   712                                                                   if tokens[-1] >= tokenizer.timestamp_begin:\n",
       "   713                                                                       tokens[-1] = tokens[-1] - round((last_final_segment_end-time_offset)/self.time_precision)\n",
       "   714                                                               prefix_tokens.extend(tokens)\n",
       "   715       109      61697.0    566.0      0.0              current_segment_count += 1\n",
       "   716                                                       #! Can return to client here instead of outputting it\n",
       "   717       109     182302.0   1672.5      0.0              output_segments.append(\n",
       "   718       109     871033.0   7991.1      0.0                  StreamSegment(\n",
       "   719       109      51354.0    471.1      0.0                      id=current_segment_count,\n",
       "   720       109      70865.0    650.1      0.0                      start=segment[\"start\"],\n",
       "   721       109      53297.0    489.0      0.0                      end=segment[\"end\"],\n",
       "   722       109      50055.0    459.2      0.0                      text=text,\n",
       "   723       109      50332.0    461.8      0.0                      temperature=temperature,\n",
       "   724       109     136346.0   1250.9      0.0                      avg_logprob=avg_logprob,\n",
       "   725       109      47604.0    436.7      0.0                      compression_ratio=compression_ratio,\n",
       "   726       109     339891.0   3118.3      0.0                      no_speech_prob=result.no_speech_prob,\n",
       "   727                                                               words=(\n",
       "   728       109      51252.0    470.2      0.0                          [Word(**word) for word in segment[\"words\"]]\n",
       "   729       109      69719.0    639.6      0.0                          if options.word_timestamps\n",
       "   730       109      50599.0    464.2      0.0                          else None\n",
       "   731                                                               ),\n",
       "   732       109      46098.0    422.9      0.0                      final=final,\n",
       "   733                                                           )\n",
       "   734                                                       )\n",
       "   735                                           \n",
       "   736                                                   #To handle the edge case where there is not text for very long\n",
       "   737        35      36921.0   1054.9      0.0          if segment_duration - last_final_segment_end - time_offset > stream_options.max_carry_forward_dur:\n",
       "   738                                                       last_final_segment_end = segment_duration + time_offset\n",
       "   739                                                   # Keeping the audio for those segment not finalised\n",
       "   740                                                   # TODO: Possible to add option keep only those words within the segment_end_thres\n",
       "   741                                                   # TODO: to reduce audio kept and lesser token generated will need a var to keep track\n",
       "   742                                                   # TODO: of the unfinalised tokens and add them to the prompt. Do this only when needs_fallback is False.\n",
       "   743                                           \n",
       "   744                                                   # TODO: If options.without_timestamp is False and single_timestamp_ending is True should ensure the last segment final=False\n",
       "   745        35      31746.0    907.0      0.0          seek_shift = round(\n",
       "   746        35      75141.0   2146.9      0.0              (last_final_segment_end - time_offset) * self.frames_per_second\n",
       "   747                                                   )\n",
       "   748                                           \n",
       "   749                                                   # Will wipe all tokens if finalise all or temperature exceeds 0.5\n",
       "   750        34      36675.0   1078.7      0.0          if temperature > 0.5 or finalise_all:\n",
       "   751         1     699433.0 699433.0      0.0              self.logger.info(\"Removing previous tokens!\")\n",
       "   752         1        919.0    919.0      0.0              previous_tokens_list = []\n",
       "   753         1        730.0    730.0      0.0              previous_text_list = []\n",
       "   754                                           \n",
       "   755        35   48896229.0 1397035.1      0.0          self.logger.debug(\"Text: %s\", \" \".join(previous_text_list))\n",
       "   756                                           \n",
       "   757        32      40416.0   1263.0      0.0          if needs_fallback:\n",
       "   758         3      12495.0   4165.0      0.0              temperature_idx = min(len(self.temperatures) - 1, temperature_idx + 1)\n",
       "   759                                                   else:\n",
       "   760        32      24274.0    758.6      0.0              temperature_idx = 0\n",
       "   761                                           \n",
       "   762                                                   # Need to discard audio that is finalized and add in need audio\n",
       "   763                                                   # Check if text occurred in the added silence\n",
       "   764        34      31309.0    920.9      0.0          if seek_shift >= segment_size:\n",
       "   765         1        749.0    749.0      0.0              seek_shift = segment_size - 1\n",
       "   766        35      29239.0    835.4      0.0          seek = seek + seek_shift\n",
       "   767        35     451501.0  12900.0      0.0          audio = audio[min(seek_shift * self.feature_extractor.hop_length, len(audio)) :]\n",
       "   768                                           \n",
       "   769        35      22433.0    640.9      0.0          return output_segments, audio, finalised_idx, seek, temperature_idx, previous_tokens_list, previous_text_list, prefix_tokens"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f model.simulate_streaming -f model.generate_segments model.simulate_streaming(\"GovTech_MFTMEP1_16000_mono_16.wav\", tokenizer, options, stream_options, min_interval=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb986b47-6482-4bd9-bb14-8cec698fe4d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50362"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.no_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3863e7e-7115-491f-9755-718f62cf43cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50363"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.timestamp_begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "669c2353-7723-4473-8dde-3042af3b80aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc368623-79f6-4fd6-b41a-8dcb8a81fa61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50360"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sot_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d1cdc5d-2a1a-4469-85b8-24fd7ac58102",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, options, stream_options = model.init_options(\n",
    "    beam_size=1,\n",
    "    best_of=3,\n",
    "    condition_on_previous_text=True, \n",
    "    no_repeat_ngram_size=3,\n",
    "    max_prompt_tokens=100,\n",
    "    drop_dup_prompt=True,\n",
    "    is_prompt_sentence=True,\n",
    "    finalised_segment_gap=3,\n",
    "    rm_seconds=0.3, \n",
    "    word_timestamps=True, \n",
    "    without_timestamps=True, \n",
    "    # log_prob_threshold=-2, \n",
    "    max_tokens_per_second=5,\n",
    "    use_prefix=False,\n",
    "    prefix_drop_num_tokens=4,\n",
    "    drop_prefix_duration=0,\n",
    "    initial_prompt=\"This is the start of the conversation.\"\n",
    "    )\n",
    "# model.max_length=448\n",
    "# model.warm_start(tokenizer, options, stream_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbb3a6d9-a252-4271-a3d3-3c3626773c02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time offset: 0.0\n",
      "segment duration: 7.0\n",
      "time offset: 0.0\n",
      "segment duration: 7.0\n",
      "Not Finalised: log_prob,Fallback,outside_bound, 0.0 4.68  Music\n",
      "Processing time: 4.228480525998748\n",
      "time offset: 0.0\n",
      "segment duration: 14.0\n",
      "time offset: 0.0\n",
      "segment duration: 14.0\n",
      " 6.380000000000002 9.32  Welcome to My Friend Tell Me One.\n",
      "Not Finalised: outside_bound, 9.66 13.66  Today we're going to speak to the GoBros, two engineers who work in DevOps.\n",
      "Processing time: 22.79311388300266\n",
      "time offset: 9.32\n",
      "segment duration: 11.0\n",
      "time offset: 9.32\n",
      "segment duration: 11.0\n",
      " 9.32 13.68  Today we're going to speak to the Go Bros, two engineers who work in DevOps.\n",
      "Processing time: 5.973286278007436\n",
      "time offset: 13.68\n",
      "segment duration: 14.0\n",
      "time offset: 13.68\n",
      "segment duration: 14.0\n",
      " 14.52 15.6  Hi, my name is Ryan.\n",
      " 16.08 17.28  I work on My Current Future.\n",
      " 18.1 19.44  So basically I'm doing DevOps.\n",
      "Processing time: 8.856779660010943\n",
      "time offset: 19.44\n",
      "segment duration: 15.0\n",
      "time offset: 19.44\n",
      "segment duration: 15.0\n",
      " 28.58 30.48  Textbook answer is DevOps is a culture.\n",
      "Not Finalised: outside_bound, 31.7 33.52  So in other words, the whole team knows how to do.\n",
      "Not Finalised: outside_bound, 33.52 33.8  The whole team know how to.\n",
      "Not Finalised: outside_bound, 33.8 34.2 The entire team knows what\n",
      "Processing time: 12.50293536800018\n",
      "time offset: 30.48\n",
      "segment duration: 11.0\n",
      "time offset: 30.48\n",
      "segment duration: 11.0\n",
      "Not Finalised: outside_bound, 31.22 41.24 So in other words, the whole team knows how to do operations, some parts of operations as well as development, giving the responsibility of what operators normally do\n",
      "Processing time: 8.503119070999674\n",
      "time offset: 30.48\n",
      "segment duration: 18.0\n",
      "time offset: 30.48\n",
      "segment duration: 18.0\n",
      " 31.24 42.04  So in other words, the whole team knows how to do operations, some parts of operations as well as development, giving the responsibility of what operators normally do to developers.\n",
      " 42.6 43.44  It's kind of a wide term.\n",
      "Not Finalised: outside_bound, 43.7 48.34 So depending on where you go, different organizations, different teams have different definitions,\n",
      "Processing time: 13.356870122996042\n",
      "time offset: 43.44\n",
      "segment duration: 12.0\n",
      "time offset: 43.44\n",
      "segment duration: 12.0\n",
      " 43.44 48.9  So depending on where you go, different organizations, different teams have different definition of it.\n",
      " 49.94 51.42  So non-textbook is Saikang.\n",
      "Not Finalised: outside_bound, 54.74 55.22 It's\n",
      "Processing time: 10.993570791993989\n",
      "time offset: 51.42\n",
      "segment duration: 11.0\n",
      "time offset: 51.42\n",
      "segment duration: 11.0\n",
      " 54.879999999999995 55.58  It's not a new thing.\n",
      " 55.76 58.68  Actually it's been around since 1993, since the Agile Manifesto came about.\n",
      "Processing time: 7.081985213008011\n",
      "time offset: 58.68\n",
      "segment duration: 11.0\n",
      "time offset: 58.68\n",
      "segment duration: 11.0\n",
      " 62.62 63.6  Did they learn it in school?\n",
      " 63.98 65.2  School cannot teach, huh?\n",
      "Not Finalised: outside_bound, 65.64 66.88  But it costs two units.\n",
      "Not Finalised: outside_bound, 67.4 69.4 It's okay, we have to run our\n",
      "Processing time: 9.12520114200015\n",
      "time offset: 65.2\n",
      "segment duration: 11.0\n",
      "time offset: 65.2\n",
      "segment duration: 11.0\n",
      " 65.2 67.46  But it costs 2 units already.\n",
      "Processing time: 5.956513390003238\n",
      "time offset: 67.46000000000001\n",
      "segment duration: 16.0\n",
      "time offset: 67.46000000000001\n",
      "segment duration: 16.0\n",
      " 67.46 69.72  Basically, we have to run our own service.\n",
      " 70.14 73.96  So by running it, you probably have to do a lot of things to get it running.\n",
      " 74.58 76.14  So that's when we start learning.\n",
      " 79.44 79.98  No one.\n",
      "Not Finalised: outside_bound, 80.16 80.64  No, I don't.\n",
      "Processing time: 14.157150156999705\n",
      "time offset: 79.98\n",
      "segment duration: 11.0\n",
      "time offset: 79.98\n",
      "segment duration: 11.0\n",
      " 79.98 80.46  No one.\n",
      " 80.56 85.5  I don't like that.\n",
      "Not Finalised: outside_bound, 87.16 88.1  I think it's a secret.\n",
      "Processing time: 8.238222473999485\n",
      "time offset: 85.5\n",
      "segment duration: 12.0\n",
      "time offset: 85.5\n",
      "segment duration: 12.0\n",
      " 85.5 86.22  I don't like that.\n",
      " 86.68 88.12  I think it's a secret.\n",
      " 89.6 92.26  I say, 2.9.\n",
      "Processing time: 9.371117274000426\n",
      "time offset: 92.26\n",
      "segment duration: 12.0\n",
      "time offset: 92.26\n",
      "segment duration: 12.0\n",
      " 94.52 96.54  This is the basement wall.\n",
      " 99.12 100.94  This is a basement wall!\n",
      "Processing time: 7.342603137003607\n",
      "time offset: 100.94\n",
      "segment duration: 11.0\n",
      "time offset: 100.94\n",
      "segment duration: 11.0\n",
      "Not Finalised: outside_bound, 104.61999999999999 110.58  Maybe you can start growing your own vegetables and cooking them so that you manage the whole thing end to end.\n",
      "Not Finalised: outside_bound, 110.78 110.9  Maybe?\n",
      "Not Finalised: outside_bound, 111.42 111.78 Do everything\n",
      "Processing time: 8.244485849005287\n",
      "time offset: 100.94\n",
      "segment duration: 18.0\n",
      "time offset: 100.94\n",
      "segment duration: 18.0\n",
      " 104.62 108.12  Maybe you can start growing your own vegetables and cooking them.\n",
      " 108.62 110.56  So that you manage the whole thing end to end.\n",
      " 110.82 110.9  Maybe?\n",
      " 111.4 112.16  Do everything yourself.\n",
      " 112.82 113.9  So previously, right?\n",
      " 114.36 115.22  Assuming you've got 2 people.\n",
      "Not Finalised: outside_bound, 115.54 116.72  So one person just cooked, right.\n",
      "Not Finalised: outside_bound, 116.94 117.92  And passed everything to the front.\n",
      "Not Finalised: outside_bound, 118.04 118.4  He don't care about the other one.\n",
      "Not Finalised: outside_bound, 118.4 118.78 He doesn't care\n",
      "Processing time: 15.243198889002088\n",
      "time offset: 115.22\n",
      "segment duration: 10.0\n",
      "time offset: 115.22\n",
      "segment duration: 10.0\n",
      " 115.22 117.92  So one person just cook and pass everything to the front.\n",
      " 118.06 119.3  He don't care what now.\n",
      " 119.66 121.84  So let's say you cook the egg, one giant piece.\n",
      "Not Finalised: outside_bound, 122.04 122.76  Then you just give to the person.\n",
      "Not Finalised: outside_bound, 122.82 124.5  Then the front person is like, you have to cut\n",
      "Processing time: 8.460717085996293\n",
      "time offset: 121.84\n",
      "segment duration: 11.0\n",
      "time offset: 121.84\n",
      "segment duration: 11.0\n",
      " 121.84 122.74  Then you just give to the person.\n",
      " 122.88 124.92  Then the front person is like, you have to cut up the piece.\n",
      " 125.46 126.38  So it's like double work.\n",
      " 127.0 129.2  So now I cook already, I cut up for you, you go to the front.\n",
      "Not Finalised: hit_limit,outside_bound, 129.3 130.08  Then you catch them more efficiently.\n",
      "Processing time: 9.746847570000682\n",
      "time offset: 129.2\n",
      "segment duration: 10.0\n",
      "time offset: 129.2\n",
      "segment duration: 10.0\n",
      " 129.2 131.64  Then the cashier more efficient because he just take one slice, put inside.\n",
      "Not Finalised: outside_bound, 136.34 139.1 So Micro Suture is a platform created to reduce\n",
      "Processing time: 11.348627006009337\n",
      "time offset: 131.64000000000001\n",
      "segment duration: 15.0\n",
      "time offset: 131.64000000000001\n",
      "segment duration: 15.0\n",
      "Not Finalised: outside_bound, 136.26000000000002 145.3  So Microus Future is a platform created to reduce unemployment in local PMACs, especially those in their 30s or 40s who find themselves unemployed and they are unable to rejoin the workforce.\n",
      "Not Finalised: outside_bound, 146.12 146.44 So we,\n",
      "Processing time: 13.180488773999969\n",
      "time offset: 131.64000000000001\n",
      "segment duration: 22.0\n",
      "time offset: 131.64000000000001\n",
      "segment duration: 22.0\n",
      " 136.34 140.56  So Microus Future is a platform created to reduce unemployment in local PMATs.\n",
      " 140.6 145.28  Especially those in their 30s or 40s who find themselves unemployed and they are unable to rejoin the workforce.\n",
      " 146.08 149.16  So we do this through different ways.\n",
      "Not Finalised: outside_bound, 149.34 152.04  So instead of matching people to jobs, we match skills to jobs for example.\n",
      "Processing time: 20.057176415997674\n",
      "time offset: 149.16\n",
      "segment duration: 11.0\n",
      "time offset: 149.16\n",
      "segment duration: 11.0\n",
      " 149.16 152.22  So instead of matching people to jobs, you match skills to jobs for example.\n",
      "Processing time: 7.1384656190057285\n",
      "time offset: 152.22\n",
      "segment duration: 15.0\n",
      "time offset: 152.22\n",
      "segment duration: 15.0\n",
      " 155.22 157.94  Yeah, FE best.\n",
      "Not Finalised: outside_bound, 160.42 164.68  How your answer?\n",
      "Processing time: 9.478604991003522\n",
      "time offset: 157.94\n",
      "segment duration: 17.0\n",
      "time offset: 157.94\n",
      "segment duration: 17.0\n",
      " 157.94 164.64  How do you answer?\n",
      "Processing time: 10.509055968999746\n",
      "time offset: 164.64000000000001\n",
      "segment duration: 17.0\n",
      "time offset: 164.64000000000001\n",
      "segment duration: 17.0\n",
      " 171.01999999999998 175.66  Hi, my name is Ryan.\n",
      "Not Finalised: outside_bound, 176.04 180.82  I work in DevOps because I think it's cool and I have a lot of control over the things that I work at.\n",
      "Processing time: 12.03164425099385\n",
      "time offset: 175.66\n",
      "segment duration: 13.0\n",
      "time offset: 175.66\n",
      "segment duration: 13.0\n",
      " 175.66 180.82  I work in DevOps because I think it's cool and I have a lot of control over the things that I work at.\n",
      " 181.36 183.68  I can play a Chinese instrument, the Erhu.\n",
      "Not Finalised: outside_bound, 184.46 188.38 So I'm Joseph and I like DevOps, because it's a lot more challenging than development\n",
      "Processing time: 11.603122572007123\n",
      "time offset: 183.68\n",
      "segment duration: 12.0\n",
      "time offset: 183.68\n",
      "segment duration: 12.0\n",
      " 184.26000000000002 188.66  So I'm Joseph and I like DevOps because it's a lot more challenging than development work.\n",
      "Not Finalised: outside_bound, 189.02 192.68  So one interesting thing for me is that I practice martial arts, Wing Chun to be specific.\n",
      "Processing time: 9.483562708002864\n",
      "time offset: 188.66\n",
      "segment duration: 10.0\n",
      "time offset: 188.66\n",
      "segment duration: 10.0\n",
      " 188.66 192.68  So one interesting thing for me is that I practice martial arts, Wing Chun to be specific.\n",
      "Processing time: 7.538193612999748\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-09 s\n",
       "\n",
       "Total time: 302.649 s\n",
       "File: /home/chiiyeh/dev/stream.py\n",
       "Function: simulate_fixed_interval_streaming at line 429\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   429                                               def simulate_fixed_interval_streaming(\n",
       "   430                                                   self,\n",
       "   431                                                   audio_file: str,\n",
       "   432                                                   tokenizer: Tokenizer,\n",
       "   433                                                   options: TranscriptionOptions,\n",
       "   434                                                   stream_options: StreamOptions,\n",
       "   435                                                   interval: float = 3,\n",
       "   436                                               ):\n",
       "   437         1       1866.0   1866.0      0.0          sampling_rate = self.feature_extractor.sampling_rate\n",
       "   438         1  101618187.0 101618187.0      0.0          full_audio = decode_audio(audio_file, sampling_rate=sampling_rate)\n",
       "   439         1       1363.0   1363.0      0.0          audio_read = 0\n",
       "   440                                           \n",
       "   441         1        523.0    523.0      0.0          idx = 0\n",
       "   442         1       1601.0   1601.0      0.0          seek = 0\n",
       "   443         1       1125.0   1125.0      0.0          previous_tokens_list = []\n",
       "   444         1        401.0    401.0      0.0          previous_text_list = []\n",
       "   445         1        591.0    591.0      0.0          prefix_tokens = []\n",
       "   446         1       1328.0   1328.0      0.0          initial_prompt_tokens = []\n",
       "   447         1        493.0    493.0      0.0          current_audio = None\n",
       "   448         1        940.0    940.0      0.0          temperature_idx = 0\n",
       "   449                                           \n",
       "   450         1       4590.0   4590.0      0.0          if options.initial_prompt is not None and options.initial_prompt.strip():\n",
       "   451         1       1128.0   1128.0      0.0              initial_prompt = \" \" + options.initial_prompt.strip()\n",
       "   452         1    3302003.0 3302003.0      0.0              initial_prompt_tokens = tokenizer.encode(initial_prompt)\n",
       "   453         1       1373.0   1373.0      0.0              if not stream_options.propagate_initial_prompt:\n",
       "   454         1       1700.0   1700.0      0.0                  previous_tokens_list.append(initial_prompt_tokens)\n",
       "   455                                           \n",
       "   456        29      33062.0   1140.1      0.0          while audio_read < len(full_audio):\n",
       "   457        29      24604.0    848.4      0.0              current_time = time.perf_counter()\n",
       "   458        29      69790.0   2406.6      0.0              samples_read = round(interval * sampling_rate)\n",
       "   459        29      87339.0   3011.7      0.0              new_audio = full_audio[audio_read:min(audio_read+samples_read, len(full_audio))]\n",
       "   460        29      17032.0    587.3      0.0              audio_read += samples_read\n",
       "   461                                           \n",
       "   462                                           \n",
       "   463        28      15009.0    536.0      0.0              if current_audio is not None:\n",
       "   464        28   10411160.0 371827.1      0.0                  current_audio = np.concatenate([current_audio, new_audio])\n",
       "   465                                                       else:\n",
       "   466         1        206.0    206.0      0.0                  current_audio = new_audio\n",
       "   467                                           \n",
       "   468        29      18159.0    626.2      0.0              (\n",
       "   469        29     178822.0   6166.3      0.0                  output_segments,\n",
       "   470        29       8142.0    280.8      0.0                  current_audio,\n",
       "   471        29       8443.0    291.1      0.0                  idx,\n",
       "   472        29       8860.0    305.5      0.0                  seek,\n",
       "   473        29      11817.0    407.5      0.0                  temperature_idx,\n",
       "   474        29       7284.0    251.2      0.0                  previous_tokens_list,\n",
       "   475        29       7361.0    253.8      0.0                  previous_text_list,\n",
       "   476        29      12622.0    435.2      0.0                  prefix_tokens,\n",
       "   477        29 302528405921.0 10432013997.3    100.0              ) = self.generate_segments(\n",
       "   478        29      11875.0    409.5      0.0                  current_audio,\n",
       "   479        29      20239.0    697.9      0.0                  tokenizer,\n",
       "   480        29      15338.0    528.9      0.0                  options,\n",
       "   481        29      15945.0    549.8      0.0                  stream_options,\n",
       "   482        29      16973.0    585.3      0.0                  idx,\n",
       "   483        29      13831.0    476.9      0.0                  seek,\n",
       "   484        29      13901.0    479.3      0.0                  temperature_idx,\n",
       "   485        29      16263.0    560.8      0.0                  previous_tokens_list,\n",
       "   486        29      20229.0    697.6      0.0                  previous_text_list,\n",
       "   487        29      19870.0    685.2      0.0                  prefix_tokens,\n",
       "   488        29       9603.0    331.1      0.0                  initial_prompt_tokens\n",
       "   489                                                       )\n",
       "   490        81     131607.0   1624.8      0.0              for seg in output_segments:\n",
       "   491        51      46148.0    904.9      0.0                  if seg.final:\n",
       "   492        51    3544458.0  69499.2      0.0                      print(f\"{seg.reason} {seg.start} {seg.end} {seg.text}\")\n",
       "   493                                                           else:\n",
       "   494        30    1003085.0  33436.2      0.0                      print(f\"Not Finalised: {seg.reason} {seg.start} {seg.end} {seg.text}\")\n",
       "   495        29     332072.0  11450.8      0.0              print(f\"Processing time: {time.perf_counter() - current_time}\")\n",
       "\n",
       "Total time: 302.523 s\n",
       "File: /home/chiiyeh/dev/stream.py\n",
       "Function: generate_segments at line 507\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "   507                                               def generate_segments(\n",
       "   508                                                   self,\n",
       "   509                                                   audio: np.ndarray,\n",
       "   510                                                   tokenizer: Tokenizer,\n",
       "   511                                                   options: TranscriptionOptions,\n",
       "   512                                                   stream_options: StreamOptions,\n",
       "   513                                                   finalised_idx: int,\n",
       "   514                                                   seek: int,\n",
       "   515                                                   temperature_idx: int,\n",
       "   516                                                   previous_tokens_list: List[List[int]],\n",
       "   517                                                   previous_text_list: List[List[str]],\n",
       "   518                                                   prefix_tokens: List[int],\n",
       "   519                                                   initial_prompt_tokens: List[int],\n",
       "   520                                               ):\n",
       "   521        29      25134.0    866.7      0.0          seek_shift = 0\n",
       "   522        29      14156.0    488.1      0.0          output_segments = []\n",
       "   523        29      35449.0   1222.4      0.0          max_initial_timestamp = options.max_initial_timestamp\n",
       "   524        29       9037.0    311.6      0.0          finalise_all = False\n",
       "   525                                                   # chunk_length = 30\n",
       "   526        29     178464.0   6153.9      0.0          chunk_length = min(30, int(len(audio)/self.feature_extractor.sampling_rate))\n",
       "   527        29   32688588.0 1127192.7      0.0          self.feature_extractor = FeatureExtractor(chunk_length=chunk_length)\n",
       "   528        29 5376765548.0 185405708.6      1.8          features = self.feature_extractor(audio)\n",
       "   529                                           \n",
       "   530                                                   #During the extraction of feat 30s of silence was appended at the\n",
       "   531                                                   #end, so to get content frames need to compensate this amount\n",
       "   532        29      14177.0    488.9      0.0          relative_content_frames = (\n",
       "   533        29     250924.0   8652.6      0.0              features.shape[-1] - self.feature_extractor.nb_max_frames\n",
       "   534                                                   )\n",
       "   535        29     117812.0   4062.5      0.0          time_offset = seek * self.feature_extractor.time_per_frame\n",
       "   536        29     171564.0   5916.0      0.0          segment = features[:, : self.feature_extractor.nb_max_frames]\n",
       "   537                                                   # to handle the case where the audio exceeds 30s\n",
       "   538        29     101490.0   3499.7      0.0          segment_size = min(\n",
       "   539        29      10124.0    349.1      0.0              self.feature_extractor.nb_max_frames, relative_content_frames\n",
       "   540                                                   )\n",
       "   541        29      30565.0   1054.0      0.0          segment_duration = segment_size * self.feature_extractor.time_per_frame\n",
       "   542        29  131270604.0 4526572.6      0.0          self.logger.info(\"time offset: %s\\nsegment duration: %s\", time_offset, segment_duration)\n",
       "   543                                           \n",
       "   544        29     153883.0   5306.3      0.0          if self.logger.isEnabledFor(logging.DEBUG):\n",
       "   545                                                       self.logger.debug(\"Processing segment at %s\", format_timestamp(time_offset))\n",
       "   546                                           \n",
       "   547        29     399184.0  13765.0      0.0          previous_tokens = list(itertools.chain.from_iterable(previous_tokens_list))\n",
       "   548        29     477892.0  16479.0      0.0          prompt = self.get_prompt(\n",
       "   549        29      17670.0    609.3      0.0              tokenizer,\n",
       "   550        29       8356.0    288.1      0.0              previous_tokens,\n",
       "   551        29      22350.0    770.7      0.0              stream_options,\n",
       "   552        29      13766.0    474.7      0.0              initial_prompt_tokens,\n",
       "   553        29      40107.0   1383.0      0.0              without_timestamps=options.without_timestamps,\n",
       "   554        29      13793.0    475.6      0.0              prefix=options.prefix if seek == 0 else None,\n",
       "   555                                                   )\n",
       "   556        29      52101.0   1796.6      0.0          self.max_length = max(\n",
       "   557        29     107291.0   3699.7      0.0              round(stream_options.max_tokens_per_second*segment_duration)*2,\n",
       "   558        29      32944.0   1136.0      0.0              len(prompt) + round(stream_options.max_tokens_per_second*segment_duration)\n",
       "   559                                                   )\n",
       "   560                                           \n",
       "   561        29      21838.0    753.0      0.0          if stream_options.use_prefix and len(prefix_tokens) > stream_options.prefix_drop_num_tokens:\n",
       "   562                                                       prompt.extend(prefix_tokens[:-stream_options.prefix_drop_num_tokens])\n",
       "   563                                                       # self.logger.debug(\n",
       "   564                                                       #     \"Prefix Tokens Added: %s %s\", \n",
       "   565                                                       #     tokenizer.decode(prefix_tokens[:-stream_options.prefix_drop_num_tokens]), \n",
       "   566                                                       #     prefix_tokens[:-stream_options.prefix_drop_num_tokens]\n",
       "   567                                                       #     )\n",
       "   568                                                       # max_initial_timestamp = 25\n",
       "   569                                           \n",
       "   570        29 134955363930.0 4653633239.0     44.6          encoder_output = self.encode(segment)\n",
       "   571                                           \n",
       "   572        29      15266.0    526.4      0.0          (\n",
       "   573        29       8520.0    293.8      0.0              result,\n",
       "   574        29       5630.0    194.1      0.0              avg_logprob,\n",
       "   575        29       9419.0    324.8      0.0              temperature,\n",
       "   576        29      12638.0    435.8      0.0              compression_ratio,\n",
       "   577        29       7330.0    252.8      0.0              needs_fallback,\n",
       "   578        29       7179.0    247.6      0.0              fallback_reason,\n",
       "   579        29 128336835685.0 4425408127.1     42.4          ) = self.generate_with_fallback(encoder_output, prompt, tokenizer, options, [self.temperatures[temperature_idx]], max_initial_timestamp)\n",
       "   580        29     266121.0   9176.6      0.0          self.logger.debug(\"Compression Ratio: %s\\nTemperature: %s\", compression_ratio, temperature)\n",
       "   581                                           \n",
       "   582        29      17298.0    596.5      0.0          if options.no_speech_threshold is not None:\n",
       "   583                                                       # no voice activity check\n",
       "   584        29      26922.0    928.3      0.0              should_skip = result.no_speech_prob > options.no_speech_threshold\n",
       "   585                                           \n",
       "   586        29       7176.0    247.4      0.0              if (\n",
       "   587        29      12867.0    443.7      0.0                  options.log_prob_threshold is not None\n",
       "   588        29       8875.0    306.0      0.0                  and avg_logprob > options.log_prob_threshold\n",
       "   589                                                       ):\n",
       "   590                                                           # don't skip if the logprob is high enough, despite the no_speech_prob\n",
       "   591        28      11410.0    407.5      0.0                  should_skip = False\n",
       "   592                                           \n",
       "   593        29      13653.0    470.8      0.0              if should_skip:\n",
       "   594                                                           self.logger.debug(\n",
       "   595                                                               \"No speech threshold is met (%f > %f)\",\n",
       "   596                                                               result.no_speech_prob,\n",
       "   597                                                               options.no_speech_threshold,\n",
       "   598                                                           )\n",
       "   599                                           \n",
       "   600                                                           # fast-forward to the next segment boundary\n",
       "   601                                                           seek_shift = segment_size - 1  # compensating by one to prevent drifting.\n",
       "   602                                                           seek = seek + seek_shift\n",
       "   603                                                           audio = audio[\n",
       "   604                                                               min(seek_shift * self.feature_extractor.hop_length, len(audio)) :\n",
       "   605                                                           ]\n",
       "   606                                                           \n",
       "   607                                                           return (\n",
       "   608                                                               output_segments,\n",
       "   609                                                               audio,\n",
       "   610                                                               finalised_idx,\n",
       "   611                                                               seek,\n",
       "   612                                                               temperature_idx,\n",
       "   613                                                               previous_tokens_list,\n",
       "   614                                                               previous_text_list,\n",
       "   615                                                               [],\n",
       "   616                                                           )\n",
       "   617                                           \n",
       "   618        29      82775.0   2854.3      0.0          tokens = result.sequences_ids[0]\n",
       "   619        29      23933.0    825.3      0.0          tot_token_len = len(tokens)\n",
       "   620                                           \n",
       "   621        29       7744.0    267.0      0.0          current_segments = []\n",
       "   622                                           \n",
       "   623        29      25781.0    889.0      0.0          if not options.without_timestamps:\n",
       "   624                                                       single_timestamp_ending = (\n",
       "   625                                                           tot_token_len >= 2\n",
       "   626                                                           and tokens[-2] < tokenizer.timestamp_begin\n",
       "   627                                                           and tokens[-1] >= tokenizer.timestamp_begin\n",
       "   628                                                       )\n",
       "   629                                           \n",
       "   630                                                       consecutive_timestamps = [\n",
       "   631                                                           i\n",
       "   632                                                           for i in range(tot_token_len)\n",
       "   633                                                           if i > 0\n",
       "   634                                                           and tokens[i] >= tokenizer.timestamp_begin\n",
       "   635                                                           and tokens[i - 1] >= tokenizer.timestamp_begin\n",
       "   636                                                       ]\n",
       "   637                                           \n",
       "   638                                                       if single_timestamp_ending:\n",
       "   639                                                           consecutive_timestamps.append(tot_token_len)\n",
       "   640                                           \n",
       "   641                                                       last_slice = 0\n",
       "   642                                                       # Refactor\n",
       "   643                                                       # Using the timestamp rule where the first entry have to be timestamp\n",
       "   644                                                       # and timestamp have to come in pairs unless is EOT.\n",
       "   645                                                       for current_slice in consecutive_timestamps:\n",
       "   646                                                           sliced_tokens = tokens[last_slice:current_slice]\n",
       "   647                                                           start_timestamp_position = sliced_tokens[0] - tokenizer.timestamp_begin\n",
       "   648                                                           end_timestamp_position = sliced_tokens[-1] - tokenizer.timestamp_begin\n",
       "   649                                                           start_time = (\n",
       "   650                                                               time_offset + start_timestamp_position * self.time_precision\n",
       "   651                                                           )\n",
       "   652                                                           end_time = time_offset + end_timestamp_position * self.time_precision\n",
       "   653                                                           #dropping segment with start time exceeding segment duration\n",
       "   654                                                           if stream_options.drop_out_of_bound and start_time >= time_offset + segment_duration - stream_options.rm_seconds:\n",
       "   655                                                               self.logger.debug(\"Segment start time %s exceeds total duration %s. Dropping segment and all after.\", start_time, time_offset + segment_duration)\n",
       "   656                                                               break\n",
       "   657                                                           else:\n",
       "   658                                                               current_segments.append(\n",
       "   659                                                                   dict(\n",
       "   660                                                                       seek=seek,\n",
       "   661                                                                       start=start_time,\n",
       "   662                                                                       end=end_time,\n",
       "   663                                                                       tokens=sliced_tokens,\n",
       "   664                                                                   )\n",
       "   665                                                               )\n",
       "   666                                                               last_slice = current_slice\n",
       "   667                                                       if not single_timestamp_ending:\n",
       "   668                                                           sliced_tokens = tokens[last_slice:tot_token_len]\n",
       "   669                                                           start_timestamp_position = sliced_tokens[0] - tokenizer.timestamp_begin\n",
       "   670                                                           start_time = (\n",
       "   671                                                               time_offset + start_timestamp_position * self.time_precision\n",
       "   672                                                           )\n",
       "   673                                                           if start_time < time_offset + segment_duration - stream_options.rm_seconds or not stream_options.drop_out_of_bound:\n",
       "   674                                                               current_segments.append(\n",
       "   675                                                                   dict(\n",
       "   676                                                                       seek=seek,\n",
       "   677                                                                       start=start_time,\n",
       "   678                                                                       end=time_offset + segment_duration,\n",
       "   679                                                                       tokens=sliced_tokens,\n",
       "   680                                                                   )\n",
       "   681                                                               )\n",
       "   682                                                   else:\n",
       "   683        29      22488.0    775.4      0.0              current_segments.append(\n",
       "   684        29      69181.0   2385.6      0.0                      dict(\n",
       "   685        29      14352.0    494.9      0.0                          seek=seek,\n",
       "   686        29      20482.0    706.3      0.0                          start=time_offset,\n",
       "   687        29      32213.0   1110.8      0.0                          end=time_offset + segment_duration,\n",
       "   688        29       7447.0    256.8      0.0                          tokens=tokens,\n",
       "   689                                                               )\n",
       "   690                                                           )\n",
       "   691                                                       #seek_shift overwritten as finalisation method changes\n",
       "   692        29      26459.0    912.4      0.0              seek_shift = segment_size - 1  # compensating by one to prevent drifting.\n",
       "   693                                           \n",
       "   694        29      41782.0   1440.8      0.0          if options.word_timestamps:\n",
       "   695        29 33675022402.0 1161207669.0     11.1              self.add_word_timestamps(\n",
       "   696        29      13647.0    470.6      0.0                  current_segments,\n",
       "   697        29      17017.0    586.8      0.0                  tokenizer,\n",
       "   698        29      10248.0    353.4      0.0                  encoder_output,\n",
       "   699        29       7954.0    274.3      0.0                  segment_size,\n",
       "   700        29      19879.0    685.5      0.0                  options.prepend_punctuations,\n",
       "   701        29      24633.0    849.4      0.0                  options.append_punctuations,\n",
       "   702                                                       )\n",
       "   703        29      95863.0   3305.6      0.0              if stream_options.drop_out_of_bound:\n",
       "   704        29      63057.0   2174.4      0.0                  for idx, segment in enumerate(current_segments):\n",
       "   705        29     329146.0  11349.9      0.0                      segment[\"words\"] = [word for word in segment[\"words\"] if word[\"start\"] < time_offset+segment_duration - stream_options.rm_seconds]\n",
       "   706        29      18458.0    636.5      0.0                      if not segment[\"words\"]:\n",
       "   707                                                                   idx = idx - 1\n",
       "   708                                                                   break\n",
       "   709        29     111484.0   3844.3      0.0                      if segment[\"tokens\"][0] >= tokenizer.timestamp_begin:\n",
       "   710                                                                   tokens = [tokens[0]]\n",
       "   711                                                               else:\n",
       "   712        29      17015.0    586.7      0.0                          tokens = []\n",
       "   713       726     260814.0    359.2      0.0                      for word in segment[\"words\"]:\n",
       "   714       726     335260.0    461.8      0.0                          tokens += word[\"tokens\"]\n",
       "   715        29      24246.0    836.1      0.0                      segment[\"tokens\"] = tokens\n",
       "   716        29      36507.0   1258.9      0.0                  current_segments = current_segments[:idx+1]\n",
       "   717        29    2572806.0  88717.4      0.0                  compression_ratio = get_compression_ratio(\" \".join([tokenizer.decode(segment[\"tokens\"]) for segment in current_segments]))\n",
       "   718                                           \n",
       "   719        29      30740.0   1060.0      0.0          if options.compression_ratio_threshold is not None:\n",
       "   720        29      27023.0    931.8      0.0              if compression_ratio > options.compression_ratio_threshold:\n",
       "   721                                                           needs_fallback = True  # too repetitive\n",
       "   722                                                           fallback_reason +=\"compression,\"\n",
       "   723                                           \n",
       "   724                                                   #For word timestamp only will split via sentences\n",
       "   725        29      38101.0   1313.8      0.0          if options.word_timestamps and options.without_timestamps and current_segments:\n",
       "   726        29      46087.0   1589.2      0.0              segment = current_segments.pop()\n",
       "   727                                                       # Form own segments based on punctuations\n",
       "   728        29    1222204.0  42145.0      0.0              current_segments = self.get_segments_from_wordtimestamp(segment, seek, not_sentence_end=stream_options.not_sentence_end)\n",
       "   729                                           \n",
       "   730        29      15972.0    550.8      0.0          last_final_segment_end = time_offset\n",
       "   731        29      15749.0    543.1      0.0          prefix_tokens = []\n",
       "   732        29      12318.0    424.8      0.0          is_exceed_max_dur = False\n",
       "   733        29     312152.0  10763.9      0.0          self.logger.debug(\"token length: %s\\nlimit: %s\", tot_token_len, round(stream_options.max_tokens_per_second*segment_duration))\n",
       "   734                                                   # need another variable to keep track of segments not finalised\n",
       "   735        29      14545.0    501.6      0.0          current_segment_count = finalised_idx\n",
       "   736        82     137620.0   1678.3      0.0          for idx, segment in enumerate(current_segments):\n",
       "   737        82      32860.0    400.7      0.0              reason = fallback_reason\n",
       "   738        82      45233.0    551.6      0.0              tokens = segment[\"tokens\"]\n",
       "   739        82    1020447.0  12444.5      0.0              text = tokenizer.decode(tokens)\n",
       "   740                                           \n",
       "   741        81     171814.0   2121.2      0.0              if segment[\"start\"] == segment[\"end\"] or not text.strip():\n",
       "   742                                                           continue\n",
       "   743                                           \n",
       "   744        81      33177.0    409.6      0.0              is_end_within_bound = (\n",
       "   745        81     112073.0   1383.6      0.0                  segment[\"end\"] - time_offset < segment_duration - stream_options.finalised_segment_gap\n",
       "   746                                                           )\n",
       "   747        81      33972.0    419.4      0.0              is_exceed_max_dur = (\n",
       "   748        81     108506.0   1339.6      0.0                  segment_duration - last_final_segment_end + time_offset > stream_options.max_carry_forward_dur\n",
       "   749                                                           )\n",
       "   750        81      89088.0   1099.9      0.0              is_hit_limit = (idx == len(current_segments)-1) and (tot_token_len == round(stream_options.max_tokens_per_second*segment_duration))\n",
       "   751                                           \n",
       "   752        81      53256.0    657.5      0.0              final = is_end_within_bound and not needs_fallback and not is_hit_limit\n",
       "   753        80      36935.0    461.7      0.0              if needs_fallback:\n",
       "   754         1      31452.0  31452.0      0.0                  reason += \"Fallback,\"\n",
       "   755        81      81198.0   1002.4      0.0              if is_exceed_max_dur:\n",
       "   756                                                           reason += \"exceed_dur,\"\n",
       "   757        80      32655.0    408.2      0.0              if is_hit_limit:\n",
       "   758         1        616.0    616.0      0.0                  reason += \"hit_limit,\"\n",
       "   759        51      25391.0    497.9      0.0              if not is_end_within_bound:\n",
       "   760        30      21299.0    710.0      0.0                  reason += \"outside_bound,\"\n",
       "   761                                           \n",
       "   762                                                       # Currently if exceed max dur and need fallback will finalise all segments\n",
       "   763        81      36696.0    453.0      0.0              if needs_fallback and is_exceed_max_dur:\n",
       "   764                                                           finalise_all = True\n",
       "   765                                           \n",
       "   766        81      45087.0    556.6      0.0              final = final or is_exceed_max_dur or finalise_all\n",
       "   767                                           \n",
       "   768                                           \n",
       "   769        71      72573.0   1022.2      0.0              if segment[\"end\"] >= time_offset + segment_duration - stream_options.rm_seconds:\n",
       "   770                                                           # dropping out of bound words\n",
       "   771        10       6838.0    683.8      0.0                  if options.word_timestamps and stream_options.drop_out_of_bound:\n",
       "   772        10      44942.0   4494.2      0.0                      segment[\"words\"] = [word for word in segment[\"words\"] if word[\"start\"] < time_offset+segment_duration - stream_options.rm_seconds]\n",
       "   773        10      35064.0   3506.4      0.0                      text = \"\".join([word[\"word\"] for word in segment[\"words\"]]).strip()\n",
       "   774        10      14600.0   1460.0      0.0                      if tokens[0] >= tokenizer.timestamp_begin:\n",
       "   775                                                                   tokens = [tokens[0]]\n",
       "   776                                                               else:\n",
       "   777        10       4460.0    446.0      0.0                          tokens = []\n",
       "   778        85      39948.0    470.0      0.0                      for word in segment[\"words\"]:\n",
       "   779        85      39729.0    467.4      0.0                          tokens += word[\"tokens\"]\n",
       "   780        10       6506.0    650.6      0.0                      if not text:\n",
       "   781                                                                   break\n",
       "   782                                                           #TODO: Possible to cap segment end and start to time_offset + segment_duration\n",
       "   783                                           \n",
       "   784        51      26991.0    529.2      0.0              if final:\n",
       "   785        51      32456.0    636.4      0.0                  last_final_segment_end = segment[\"end\"]\n",
       "   786        51      28835.0    565.4      0.0                  finalised_idx += 1\n",
       "   787        51      32576.0    638.7      0.0                  if options.condition_on_previous_text and not needs_fallback:\n",
       "   788        49      70193.0   1432.5      0.0                      if stream_options.drop_dup_prompt and (text in previous_text_list):\n",
       "   789         2       6397.0   3198.5      0.0                          self.logger.debug(\"Drop duplicate prompt!\")\n",
       "   790                                                               else:\n",
       "   791        49      34287.0    699.7      0.0                          previous_text_list.append(text)\n",
       "   792        49      36490.0    744.7      0.0                          previous_tokens_list.append(tokens)\n",
       "   793        49      39760.0    811.4      0.0                          tot_len = len(previous_tokens) + len(tokens)\n",
       "   794        49      26729.0    545.5      0.0                          old_tokens = []\n",
       "   795        49      29294.0    597.8      0.0                          while (tot_len > stream_options.max_prompt_tokens) and previous_tokens_list:\n",
       "   796        43      31726.0    737.8      0.0                              old_text = previous_text_list.pop(0)\n",
       "   797        43      42129.0    979.7      0.0                              old_tokens = previous_tokens_list.pop(0)\n",
       "   798        43      26657.0    619.9      0.0                              tot_len -= len(old_tokens)\n",
       "   799        49      30990.0    632.4      0.0                          if not stream_options.is_prompt_sentence and old_tokens:\n",
       "   800                                                                       previous_text_list.insert(0, old_text)\n",
       "   801                                                                       previous_tokens_list.insert(0, old_tokens)\n",
       "   802                                                       else:\n",
       "   803                                                           # For using timestamp and prefix, need to adjust the timestamp, though not working as expected\n",
       "   804        30      22644.0    754.8      0.0                  if stream_options.use_prefix and not needs_fallback:\n",
       "   805                                                               if not options.without_timestamps :\n",
       "   806                                                                   tokens[0] = tokens[0] - round((last_final_segment_end-time_offset)/self.time_precision)\n",
       "   807                                                                   if tokens[-1] >= tokenizer.timestamp_begin:\n",
       "   808                                                                       tokens[-1] = tokens[-1] - round((last_final_segment_end-time_offset)/self.time_precision)\n",
       "   809                                                               if stream_options.drop_prefix_duration > 0 and options.word_timestamps:\n",
       "   810                                                                   if segment['end'] < time_offset + segment_duration - stream_options.drop_prefix_duration:\n",
       "   811                                                                       prefix_tokens.extend(tokens)\n",
       "   812                                                                   elif segment['start'] < time_offset + segment_duration - stream_options.drop_prefix_duration:\n",
       "   813                                                                       tokens = []\n",
       "   814                                                                       for word in segment[\"words\"]:\n",
       "   815                                                                           if word[\"start\"] < time_offset + segment_duration - stream_options.drop_prefix_duration:\n",
       "   816                                                                               prefix_tokens.extend(word[\"tokens\"])\n",
       "   817                                                                           else:\n",
       "   818                                                                               break\n",
       "   819                                                               else:\n",
       "   820                                                                   prefix_tokens.extend(tokens)\n",
       "   821        81      58673.0    724.4      0.0              current_segment_count += 1\n",
       "   822                                                       #! Can return to client here instead of outputting it\n",
       "   823        81      74635.0    921.4      0.0              output_segments.append(\n",
       "   824        81     294722.0   3638.5      0.0                  StreamSegment(\n",
       "   825        81      36330.0    448.5      0.0                      id=current_segment_count,\n",
       "   826        81      48477.0    598.5      0.0                      start=segment[\"start\"],\n",
       "   827        81      39672.0    489.8      0.0                      end=segment[\"end\"],\n",
       "   828        81      40050.0    494.4      0.0                      text=text,\n",
       "   829        81      41080.0    507.2      0.0                      temperature=temperature,\n",
       "   830        81      41492.0    512.2      0.0                      avg_logprob=avg_logprob,\n",
       "   831        81      46685.0    576.4      0.0                      compression_ratio=compression_ratio,\n",
       "   832        81     321817.0   3973.0      0.0                      no_speech_prob=result.no_speech_prob,\n",
       "   833                                                               words=(\n",
       "   834        81    1085687.0  13403.5      0.0                          [Word(**word) for word in segment[\"words\"]]\n",
       "   835        81      46953.0    579.7      0.0                          if options.word_timestamps\n",
       "   836                                                                   else None\n",
       "   837                                                               ),\n",
       "   838        81      41882.0    517.1      0.0                      final=final,\n",
       "   839        81      40768.0    503.3      0.0                      reason=reason\n",
       "   840                                                           )\n",
       "   841                                                       )\n",
       "   842                                           \n",
       "   843                                                   #To handle the edge case where there is not text for very long\n",
       "   844        29      47995.0   1655.0      0.0          if segment_duration - last_final_segment_end - time_offset > stream_options.max_carry_forward_dur:\n",
       "   845                                                       last_final_segment_end = segment_duration + time_offset\n",
       "   846                                                   # Keeping the audio for those segment not finalised\n",
       "   847                                                   # TODO: Possible to add option keep only those words within the segment_end_thres\n",
       "   848                                                   # TODO: to reduce audio kept and lesser token generated will need a var to keep track\n",
       "   849                                                   # TODO: of the unfinalised tokens and add them to the prompt. Do this only when needs_fallback is False.\n",
       "   850                                           \n",
       "   851                                                   # TODO: If options.without_timestamp is False and single_timestamp_ending is True should ensure the last segment final=False\n",
       "   852        29     230039.0   7932.4      0.0          seek_shift = round(\n",
       "   853        29      34624.0   1193.9      0.0              (last_final_segment_end - time_offset) * self.frames_per_second\n",
       "   854                                                   )\n",
       "   855                                           \n",
       "   856                                                   # Will wipe all tokens if finalise all or temperature exceeds prompt_reset_on_temperature\n",
       "   857        29      34512.0   1190.1      0.0          if temperature > options.prompt_reset_on_temperature or finalise_all:\n",
       "   858                                                       self.logger.info(\"Removing previous tokens!\")\n",
       "   859                                                       previous_tokens_list = []\n",
       "   860                                                       previous_text_list = []\n",
       "   861                                           \n",
       "   862        29     122136.0   4211.6      0.0          self.logger.debug(\"Text: %s\", \" \".join(previous_text_list))\n",
       "   863                                           \n",
       "   864        28      15592.0    556.9      0.0          if needs_fallback:\n",
       "   865         1      10029.0  10029.0      0.0              temperature_idx = min(len(self.temperatures) - 1, temperature_idx + 1)\n",
       "   866                                                   else:\n",
       "   867        28      19289.0    688.9      0.0              temperature_idx = 0\n",
       "   868                                           \n",
       "   869                                                   # Need to discard audio that is finalized and add in need audio\n",
       "   870                                                   # Check if text occurred in the added silence\n",
       "   871        29      23266.0    802.3      0.0          if seek_shift >= segment_size:\n",
       "   872                                                       seek_shift = segment_size - 1\n",
       "   873        29      20719.0    714.4      0.0          seek = seek + seek_shift\n",
       "   874        29     125962.0   4343.5      0.0          audio = audio[min(seek_shift * self.feature_extractor.hop_length, len(audio)) :]\n",
       "   875                                           \n",
       "   876        29      12905.0    445.0      0.0          return output_segments, audio, finalised_idx, seek, temperature_idx, previous_tokens_list, previous_text_list, prefix_tokens"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f model.simulate_fixed_interval_streaming -f model.generate_segments model.simulate_fixed_interval_streaming(\"GovTech_MFTMEP1_16000_mono_16.wav\", tokenizer, options, stream_options, interval=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c1c26b2-5842-438b-baf8-0479d73c9c3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(1.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c11a2963-128f-4f59-bcfc-3ed3f8d9c8b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StreamOptions(finalised_segment_gap=2, out_of_bound_gap=5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8dd9213b-b089-4489-841b-91116dad68c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<faster_whisper.tokenizer.Tokenizer at 0x7f9058432640>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b70dc43e-f3e4-43d2-9351-1b862260d726",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import soundfile as sf\n",
    "import time\n",
    "import numpy as np\n",
    "import importlib\n",
    "import jeff_live\n",
    "importlib.reload(jeff_live)\n",
    "from jeff_live import WhisperModel_live\n",
    "\n",
    "model_size = 'small.en'\n",
    "model_dir = \"pretrained_models/faster_whisper/\"\n",
    "# audio_file = \"data/wav/IMDA_test_60s.wav\"\n",
    "audio_file = \"GovTech_MFTMEP1_16000_mono_16.wav\"\n",
    "\n",
    "# Load model before model files have been downloaded\n",
    "# faster_model_live = WhisperModel_live(model_size, compute_type=\"int8\", download_root=model_dir)\n",
    "# Load model after model files have been downloaded\n",
    "faster_model_live = WhisperModel_live(model_size, compute_type=\"int8\")\n",
    "\n",
    "def run_audio(audio_file, faster_model_live):\n",
    "  wav, fs = sf.read(audio_file)\n",
    "  wav_len = len(wav)\n",
    "  \n",
    "  RATE = 16000\n",
    "  CHUNK_S = 7\n",
    "  CHUNK = CHUNK_S * RATE\n",
    "  # max_time = 440.0\n",
    "  max_time = -1\n",
    "  compression_ratio_threshold = 2.4\n",
    "  \n",
    "  if max_time > 0:\n",
    "      wav_len = min(wav_len, int(max_time*RATE))\n",
    "  chunk_starts = list(range(0, wav_len, CHUNK))\n",
    "  final_text = ''\n",
    "  seek = 0.0\n",
    "  for chunk_start in chunk_starts[0:]:\n",
    "      t1 = time.time()\n",
    "      non_final_text = ''\n",
    "      repeat_text = ''\n",
    "      start_idx = min(chunk_start, int(seek*RATE))\n",
    "      end_idx = min(wav_len, chunk_start + CHUNK)\n",
    "  \n",
    "      # Make sure audio chunk is less than 30s\n",
    "      # Skip to most recent 30s if longer\n",
    "      start_idx = max(start_idx, end_idx - 30*RATE)\n",
    "      start_time = start_idx/RATE\n",
    "      end_time = end_idx/RATE\n",
    "  \n",
    "      print(f'Transcribing {start_time:.2f}->{end_time:.2f}s...')\n",
    "      wav_chunk = np.array(wav[start_idx:end_idx], dtype=np.float32)\n",
    "      \n",
    "      segments_live, info = faster_model_live.transcribe(wav_chunk, beam_size=1, temperature=0, vad_filter=False, word_timestamps=False)\n",
    "      seek = start_time\n",
    "      for seg in segments_live:\n",
    "          # print(seg)\n",
    "          print(f'SEG {start_time+seg.start:.2f}->{start_time+seg.end:.2f}, seek:{start_time+seg.seek_live:.2f}'\n",
    "                  + f', final:{seg.final}, compression_ratio: {seg.compression_ratio:.2f}, words: {seg.text.lstrip()}')\n",
    "  \n",
    "          if seg.compression_ratio > compression_ratio_threshold:\n",
    "              repeat_text += seg.text\n",
    "  \n",
    "          elif seg.final:\n",
    "              final_text += seg.text\n",
    "              seek = start_time + seg.seek_live\n",
    "          else:\n",
    "              non_final_text = seg.text\n",
    "  \n",
    "      t2 = time.time()\n",
    "      time_elapsed = t2 - t1\n",
    "      print(f'Output: {final_text.lstrip()} | {non_final_text.lstrip()}')\n",
    "      if repeat_text:\n",
    "          print(f'REPEAT: {repeat_text.lstrip()}')\n",
    "      print(f'Time elapsed: {time_elapsed:0.2f}s\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97f7a366-9c00-4b45-8b62-dfb64791bab7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing 0.00->7.00s...\n",
      "Processing audio with duration 00:07.000\n",
      "Processing audio with duration 00:07.000\n",
      "SEG 0.00->6.94, seek:6.94, final:False, compression_ratio: 0.38, words: Music\n",
      "Output:  | Music\n",
      "Time elapsed: 5.67s\n",
      "\n",
      "Transcribing 0.00->14.00s...\n",
      "Processing audio with duration 00:14.000\n",
      "Processing audio with duration 00:14.000\n",
      "SEG 0.00->9.60, seek:9.60, final:True, compression_ratio: 1.09, words: Welcome to My Friend Tell Me One.\n",
      "SEG 9.60->14.00, seek:9.60, final:False, compression_ratio: 1.09, words: Today we're going to speak to the Go Bros, two engineers who work in DevOps.\n",
      "Output: Welcome to My Friend Tell Me One. | Today we're going to speak to the Go Bros, two engineers who work in DevOps.\n",
      "Time elapsed: 5.79s\n",
      "\n",
      "Transcribing 9.60->21.00s...\n",
      "Processing audio with duration 00:11.400\n",
      "Processing audio with duration 00:11.400\n",
      "SEG 9.60->13.76, seek:19.60, final:True, compression_ratio: 1.26, words: Today we're going to speak to the Go Bros, two engineers who work in DevOps.\n",
      "SEG 14.32->17.52, seek:19.60, final:True, compression_ratio: 1.26, words: Okay, hi, my name is Ryan. I work on my current future.\n",
      "SEG 18.00->19.60, seek:19.60, final:True, compression_ratio: 1.26, words: So basically I'm doing DevOps.\n",
      "SEG 20.32->21.00, seek:19.60, final:False, compression_ratio: 1.26, words: Same as I go.\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. | Same as I go.\n",
      "Time elapsed: 6.64s\n",
      "\n",
      "Transcribing 19.60->28.00s...\n",
      "Processing audio with duration 00:08.400\n",
      "Processing audio with duration 00:08.400\n",
      "SEG 19.60->21.60, seek:21.60, final:False, compression_ratio: 0.69, words: See you next time.\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. | See you next time.\n",
      "Time elapsed: 5.27s\n",
      "\n",
      "Transcribing 19.60->35.00s...\n",
      "Processing audio with duration 00:15.400\n",
      "Processing audio with duration 00:15.400\n",
      "SEG 19.60->21.60, seek:21.60, final:False, compression_ratio: 0.67, words: Same as the guy.\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. | Same as the guy.\n",
      "Time elapsed: 4.73s\n",
      "\n",
      "Transcribing 19.60->42.00s...\n",
      "Processing audio with duration 00:22.400\n",
      "Processing audio with duration 00:22.400\n",
      "SEG 19.60->21.60, seek:37.60, final:True, compression_ratio: 1.50, words: Same as the guy\n",
      "SEG 28.60->30.60, seek:37.60, final:True, compression_ratio: 1.50, words: Text word answer is divorce is a culture\n",
      "SEG 31.60->37.60, seek:37.60, final:True, compression_ratio: 1.50, words: So in other words, the whole team knows how to do operations, some parts of operations as well as development\n",
      "SEG 37.60->42.00, seek:37.60, final:False, compression_ratio: 1.50, words: Giving the responsibility of what operators normally do to developers\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. Same as the guy Text word answer is divorce is a culture So in other words, the whole team knows how to do operations, some parts of operations as well as development | Giving the responsibility of what operators normally do to developers\n",
      "Time elapsed: 5.60s\n",
      "\n",
      "Transcribing 37.60->49.00s...\n",
      "Processing audio with duration 00:11.400\n",
      "Processing audio with duration 00:11.400\n",
      "SEG 37.60->42.24, seek:42.24, final:True, compression_ratio: 1.47, words: giving the responsibility of what operators normally do to developers.\n",
      "SEG 42.24->49.00, seek:42.24, final:False, compression_ratio: 1.47, words: It's kind of a wide term, so depending on where you go, different organisations, different teams have different definitions of it.\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. Same as the guy Text word answer is divorce is a culture So in other words, the whole team knows how to do operations, some parts of operations as well as development giving the responsibility of what operators normally do to developers. | It's kind of a wide term, so depending on where you go, different organisations, different teams have different definitions of it.\n",
      "Time elapsed: 5.48s\n",
      "\n",
      "Transcribing 42.24->56.00s...\n",
      "Processing audio with duration 00:13.760\n",
      "Processing audio with duration 00:13.760\n",
      "SEG 42.24->49.24, seek:51.24, final:True, compression_ratio: 1.44, words: It's kind of a wide term, so depending on where you go, different organisations, different teams have different definition of it.\n",
      "SEG 49.24->51.24, seek:51.24, final:True, compression_ratio: 1.44, words: Yeah, so non-text vote is high-tech.\n",
      "SEG 54.24->56.00, seek:51.24, final:False, compression_ratio: 1.44, words: It's not a new thing, actually it's me.\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. Same as the guy Text word answer is divorce is a culture So in other words, the whole team knows how to do operations, some parts of operations as well as development giving the responsibility of what operators normally do to developers. It's kind of a wide term, so depending on where you go, different organisations, different teams have different definition of it. Yeah, so non-text vote is high-tech. | It's not a new thing, actually it's me.\n",
      "Time elapsed: 7.13s\n",
      "\n",
      "Transcribing 51.24->63.00s...\n",
      "Processing audio with duration 00:11.760\n",
      "Processing audio with duration 00:11.760\n",
      "SEG 51.24->51.74, seek:51.74, final:False, compression_ratio: 0.50, words: Welcome!\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. Same as the guy Text word answer is divorce is a culture So in other words, the whole team knows how to do operations, some parts of operations as well as development giving the responsibility of what operators normally do to developers. It's kind of a wide term, so depending on where you go, different organisations, different teams have different definition of it. Yeah, so non-text vote is high-tech. | Welcome!\n",
      "Time elapsed: 4.77s\n",
      "\n",
      "Transcribing 51.24->70.00s...\n",
      "Processing audio with duration 00:18.760\n",
      "Processing audio with duration 00:18.760\n",
      "SEG 51.24->51.74, seek:51.74, final:False, compression_ratio: 0.50, words: Welcome!\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. Same as the guy Text word answer is divorce is a culture So in other words, the whole team knows how to do operations, some parts of operations as well as development giving the responsibility of what operators normally do to developers. It's kind of a wide term, so depending on where you go, different organisations, different teams have different definition of it. Yeah, so non-text vote is high-tech. | Welcome!\n",
      "Time elapsed: 4.37s\n",
      "\n",
      "Transcribing 51.24->77.00s...\n",
      "Processing audio with duration 00:25.760\n",
      "Processing audio with duration 00:25.760\n",
      "SEG 51.24->51.74, seek:74.08, final:True, compression_ratio: 1.51, words: Welcome!\n",
      "SEG 55.08->57.32, seek:74.08, final:True, compression_ratio: 1.51, words: It's not a new thing. Actually it's been around since 1993.\n",
      "SEG 57.32->58.76, seek:74.08, final:True, compression_ratio: 1.51, words: Since the agile manifesto came out.\n",
      "SEG 62.48->63.80, seek:74.08, final:True, compression_ratio: 1.51, words: Like, didn't learn it in school?\n",
      "SEG 63.80->64.80, seek:74.08, final:True, compression_ratio: 1.51, words: School cannot teach us.\n",
      "SEG 65.44->66.24, seek:74.08, final:True, compression_ratio: 1.51, words: But...\n",
      "SEG 66.24->67.40, seek:74.08, final:True, compression_ratio: 1.51, words: It costs 2 unique earnings.\n",
      "SEG 67.40->70.16, seek:74.08, final:True, compression_ratio: 1.51, words: Basically, like, we have to run our own service order.\n",
      "SEG 70.16->74.08, seek:74.08, final:True, compression_ratio: 1.51, words: So, by running it, you probably have to do a lot of things to get it running.\n",
      "SEG 74.48->77.00, seek:74.08, final:False, compression_ratio: 1.51, words: So that's when we start learning.\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. Same as the guy Text word answer is divorce is a culture So in other words, the whole team knows how to do operations, some parts of operations as well as development giving the responsibility of what operators normally do to developers. It's kind of a wide term, so depending on where you go, different organisations, different teams have different definition of it. Yeah, so non-text vote is high-tech. Welcome! It's not a new thing. Actually it's been around since 1993. Since the agile manifesto came out. Like, didn't learn it in school? School cannot teach us. But... It costs 2 unique earnings. Basically, like, we have to run our own service order. So, by running it, you probably have to do a lot of things to get it running. | So that's when we start learning.\n",
      "Time elapsed: 7.16s\n",
      "\n",
      "Transcribing 74.08->84.00s...\n",
      "Processing audio with duration 00:09.920\n",
      "Processing audio with duration 00:09.920\n",
      "SEG 74.08->76.08, seek:80.08, final:True, compression_ratio: 0.91, words: So that's when we start learning.\n",
      "SEG 79.58->80.08, seek:80.08, final:True, compression_ratio: 0.91, words: No.\n",
      "SEG 80.08->84.00, seek:80.08, final:False, compression_ratio: 0.91, words: No.\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. Same as the guy Text word answer is divorce is a culture So in other words, the whole team knows how to do operations, some parts of operations as well as development giving the responsibility of what operators normally do to developers. It's kind of a wide term, so depending on where you go, different organisations, different teams have different definition of it. Yeah, so non-text vote is high-tech. Welcome! It's not a new thing. Actually it's been around since 1993. Since the agile manifesto came out. Like, didn't learn it in school? School cannot teach us. But... It costs 2 unique earnings. Basically, like, we have to run our own service order. So, by running it, you probably have to do a lot of things to get it running. So that's when we start learning. No. | No.\n",
      "Time elapsed: 5.01s\n",
      "\n",
      "Transcribing 80.08->91.00s...\n",
      "Processing audio with duration 00:10.920\n",
      "Processing audio with duration 00:10.920\n",
      "SEG 80.08->81.08, seek:88.26, final:True, compression_ratio: 1.00, words: No, I haven't.\n",
      "SEG 85.00->85.52, seek:88.26, final:True, compression_ratio: 1.00, words: Two ones.\n",
      "SEG 85.52->86.16, seek:88.26, final:True, compression_ratio: 1.00, words: Oh, I don't like that.\n",
      "SEG 86.16->86.66, seek:88.26, final:True, compression_ratio: 1.00, words: Seven?\n",
      "SEG 87.16->87.66, seek:88.26, final:True, compression_ratio: 1.00, words: I think?\n",
      "SEG 87.76->88.26, seek:88.26, final:True, compression_ratio: 1.00, words: Six.\n",
      "SEG 89.92->91.00, seek:88.26, final:False, compression_ratio: 1.00, words: How you say, huh?\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. Same as the guy Text word answer is divorce is a culture So in other words, the whole team knows how to do operations, some parts of operations as well as development giving the responsibility of what operators normally do to developers. It's kind of a wide term, so depending on where you go, different organisations, different teams have different definition of it. Yeah, so non-text vote is high-tech. Welcome! It's not a new thing. Actually it's been around since 1993. Since the agile manifesto came out. Like, didn't learn it in school? School cannot teach us. But... It costs 2 unique earnings. Basically, like, we have to run our own service order. So, by running it, you probably have to do a lot of things to get it running. So that's when we start learning. No. No, I haven't. Two ones. Oh, I don't like that. Seven? I think? Six. | How you say, huh?\n",
      "Time elapsed: 5.22s\n",
      "\n",
      "Transcribing 88.26->98.00s...\n",
      "Processing audio with duration 00:09.740\n",
      "Processing audio with duration 00:09.740\n",
      "SEG 88.26->90.26, seek:98.00, final:True, compression_ratio: 9.69, words: I'm so sorry!\n",
      "SEG 90.26->91.26, seek:98.00, final:True, compression_ratio: 9.69, words: I'm sorry?\n",
      "SEG 91.26->92.26, seek:98.00, final:True, compression_ratio: 9.69, words: I'm sorry!\n",
      "SEG 92.26->93.26, seek:98.00, final:True, compression_ratio: 9.69, words: I'm sorry!\n",
      "SEG 93.26->94.26, seek:98.00, final:True, compression_ratio: 9.69, words: I'm sorry!\n",
      "SEG 94.26->95.26, seek:98.00, final:True, compression_ratio: 9.69, words: I'm sorry!\n",
      "SEG 95.26->96.26, seek:98.00, final:True, compression_ratio: 9.69, words: I'm sorry!\n",
      "SEG 96.26->97.26, seek:98.00, final:True, compression_ratio: 9.69, words: I'm sorry!\n",
      "SEG 97.26->98.26, seek:98.00, final:True, compression_ratio: 9.69, words: I'm sorry!\n",
      "SEG 98.26->99.26, seek:98.00, final:True, compression_ratio: 9.69, words: I'm sorry!\n",
      "SEG 99.26->100.26, seek:98.00, final:True, compression_ratio: 9.69, words: I'm sorry!\n",
      "SEG 100.26->101.26, seek:98.00, final:True, compression_ratio: 9.69, words: I'm sorry!\n",
      "SEG 101.26->102.26, seek:98.00, final:True, compression_ratio: 9.69, words: I'm sorry!\n",
      "SEG 102.26->103.26, seek:98.00, final:True, compression_ratio: 9.69, words: I'm sorry!\n",
      "SEG 103.26->104.26, seek:98.00, final:True, compression_ratio: 9.69, words: I'm sorry!\n",
      "SEG 104.26->105.26, seek:98.00, final:True, compression_ratio: 9.69, words: I'm sorry!\n",
      "SEG 105.26->106.26, seek:98.00, final:True, compression_ratio: 9.69, words: I'm sorry!\n",
      "SEG 106.26->107.26, seek:98.00, final:True, compression_ratio: 9.69, words: I'm sorry!\n",
      "SEG 107.26->108.26, seek:98.00, final:True, compression_ratio: 9.69, words: I'm sorry!\n",
      "SEG 108.26->109.26, seek:98.00, final:True, compression_ratio: 9.69, words: I'm sorry!\n",
      "SEG 109.26->110.26, seek:98.00, final:True, compression_ratio: 9.69, words: I'm sorry!\n",
      "SEG 110.26->111.26, seek:98.00, final:True, compression_ratio: 9.69, words: I'm sorry!\n",
      "SEG 111.26->112.26, seek:98.00, final:True, compression_ratio: 9.69, words: I'm sorry!\n",
      "SEG 112.26->113.26, seek:98.00, final:True, compression_ratio: 9.69, words: I'm sorry!\n",
      "SEG 113.26->114.26, seek:98.00, final:True, compression_ratio: 9.69, words: I'm sorry!\n",
      "SEG 114.26->115.26, seek:98.00, final:True, compression_ratio: 9.69, words: I'm sorry!\n",
      "SEG 115.26->116.26, seek:98.00, final:True, compression_ratio: 9.69, words: I'm sorry!\n",
      "SEG 116.26->117.26, seek:98.00, final:True, compression_ratio: 9.69, words: I'm sorry!\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. Same as the guy Text word answer is divorce is a culture So in other words, the whole team knows how to do operations, some parts of operations as well as development giving the responsibility of what operators normally do to developers. It's kind of a wide term, so depending on where you go, different organisations, different teams have different definition of it. Yeah, so non-text vote is high-tech. Welcome! It's not a new thing. Actually it's been around since 1993. Since the agile manifesto came out. Like, didn't learn it in school? School cannot teach us. But... It costs 2 unique earnings. Basically, like, we have to run our own service order. So, by running it, you probably have to do a lot of things to get it running. So that's when we start learning. No. No, I haven't. Two ones. Oh, I don't like that. Seven? I think? Six. | \n",
      "REPEAT: I'm so sorry! I'm sorry? I'm sorry! I'm sorry! I'm sorry! I'm sorry! I'm sorry! I'm sorry! I'm sorry! I'm sorry! I'm sorry! I'm sorry! I'm sorry! I'm sorry! I'm sorry! I'm sorry! I'm sorry! I'm sorry! I'm sorry! I'm sorry! I'm sorry! I'm sorry! I'm sorry! I'm sorry! I'm sorry! I'm sorry! I'm sorry! I'm sorry!\n",
      "Time elapsed: 9.24s\n",
      "\n",
      "Transcribing 88.26->105.00s...\n",
      "Processing audio with duration 00:16.740\n",
      "Processing audio with duration 00:16.740\n",
      "SEG 88.26->90.26, seek:98.26, final:True, compression_ratio: 2.17, words: I'm not sure.\n",
      "SEG 90.26->92.26, seek:98.26, final:True, compression_ratio: 2.17, words: I'm not sure.\n",
      "SEG 92.26->94.26, seek:98.26, final:True, compression_ratio: 2.17, words: I'm not sure.\n",
      "SEG 94.26->96.26, seek:98.26, final:True, compression_ratio: 2.17, words: You're amazing.\n",
      "SEG 96.26->98.26, seek:98.26, final:True, compression_ratio: 2.17, words: You're amazing.\n",
      "SEG 98.26->105.00, seek:98.26, final:False, compression_ratio: 2.17, words: You're amazing.\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. Same as the guy Text word answer is divorce is a culture So in other words, the whole team knows how to do operations, some parts of operations as well as development giving the responsibility of what operators normally do to developers. It's kind of a wide term, so depending on where you go, different organisations, different teams have different definition of it. Yeah, so non-text vote is high-tech. Welcome! It's not a new thing. Actually it's been around since 1993. Since the agile manifesto came out. Like, didn't learn it in school? School cannot teach us. But... It costs 2 unique earnings. Basically, like, we have to run our own service order. So, by running it, you probably have to do a lot of things to get it running. So that's when we start learning. No. No, I haven't. Two ones. Oh, I don't like that. Seven? I think? Six. I'm not sure. I'm not sure. I'm not sure. You're amazing. You're amazing. | You're amazing.\n",
      "Time elapsed: 5.56s\n",
      "\n",
      "Transcribing 98.26->112.00s...\n",
      "Processing audio with duration 00:13.740\n",
      "Processing audio with duration 00:13.740\n",
      "SEG 98.26->100.26, seek:100.26, final:False, compression_ratio: 0.68, words: You look amazing!\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. Same as the guy Text word answer is divorce is a culture So in other words, the whole team knows how to do operations, some parts of operations as well as development giving the responsibility of what operators normally do to developers. It's kind of a wide term, so depending on where you go, different organisations, different teams have different definition of it. Yeah, so non-text vote is high-tech. Welcome! It's not a new thing. Actually it's been around since 1993. Since the agile manifesto came out. Like, didn't learn it in school? School cannot teach us. But... It costs 2 unique earnings. Basically, like, we have to run our own service order. So, by running it, you probably have to do a lot of things to get it running. So that's when we start learning. No. No, I haven't. Two ones. Oh, I don't like that. Seven? I think? Six. I'm not sure. I'm not sure. I'm not sure. You're amazing. You're amazing. | You look amazing!\n",
      "Time elapsed: 4.64s\n",
      "\n",
      "Transcribing 98.26->119.00s...\n",
      "Processing audio with duration 00:20.740\n",
      "Processing audio with duration 00:20.740\n",
      "SEG 98.26->100.26, seek:100.26, final:False, compression_ratio: 0.68, words: You look amazing!\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. Same as the guy Text word answer is divorce is a culture So in other words, the whole team knows how to do operations, some parts of operations as well as development giving the responsibility of what operators normally do to developers. It's kind of a wide term, so depending on where you go, different organisations, different teams have different definition of it. Yeah, so non-text vote is high-tech. Welcome! It's not a new thing. Actually it's been around since 1993. Since the agile manifesto came out. Like, didn't learn it in school? School cannot teach us. But... It costs 2 unique earnings. Basically, like, we have to run our own service order. So, by running it, you probably have to do a lot of things to get it running. So that's when we start learning. No. No, I haven't. Two ones. Oh, I don't like that. Seven? I think? Six. I'm not sure. I'm not sure. I'm not sure. You're amazing. You're amazing. | You look amazing!\n",
      "Time elapsed: 5.06s\n",
      "\n",
      "Transcribing 98.26->126.00s...\n",
      "Processing audio with duration 00:27.740\n",
      "Processing audio with duration 00:27.740\n",
      "SEG 98.26->101.26, seek:101.26, final:False, compression_ratio: 0.73, words: You look amazing! Wow!\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. Same as the guy Text word answer is divorce is a culture So in other words, the whole team knows how to do operations, some parts of operations as well as development giving the responsibility of what operators normally do to developers. It's kind of a wide term, so depending on where you go, different organisations, different teams have different definition of it. Yeah, so non-text vote is high-tech. Welcome! It's not a new thing. Actually it's been around since 1993. Since the agile manifesto came out. Like, didn't learn it in school? School cannot teach us. But... It costs 2 unique earnings. Basically, like, we have to run our own service order. So, by running it, you probably have to do a lot of things to get it running. So that's when we start learning. No. No, I haven't. Two ones. Oh, I don't like that. Seven? I think? Six. I'm not sure. I'm not sure. I'm not sure. You're amazing. You're amazing. | You look amazing! Wow!\n",
      "Time elapsed: 5.56s\n",
      "\n",
      "Transcribing 103.00->133.00s...\n",
      "Processing audio with duration 00:30.000\n",
      "Processing audio with duration 00:30.000\n",
      "SEG 103.00->109.82, seek:133.00, final:True, compression_ratio: 1.78, words: If me, you can start growing your own vegetables and cooking them so that you manage the whole\n",
      "SEG 109.82->110.82, seek:133.00, final:True, compression_ratio: 1.78, words: thing end to end.\n",
      "SEG 110.82->111.82, seek:133.00, final:True, compression_ratio: 1.78, words: Maybe?\n",
      "SEG 111.82->112.82, seek:133.00, final:True, compression_ratio: 1.78, words: Do everything yourself.\n",
      "SEG 112.82->117.56, seek:133.00, final:True, compression_ratio: 1.78, words: So previously, assuming you've got two people, so one person just cook right and pass everything\n",
      "SEG 117.56->118.56, seek:133.00, final:True, compression_ratio: 1.78, words: to a fan.\n",
      "SEG 118.56->119.56, seek:133.00, final:True, compression_ratio: 1.78, words: You don't care what.\n",
      "SEG 119.56->122.44, seek:133.00, final:True, compression_ratio: 1.78, words: So let's say you cook the egg right, you cook one giant piece, then you just give it\n",
      "SEG 122.44->123.44, seek:133.00, final:True, compression_ratio: 1.78, words: to the person.\n",
      "SEG 123.44->125.72, seek:133.00, final:True, compression_ratio: 1.78, words: Then the front person like, wow, you have to cut out the piece.\n",
      "SEG 125.72->126.72, seek:133.00, final:True, compression_ratio: 1.78, words: So it's like double work.\n",
      "SEG 126.72->130.16, seek:133.00, final:True, compression_ratio: 1.78, words: So now, I cook already, I cut out for you, you go to the fan, then I catch more efficient,\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. Same as the guy Text word answer is divorce is a culture So in other words, the whole team knows how to do operations, some parts of operations as well as development giving the responsibility of what operators normally do to developers. It's kind of a wide term, so depending on where you go, different organisations, different teams have different definition of it. Yeah, so non-text vote is high-tech. Welcome! It's not a new thing. Actually it's been around since 1993. Since the agile manifesto came out. Like, didn't learn it in school? School cannot teach us. But... It costs 2 unique earnings. Basically, like, we have to run our own service order. So, by running it, you probably have to do a lot of things to get it running. So that's when we start learning. No. No, I haven't. Two ones. Oh, I don't like that. Seven? I think? Six. I'm not sure. I'm not sure. I'm not sure. You're amazing. You're amazing. If me, you can start growing your own vegetables and cooking them so that you manage the whole thing end to end. Maybe? Do everything yourself. So previously, assuming you've got two people, so one person just cook right and pass everything to a fan. You don't care what. So let's say you cook the egg right, you cook one giant piece, then you just give it to the person. Then the front person like, wow, you have to cut out the piece. So it's like double work. So now, I cook already, I cut out for you, you go to the fan, then I catch more efficient, | \n",
      "Time elapsed: 8.80s\n",
      "\n",
      "Transcribing 133.00->140.00s...\n",
      "Processing audio with duration 00:07.000\n",
      "Processing audio with duration 00:07.000\n",
      "SEG 133.00->140.00, seek:140.00, final:False, compression_ratio: 1.07, words: Okay, so MicroS future is a platform created to reduce unemployment in local countries.\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. Same as the guy Text word answer is divorce is a culture So in other words, the whole team knows how to do operations, some parts of operations as well as development giving the responsibility of what operators normally do to developers. It's kind of a wide term, so depending on where you go, different organisations, different teams have different definition of it. Yeah, so non-text vote is high-tech. Welcome! It's not a new thing. Actually it's been around since 1993. Since the agile manifesto came out. Like, didn't learn it in school? School cannot teach us. But... It costs 2 unique earnings. Basically, like, we have to run our own service order. So, by running it, you probably have to do a lot of things to get it running. So that's when we start learning. No. No, I haven't. Two ones. Oh, I don't like that. Seven? I think? Six. I'm not sure. I'm not sure. I'm not sure. You're amazing. You're amazing. If me, you can start growing your own vegetables and cooking them so that you manage the whole thing end to end. Maybe? Do everything yourself. So previously, assuming you've got two people, so one person just cook right and pass everything to a fan. You don't care what. So let's say you cook the egg right, you cook one giant piece, then you just give it to the person. Then the front person like, wow, you have to cut out the piece. So it's like double work. So now, I cook already, I cut out for you, you go to the fan, then I catch more efficient, | Okay, so MicroS future is a platform created to reduce unemployment in local countries.\n",
      "Time elapsed: 5.50s\n",
      "\n",
      "Transcribing 133.00->147.00s...\n",
      "Processing audio with duration 00:14.000\n",
      "Processing audio with duration 00:14.000\n",
      "SEG 133.00->140.88, seek:145.00, final:True, compression_ratio: 1.33, words: Okay, so MicroS Future is a platform created to reduce unemployment in local PMAs, especially\n",
      "SEG 140.88->145.00, seek:145.00, final:True, compression_ratio: 1.33, words: those in the 30s and 40s who find themselves unemployed and they are unable to rejoin the\n",
      "SEG 145.00->147.00, seek:145.00, final:False, compression_ratio: 1.33, words: workforce.\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. Same as the guy Text word answer is divorce is a culture So in other words, the whole team knows how to do operations, some parts of operations as well as development giving the responsibility of what operators normally do to developers. It's kind of a wide term, so depending on where you go, different organisations, different teams have different definition of it. Yeah, so non-text vote is high-tech. Welcome! It's not a new thing. Actually it's been around since 1993. Since the agile manifesto came out. Like, didn't learn it in school? School cannot teach us. But... It costs 2 unique earnings. Basically, like, we have to run our own service order. So, by running it, you probably have to do a lot of things to get it running. So that's when we start learning. No. No, I haven't. Two ones. Oh, I don't like that. Seven? I think? Six. I'm not sure. I'm not sure. I'm not sure. You're amazing. You're amazing. If me, you can start growing your own vegetables and cooking them so that you manage the whole thing end to end. Maybe? Do everything yourself. So previously, assuming you've got two people, so one person just cook right and pass everything to a fan. You don't care what. So let's say you cook the egg right, you cook one giant piece, then you just give it to the person. Then the front person like, wow, you have to cut out the piece. So it's like double work. So now, I cook already, I cut out for you, you go to the fan, then I catch more efficient, Okay, so MicroS Future is a platform created to reduce unemployment in local PMAs, especially those in the 30s and 40s who find themselves unemployed and they are unable to rejoin the | workforce.\n",
      "Time elapsed: 5.72s\n",
      "\n",
      "Transcribing 145.00->154.00s...\n",
      "Processing audio with duration 00:09.000\n",
      "Processing audio with duration 00:09.000\n",
      "SEG 145.00->152.00, seek:152.00, final:False, compression_ratio: 1.22, words: So we do this through different ways, so instead of matching people to drops, we match skills to drops for example.\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. Same as the guy Text word answer is divorce is a culture So in other words, the whole team knows how to do operations, some parts of operations as well as development giving the responsibility of what operators normally do to developers. It's kind of a wide term, so depending on where you go, different organisations, different teams have different definition of it. Yeah, so non-text vote is high-tech. Welcome! It's not a new thing. Actually it's been around since 1993. Since the agile manifesto came out. Like, didn't learn it in school? School cannot teach us. But... It costs 2 unique earnings. Basically, like, we have to run our own service order. So, by running it, you probably have to do a lot of things to get it running. So that's when we start learning. No. No, I haven't. Two ones. Oh, I don't like that. Seven? I think? Six. I'm not sure. I'm not sure. I'm not sure. You're amazing. You're amazing. If me, you can start growing your own vegetables and cooking them so that you manage the whole thing end to end. Maybe? Do everything yourself. So previously, assuming you've got two people, so one person just cook right and pass everything to a fan. You don't care what. So let's say you cook the egg right, you cook one giant piece, then you just give it to the person. Then the front person like, wow, you have to cut out the piece. So it's like double work. So now, I cook already, I cut out for you, you go to the fan, then I catch more efficient, Okay, so MicroS Future is a platform created to reduce unemployment in local PMAs, especially those in the 30s and 40s who find themselves unemployed and they are unable to rejoin the | So we do this through different ways, so instead of matching people to drops, we match skills to drops for example.\n",
      "Time elapsed: 6.29s\n",
      "\n",
      "Transcribing 145.00->161.00s...\n",
      "Processing audio with duration 00:16.000\n",
      "Processing audio with duration 00:16.000\n",
      "SEG 145.00->152.00, seek:152.00, final:True, compression_ratio: 1.27, words: So we do this through different ways, so instead of matching people to drops, we match skills to drops for example\n",
      "SEG 156.00->161.00, seek:152.00, final:False, compression_ratio: 1.27, words: Yeah, definitely, best\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. Same as the guy Text word answer is divorce is a culture So in other words, the whole team knows how to do operations, some parts of operations as well as development giving the responsibility of what operators normally do to developers. It's kind of a wide term, so depending on where you go, different organisations, different teams have different definition of it. Yeah, so non-text vote is high-tech. Welcome! It's not a new thing. Actually it's been around since 1993. Since the agile manifesto came out. Like, didn't learn it in school? School cannot teach us. But... It costs 2 unique earnings. Basically, like, we have to run our own service order. So, by running it, you probably have to do a lot of things to get it running. So that's when we start learning. No. No, I haven't. Two ones. Oh, I don't like that. Seven? I think? Six. I'm not sure. I'm not sure. I'm not sure. You're amazing. You're amazing. If me, you can start growing your own vegetables and cooking them so that you manage the whole thing end to end. Maybe? Do everything yourself. So previously, assuming you've got two people, so one person just cook right and pass everything to a fan. You don't care what. So let's say you cook the egg right, you cook one giant piece, then you just give it to the person. Then the front person like, wow, you have to cut out the piece. So it's like double work. So now, I cook already, I cut out for you, you go to the fan, then I catch more efficient, Okay, so MicroS Future is a platform created to reduce unemployment in local PMAs, especially those in the 30s and 40s who find themselves unemployed and they are unable to rejoin the So we do this through different ways, so instead of matching people to drops, we match skills to drops for example | Yeah, definitely, best\n",
      "Time elapsed: 8.08s\n",
      "\n",
      "Transcribing 152.00->168.00s...\n",
      "Processing audio with duration 00:16.000\n",
      "Processing audio with duration 00:16.000\n",
      "SEG 152.00->159.00, seek:159.00, final:False, compression_ratio: 0.60, words: I think best\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. Same as the guy Text word answer is divorce is a culture So in other words, the whole team knows how to do operations, some parts of operations as well as development giving the responsibility of what operators normally do to developers. It's kind of a wide term, so depending on where you go, different organisations, different teams have different definition of it. Yeah, so non-text vote is high-tech. Welcome! It's not a new thing. Actually it's been around since 1993. Since the agile manifesto came out. Like, didn't learn it in school? School cannot teach us. But... It costs 2 unique earnings. Basically, like, we have to run our own service order. So, by running it, you probably have to do a lot of things to get it running. So that's when we start learning. No. No, I haven't. Two ones. Oh, I don't like that. Seven? I think? Six. I'm not sure. I'm not sure. I'm not sure. You're amazing. You're amazing. If me, you can start growing your own vegetables and cooking them so that you manage the whole thing end to end. Maybe? Do everything yourself. So previously, assuming you've got two people, so one person just cook right and pass everything to a fan. You don't care what. So let's say you cook the egg right, you cook one giant piece, then you just give it to the person. Then the front person like, wow, you have to cut out the piece. So it's like double work. So now, I cook already, I cut out for you, you go to the fan, then I catch more efficient, Okay, so MicroS Future is a platform created to reduce unemployment in local PMAs, especially those in the 30s and 40s who find themselves unemployed and they are unable to rejoin the So we do this through different ways, so instead of matching people to drops, we match skills to drops for example | I think best\n",
      "Time elapsed: 4.55s\n",
      "\n",
      "Transcribing 152.00->175.00s...\n",
      "Processing audio with duration 00:23.000\n",
      "Processing audio with duration 00:23.000\n",
      "SEG 152.00->154.00, seek:172.00, final:True, compression_ratio: 5.01, words: I'm going to ask you about the game.\n",
      "SEG 154.00->156.00, seek:172.00, final:True, compression_ratio: 5.01, words: I'm going to ask you about the game.\n",
      "SEG 156.00->158.00, seek:172.00, final:True, compression_ratio: 5.01, words: How do you answer it?\n",
      "SEG 158.00->160.00, seek:172.00, final:True, compression_ratio: 5.01, words: I'm going to ask you about the game.\n",
      "SEG 160.00->162.00, seek:172.00, final:True, compression_ratio: 5.01, words: I'm going to ask you about the game.\n",
      "SEG 162.00->164.00, seek:172.00, final:True, compression_ratio: 5.01, words: I'm going to ask you about the game.\n",
      "SEG 164.00->166.00, seek:172.00, final:True, compression_ratio: 5.01, words: How do you answer it?\n",
      "SEG 166.00->168.00, seek:172.00, final:True, compression_ratio: 5.01, words: I'm going to ask you about the game.\n",
      "SEG 168.00->170.00, seek:172.00, final:True, compression_ratio: 5.01, words: I'm going to ask you about the game.\n",
      "SEG 170.00->172.00, seek:172.00, final:True, compression_ratio: 5.01, words: I'm going to ask you about the game.\n",
      "SEG 172.00->175.00, seek:172.00, final:False, compression_ratio: 5.01, words: I'm going to ask you about the game.\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. Same as the guy Text word answer is divorce is a culture So in other words, the whole team knows how to do operations, some parts of operations as well as development giving the responsibility of what operators normally do to developers. It's kind of a wide term, so depending on where you go, different organisations, different teams have different definition of it. Yeah, so non-text vote is high-tech. Welcome! It's not a new thing. Actually it's been around since 1993. Since the agile manifesto came out. Like, didn't learn it in school? School cannot teach us. But... It costs 2 unique earnings. Basically, like, we have to run our own service order. So, by running it, you probably have to do a lot of things to get it running. So that's when we start learning. No. No, I haven't. Two ones. Oh, I don't like that. Seven? I think? Six. I'm not sure. I'm not sure. I'm not sure. You're amazing. You're amazing. If me, you can start growing your own vegetables and cooking them so that you manage the whole thing end to end. Maybe? Do everything yourself. So previously, assuming you've got two people, so one person just cook right and pass everything to a fan. You don't care what. So let's say you cook the egg right, you cook one giant piece, then you just give it to the person. Then the front person like, wow, you have to cut out the piece. So it's like double work. So now, I cook already, I cut out for you, you go to the fan, then I catch more efficient, Okay, so MicroS Future is a platform created to reduce unemployment in local PMAs, especially those in the 30s and 40s who find themselves unemployed and they are unable to rejoin the So we do this through different ways, so instead of matching people to drops, we match skills to drops for example | \n",
      "REPEAT: I'm going to ask you about the game. I'm going to ask you about the game. How do you answer it? I'm going to ask you about the game. I'm going to ask you about the game. I'm going to ask you about the game. How do you answer it? I'm going to ask you about the game. I'm going to ask you about the game. I'm going to ask you about the game. I'm going to ask you about the game.\n",
      "Time elapsed: 7.72s\n",
      "\n",
      "Transcribing 152.00->182.00s...\n",
      "Processing audio with duration 00:30.000\n",
      "Processing audio with duration 00:30.000\n",
      "SEG 152.00->154.00, seek:182.00, final:True, compression_ratio: 3.44, words: I'm going to ask you about your own game.\n",
      "SEG 154.00->156.00, seek:182.00, final:True, compression_ratio: 3.44, words: I'm going to ask you about your own game.\n",
      "SEG 156.00->158.00, seek:182.00, final:True, compression_ratio: 3.44, words: How do you answer?\n",
      "SEG 158.00->160.00, seek:182.00, final:True, compression_ratio: 3.44, words: I'm going to ask you about your own game.\n",
      "SEG 160.00->162.00, seek:182.00, final:True, compression_ratio: 3.44, words: I'm going to ask you about your own game.\n",
      "SEG 162.00->164.00, seek:182.00, final:True, compression_ratio: 3.44, words: I'm going to ask you about your own game.\n",
      "SEG 164.00->166.00, seek:182.00, final:True, compression_ratio: 3.44, words: How do you answer?\n",
      "SEG 166.00->168.00, seek:182.00, final:True, compression_ratio: 3.44, words: I'm going to ask you about your own game.\n",
      "SEG 168.00->170.00, seek:182.00, final:True, compression_ratio: 3.44, words: I'm going to ask you about your own game.\n",
      "SEG 170.00->172.00, seek:182.00, final:True, compression_ratio: 3.44, words: I'm going to ask you about your own game.\n",
      "SEG 172.00->174.00, seek:182.00, final:True, compression_ratio: 3.44, words: I'm going to ask you about your own game.\n",
      "SEG 174.00->176.00, seek:182.00, final:True, compression_ratio: 3.44, words: Hi, my name is Ryan.\n",
      "SEG 176.00->178.00, seek:182.00, final:True, compression_ratio: 3.44, words: I work in Death Force because I think it's cool\n",
      "SEG 178.00->180.00, seek:182.00, final:True, compression_ratio: 3.44, words: and I have a lot of control over the things that I work at.\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. Same as the guy Text word answer is divorce is a culture So in other words, the whole team knows how to do operations, some parts of operations as well as development giving the responsibility of what operators normally do to developers. It's kind of a wide term, so depending on where you go, different organisations, different teams have different definition of it. Yeah, so non-text vote is high-tech. Welcome! It's not a new thing. Actually it's been around since 1993. Since the agile manifesto came out. Like, didn't learn it in school? School cannot teach us. But... It costs 2 unique earnings. Basically, like, we have to run our own service order. So, by running it, you probably have to do a lot of things to get it running. So that's when we start learning. No. No, I haven't. Two ones. Oh, I don't like that. Seven? I think? Six. I'm not sure. I'm not sure. I'm not sure. You're amazing. You're amazing. If me, you can start growing your own vegetables and cooking them so that you manage the whole thing end to end. Maybe? Do everything yourself. So previously, assuming you've got two people, so one person just cook right and pass everything to a fan. You don't care what. So let's say you cook the egg right, you cook one giant piece, then you just give it to the person. Then the front person like, wow, you have to cut out the piece. So it's like double work. So now, I cook already, I cut out for you, you go to the fan, then I catch more efficient, Okay, so MicroS Future is a platform created to reduce unemployment in local PMAs, especially those in the 30s and 40s who find themselves unemployed and they are unable to rejoin the So we do this through different ways, so instead of matching people to drops, we match skills to drops for example | \n",
      "REPEAT: I'm going to ask you about your own game. I'm going to ask you about your own game. How do you answer? I'm going to ask you about your own game. I'm going to ask you about your own game. I'm going to ask you about your own game. How do you answer? I'm going to ask you about your own game. I'm going to ask you about your own game. I'm going to ask you about your own game. I'm going to ask you about your own game. Hi, my name is Ryan. I work in Death Force because I think it's cool and I have a lot of control over the things that I work at.\n",
      "Time elapsed: 9.75s\n",
      "\n",
      "Transcribing 159.00->189.00s...\n",
      "Processing audio with duration 00:30.000\n",
      "Processing audio with duration 00:30.000\n",
      "SEG 159.00->163.96, seek:189.00, final:True, compression_ratio: 1.91, words: I'm not a Chinese person, I'm a Chinese person.\n",
      "SEG 163.96->164.96, seek:189.00, final:True, compression_ratio: 1.91, words: How do you like it?\n",
      "SEG 165.80->166.80, seek:189.00, final:True, compression_ratio: 1.91, words: I like it a lot.\n",
      "SEG 166.80->167.80, seek:189.00, final:True, compression_ratio: 1.91, words: A lot?\n",
      "SEG 168.16->169.16, seek:189.00, final:True, compression_ratio: 1.91, words: I like it a lot.\n",
      "SEG 169.16->170.16, seek:189.00, final:True, compression_ratio: 1.91, words: I like it a lot.\n",
      "SEG 170.16->171.16, seek:189.00, final:True, compression_ratio: 1.91, words: I like it a lot.\n",
      "SEG 171.16->172.16, seek:189.00, final:True, compression_ratio: 1.91, words: I like it a lot.\n",
      "SEG 174.92->175.92, seek:189.00, final:True, compression_ratio: 1.91, words: Hi, my name is Ryan.\n",
      "SEG 175.92->178.12, seek:189.00, final:True, compression_ratio: 1.91, words: I work in DevOps because I think it's cool\n",
      "SEG 178.12->180.56, seek:189.00, final:True, compression_ratio: 1.91, words: and I have a lot of control over the things that I work at.\n",
      "SEG 180.56->183.96, seek:189.00, final:True, compression_ratio: 1.91, words: And I can play a Chinese instrument at the Earth.\n",
      "SEG 184.44->188.76, seek:189.00, final:True, compression_ratio: 1.91, words: So I'm Joseph and I like DevOps because it's a lot more challenging than development work.\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. Same as the guy Text word answer is divorce is a culture So in other words, the whole team knows how to do operations, some parts of operations as well as development giving the responsibility of what operators normally do to developers. It's kind of a wide term, so depending on where you go, different organisations, different teams have different definition of it. Yeah, so non-text vote is high-tech. Welcome! It's not a new thing. Actually it's been around since 1993. Since the agile manifesto came out. Like, didn't learn it in school? School cannot teach us. But... It costs 2 unique earnings. Basically, like, we have to run our own service order. So, by running it, you probably have to do a lot of things to get it running. So that's when we start learning. No. No, I haven't. Two ones. Oh, I don't like that. Seven? I think? Six. I'm not sure. I'm not sure. I'm not sure. You're amazing. You're amazing. If me, you can start growing your own vegetables and cooking them so that you manage the whole thing end to end. Maybe? Do everything yourself. So previously, assuming you've got two people, so one person just cook right and pass everything to a fan. You don't care what. So let's say you cook the egg right, you cook one giant piece, then you just give it to the person. Then the front person like, wow, you have to cut out the piece. So it's like double work. So now, I cook already, I cut out for you, you go to the fan, then I catch more efficient, Okay, so MicroS Future is a platform created to reduce unemployment in local PMAs, especially those in the 30s and 40s who find themselves unemployed and they are unable to rejoin the So we do this through different ways, so instead of matching people to drops, we match skills to drops for example I'm not a Chinese person, I'm a Chinese person. How do you like it? I like it a lot. A lot? I like it a lot. I like it a lot. I like it a lot. I like it a lot. Hi, my name is Ryan. I work in DevOps because I think it's cool and I have a lot of control over the things that I work at. And I can play a Chinese instrument at the Earth. So I'm Joseph and I like DevOps because it's a lot more challenging than development work. | \n",
      "Time elapsed: 11.35s\n",
      "\n",
      "Transcribing 189.00->196.00s...\n",
      "Processing audio with duration 00:07.000\n",
      "Processing audio with duration 00:07.000\n",
      "SEG 189.00->191.40, seek:191.40, final:True, compression_ratio: 1.10, words: So one interesting thing about me is that I practice martial arts,\n",
      "SEG 191.40->196.00, seek:191.40, final:False, compression_ratio: 1.10, words: going to be specific.\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. Same as the guy Text word answer is divorce is a culture So in other words, the whole team knows how to do operations, some parts of operations as well as development giving the responsibility of what operators normally do to developers. It's kind of a wide term, so depending on where you go, different organisations, different teams have different definition of it. Yeah, so non-text vote is high-tech. Welcome! It's not a new thing. Actually it's been around since 1993. Since the agile manifesto came out. Like, didn't learn it in school? School cannot teach us. But... It costs 2 unique earnings. Basically, like, we have to run our own service order. So, by running it, you probably have to do a lot of things to get it running. So that's when we start learning. No. No, I haven't. Two ones. Oh, I don't like that. Seven? I think? Six. I'm not sure. I'm not sure. I'm not sure. You're amazing. You're amazing. If me, you can start growing your own vegetables and cooking them so that you manage the whole thing end to end. Maybe? Do everything yourself. So previously, assuming you've got two people, so one person just cook right and pass everything to a fan. You don't care what. So let's say you cook the egg right, you cook one giant piece, then you just give it to the person. Then the front person like, wow, you have to cut out the piece. So it's like double work. So now, I cook already, I cut out for you, you go to the fan, then I catch more efficient, Okay, so MicroS Future is a platform created to reduce unemployment in local PMAs, especially those in the 30s and 40s who find themselves unemployed and they are unable to rejoin the So we do this through different ways, so instead of matching people to drops, we match skills to drops for example I'm not a Chinese person, I'm a Chinese person. How do you like it? I like it a lot. A lot? I like it a lot. I like it a lot. I like it a lot. I like it a lot. Hi, my name is Ryan. I work in DevOps because I think it's cool and I have a lot of control over the things that I work at. And I can play a Chinese instrument at the Earth. So I'm Joseph and I like DevOps because it's a lot more challenging than development work. So one interesting thing about me is that I practice martial arts, | going to be specific.\n",
      "Time elapsed: 6.15s\n",
      "\n",
      "Transcribing 191.40->198.81s...\n",
      "Processing audio with duration 00:07.405\n",
      "Processing audio with duration 00:07.405\n",
      "SEG 191.40->193.40, seek:193.40, final:False, compression_ratio: 0.76, words: Wing Chun to be specific.\n",
      "Output: Welcome to My Friend Tell Me One. Today we're going to speak to the Go Bros, two engineers who work in DevOps. Okay, hi, my name is Ryan. I work on my current future. So basically I'm doing DevOps. Same as the guy Text word answer is divorce is a culture So in other words, the whole team knows how to do operations, some parts of operations as well as development giving the responsibility of what operators normally do to developers. It's kind of a wide term, so depending on where you go, different organisations, different teams have different definition of it. Yeah, so non-text vote is high-tech. Welcome! It's not a new thing. Actually it's been around since 1993. Since the agile manifesto came out. Like, didn't learn it in school? School cannot teach us. But... It costs 2 unique earnings. Basically, like, we have to run our own service order. So, by running it, you probably have to do a lot of things to get it running. So that's when we start learning. No. No, I haven't. Two ones. Oh, I don't like that. Seven? I think? Six. I'm not sure. I'm not sure. I'm not sure. You're amazing. You're amazing. If me, you can start growing your own vegetables and cooking them so that you manage the whole thing end to end. Maybe? Do everything yourself. So previously, assuming you've got two people, so one person just cook right and pass everything to a fan. You don't care what. So let's say you cook the egg right, you cook one giant piece, then you just give it to the person. Then the front person like, wow, you have to cut out the piece. So it's like double work. So now, I cook already, I cut out for you, you go to the fan, then I catch more efficient, Okay, so MicroS Future is a platform created to reduce unemployment in local PMAs, especially those in the 30s and 40s who find themselves unemployed and they are unable to rejoin the So we do this through different ways, so instead of matching people to drops, we match skills to drops for example I'm not a Chinese person, I'm a Chinese person. How do you like it? I like it a lot. A lot? I like it a lot. I like it a lot. I like it a lot. I like it a lot. Hi, my name is Ryan. I work in DevOps because I think it's cool and I have a lot of control over the things that I work at. And I can play a Chinese instrument at the Earth. So I'm Joseph and I like DevOps because it's a lot more challenging than development work. So one interesting thing about me is that I practice martial arts, | Wing Chun to be specific.\n",
      "Time elapsed: 4.79s\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-09 s\n",
       "\n",
       "Total time: 181.643 s\n",
       "File: /tmp/ipykernel_10373/3897158399.py\n",
       "Function: run_audio at line 20\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    20                                           def run_audio(audio_file, faster_model_live):\n",
       "    21         1   13333197.0 13333197.0      0.0    wav, fs = sf.read(audio_file)\n",
       "    22         1       1353.0   1353.0      0.0    wav_len = len(wav)\n",
       "    23                                             \n",
       "    24         1        337.0    337.0      0.0    RATE = 16000\n",
       "    25         1        376.0    376.0      0.0    CHUNK_S = 7\n",
       "    26         1        939.0    939.0      0.0    CHUNK = CHUNK_S * RATE\n",
       "    27                                             # max_time = 440.0\n",
       "    28         1        303.0    303.0      0.0    max_time = -1\n",
       "    29         1        156.0    156.0      0.0    compression_ratio_threshold = 2.4\n",
       "    30                                             \n",
       "    31         1        406.0    406.0      0.0    if max_time > 0:\n",
       "    32                                                 wav_len = min(wav_len, int(max_time*RATE))\n",
       "    33         1       4376.0   4376.0      0.0    chunk_starts = list(range(0, wav_len, CHUNK))\n",
       "    34         1        848.0    848.0      0.0    final_text = ''\n",
       "    35         1        644.0    644.0      0.0    seek = 0.0\n",
       "    36        29      22620.0    780.0      0.0    for chunk_start in chunk_starts[0:]:\n",
       "    37        29      16675.0    575.0      0.0        t1 = time.time()\n",
       "    38        29       7315.0    252.2      0.0        non_final_text = ''\n",
       "    39        29       6341.0    218.7      0.0        repeat_text = ''\n",
       "    40        29      92481.0   3189.0      0.0        start_idx = min(chunk_start, int(seek*RATE))\n",
       "    41        29      65204.0   2248.4      0.0        end_idx = min(wav_len, chunk_start + CHUNK)\n",
       "    42                                             \n",
       "    43                                                 # Make sure audio chunk is less than 30s\n",
       "    44                                                 # Skip to most recent 30s if longer\n",
       "    45        29      40988.0   1413.4      0.0        start_idx = max(start_idx, end_idx - 30*RATE)\n",
       "    46        29      15426.0    531.9      0.0        start_time = start_idx/RATE\n",
       "    47        29       8616.0    297.1      0.0        end_time = end_idx/RATE\n",
       "    48                                             \n",
       "    49        29     474428.0  16359.6      0.0        print(f'Transcribing {start_time:.2f}->{end_time:.2f}s...')\n",
       "    50        29    9801392.0 337979.0      0.0        wav_chunk = np.array(wav[start_idx:end_idx], dtype=np.float32)\n",
       "    51                                                 \n",
       "    52        29 8834222019.0 304628345.5      4.9        segments_live, info = faster_model_live.transcribe(wav_chunk, beam_size=1, temperature=0, vad_filter=False, word_timestamps=False)\n",
       "    53        29      52275.0   1802.6      0.0        seek = start_time\n",
       "    54       138 172780090727.0 1252029642.9     95.1        for seg in segments_live:\n",
       "    55                                                     # print(seg)\n",
       "    56       138    3245702.0  23519.6      0.0            print(f'SEG {start_time+seg.start:.2f}->{start_time+seg.end:.2f}, seek:{start_time+seg.seek_live:.2f}'\n",
       "    57       138     312700.0   2265.9      0.0                    + f', final:{seg.final}, compression_ratio: {seg.compression_ratio:.2f}, words: {seg.text.lstrip()}')\n",
       "    58                                             \n",
       "    59        85      41009.0    482.5      0.0            if seg.compression_ratio > compression_ratio_threshold:\n",
       "    60        53      29470.0    556.0      0.0                repeat_text += seg.text\n",
       "    61                                             \n",
       "    62        61      18858.0    309.1      0.0            elif seg.final:\n",
       "    63        61      52254.0    856.6      0.0                final_text += seg.text\n",
       "    64        61      35005.0    573.9      0.0                seek = start_time + seg.seek_live\n",
       "    65                                                     else:\n",
       "    66        24      11160.0    465.0      0.0                non_final_text = seg.text\n",
       "    67                                             \n",
       "    68        29      61087.0   2106.4      0.0        t2 = time.time()\n",
       "    69        29      17380.0    599.3      0.0        time_elapsed = t2 - t1\n",
       "    70        29     478659.0  16505.5      0.0        print(f'Output: {final_text.lstrip()} | {non_final_text.lstrip()}')\n",
       "    71        26      10179.0    391.5      0.0        if repeat_text:\n",
       "    72         3      16947.0   5649.0      0.0            print(f'REPEAT: {repeat_text.lstrip()}')\n",
       "    73        29     271802.0   9372.5      0.0        print(f'Time elapsed: {time_elapsed:0.2f}s\\n')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f run_audio run_audio(audio_file, faster_model_live)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc551fb4-0d5f-44d9-b7d7-e74f1bfd19e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
